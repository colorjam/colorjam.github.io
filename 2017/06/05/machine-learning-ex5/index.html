<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Machine Learning ex5 · Colorjam's Blog</title><meta name="description" content="Machine Learning ex5 - Colorjam"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Colorjam's Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/pinkladies" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Machine Learning ex5</h1><div class="post-info">Jun 5, 2017</div><div class="post-content"><p>本次练习是利用正则化的线性回归来学习不同的偏差-方差（bias-variance）模型，并且学会判断其误差类型，能够对算法进行改进。</p>
<p>根据公式完成线性回归的代价函数和梯度的函数。接下来会根据我们算出来的theta值显示一个模型，如下图</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/line.png" alt="line.png" title="">
<p>可以看到模型不太适合我们的训练值，具有高偏差（high-bias），并且可以看出欠拟合（underfitting）。因此接下来我们要通过显示训练和测试的误差学习曲线来诊断偏差-方差问题。</p>
<a id="more"></a>
<h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>我们可以利用之前写好的函数来计算训练集的θ，并通过这个θ来计算训练集误差和交叉验证误差，显示学习曲线。需要注意的是训练集误差计算时要包括不同的训练集大小，并且两者都不需要正则化。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[theta_train] = trainLinearReg(X, y, lambda);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>运行完图像如下：</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross1.png" alt="cross1.png" title="">
<p>和pdf中的示例还差挺多的，我应该是哪里理解错了。修改for循环中交叉验证误差<code>error_val(i) = linearRegCostFunction(Xval(1:i, :), yval(1:i), theta_train, lambda);</code>，图像如下</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross2.png" alt="cross2.png" title="">
<p>结果还是不正确，交叉验证误差的error值不应该这么小。而且pdf里面也说了</p>
<blockquote>
<p>When you are computing the training set error, make sure you compute it on the training subset (i.e., X(1:n,:) and y(1:n))(instead of the entire training set). However, for the cross validation error,you should compute it over the entire cross validation set.  </p>
</blockquote>
<p>也就是说交叉验证误差要用到全部的交叉验证训练集，而不是X(1:n,:) and y(1:n)。看了一下论坛，应该不同大小的训练集计算不同的θ，因此才有正确的结果：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [theta_train] = trainLinearReg(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), lambda);</div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross3.png" alt="cross3.png" title="">
<p>但是submit以后发现并没有获得分数，上论坛找找答案：</p>
<div class="tip"><br><br>我们不需要在学习曲线的函数中自己添加<code>lambda = 0</code>，λ用于正则化θ，当我们在计算 Jtrain, Jcv以及 Jtest的时候，我们需要的是真正的误差，而不需要额外的惩罚项。<br><br></div>

<p>因此修改4句为<code>  [error_train(i)] = linearRegCostFunction(X(1:i,:), y(1:i), theta_train, 0);</code>，第5句同理。</p>
<h3 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h3><p>本来以为这个函数很简单，简单的用一个循环次方计算出每一列就可以了：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : p</div><div class="line">  X_poly(:, <span class="built_in">i</span>) = X .^ p</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>但是结果却不尽如人意，把计算的结果显示出来，发现每一行的值都是一样的，并且看上去与X好像毫无关系</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly.png" alt="poly.png" title="">
<p>发现问题了！mdzz…无视了循环，就一句代码也会错。。应该改成<code>X_poly(:, i) = X .^ i</code>就对了！</p>
<p>在利用多项式回归函数来计算theta值之前，我们必须对训练集进行特征缩放，不然后面的数据会变得炒鸡大。</p>
<p>在 λ= 0的 情况下，我们发现多项式回归函数可以很好的拟合我们的训练集（下图1），并且可以看到多项式回归的学习曲线（下图2）</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot.png" alt="poly_plot.png" title="">
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning.png" alt="poly_learning.png" title="">
<p>接下来我们通过修改不同的λ值来看一下正则化对偏差-方差的影响。</p>
<p>λ = 1 时：</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda1.png" alt="poly_plot_lambda1.png" title="">
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda1.png" alt="poly_learning_lambda1.png" title="">
<p>λ = 100 时：</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda100.png" alt="poly_plot_lambda100.png" title="">
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda100.png" alt="poly_learning_lambda100.png" title="">
<p>通过上面两个例子可以看出，λ = 1 的时候最好的适应了训练集，并且训练集误差和验证误差都挺小的，比 λ = 0 的时候来得优秀，而λ= 100 的时候就严重失控了，不但欠拟合，而且误差也高的要死。</p>
<h3 id="Selecting-λ-using-a-cross-validation-set"><a href="#Selecting-λ-using-a-cross-validation-set" class="headerlink" title="Selecting λ using a cross validation set"></a>Selecting λ using a cross validation set</h3><p>前面我们通过训练集大小的变化来训练θ，并显示学习曲线，现在我们要通过改变λ来显示学习曲线。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">length</span>(lambda_vec)</div><div class="line">  lambda = lambda_vec(<span class="built_in">i</span>);</div><div class="line">  [theta] = trainLinearReg(X, y, lambda);</div><div class="line">  error_train(<span class="built_in">i</span>) = linearRegCostFunction(X, y, theta, <span class="number">0</span>);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta, <span class="number">0</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>在使用每个函数的时候应该注意参数的带入，在使用_linearRegCostFunction_漏了θ参数，活生生的报错了。显示的学习曲线如下：</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/lambda_learning.png" alt="lambda_learning.png" title="">
<p>可以看出随着λ的逐渐增加，交叉验证误差先减小后增大，有一个极值点，这个极值点便是我们应该选择的λ。</p>
<h3 id="Computing-test-set-error"><a href="#Computing-test-set-error" class="headerlink" title="Computing test set error"></a>Computing test set error</h3><p>测试集误差的计算对于评估最终模型来说很重要，我们使用前面看出来的最优λ = 3来计算测试集误差。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">best_lambda = <span class="number">3</span>;</div><div class="line">[best_theta] = trainLinearReg(X, y, best_lambda);</div><div class="line">error_test = linearRegCostFunction(Xtest, ytest, best_theta, <span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>在这里要注意主函数里已经对Xtest进行了多项式化，在传参的时候要传<strong>X_poly_test</strong>。</p>
<h3 id="Plotting-learning-curves-with-randomly-selected-examples"><a href="#Plotting-learning-curves-with-randomly-selected-examples" class="headerlink" title="Plotting learning curves with randomly selected examples"></a>Plotting learning curves with randomly selected examples</h3><p>随机选择样本来计算θ并显示学习曲线。我使用了双重循环，外面一重i定义为样本的数量error(i)返回样本为i时的误差，内层j为误差迭代次数，每次计算便存入一个error(j)中，循环完成后用这个向量来计算平均值。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[aver_error_train, aver_error_val]</span> = ...</span></div><div class="line">   randlyExamplesCurve(X, y, Xval, yval, lambda)</div><div class="line"></div><div class="line">mx = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">mxal = <span class="built_in">size</span>(Xval, <span class="number">1</span>);</div><div class="line">   </div><div class="line">aver_error_train = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line">aver_error_val = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  <span class="comment">% 随机选择i个样本</span></div><div class="line">  rndIDX1= randperm(mx);</div><div class="line">  rndIDX2 = randperm(mxal);</div><div class="line">  rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">  <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">  theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line"></div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X, rand_y, theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval, rand_yval, theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>最后显示出来的学习曲线。。。五花八门。。。</p>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly1.png" alt="randly1.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly2.png" alt="randly2.png" title="">
<p>再仔细看一下论坛，计算误差值时theta如果不改变是没有意义的，并且无需使用全部的样本，i个就使用前i个样本，因此修改代码</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    <span class="comment">% 随机选择i个样本</span></div><div class="line">    rndIDX1= randperm(mx);</div><div class="line">    rndIDX2 = randperm(mxal);</div><div class="line">    rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">    <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">    theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_y(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_yval(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly3.png" alt="randly3.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly4.png" alt="randly4.png" title="">
<p>要运行老长的时间了，试了两次，这下好像比较像样咯~</p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/06/10/machine-learning-ex6/" class="prev">PREV</a><a href="/2017/05/31/machine-learning-ex4/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2017 <a href="http://yoursite.com">Colorjam</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>