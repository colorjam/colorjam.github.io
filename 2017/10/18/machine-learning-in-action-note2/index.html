<!DOCTYPE html>
<html lang=en>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。 假设有i个分类，我们需要比较的其实是后验概率 P(Y=c~k~|X=x) 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于">
<meta property="og:type" content="article">
<meta property="og:title" content="《Machine Learning in Action》学习笔记二：朴素贝叶斯">
<meta property="og:url" content="http://yoursite.com/2017/10/18/machine-learning-in-action-note2/index.html">
<meta property="og:site_name" content="Colorjam">
<meta property="og:description" content="朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。 假设有i个分类，我们需要比较的其实是后验概率 P(Y=c~k~|X=x) 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-11-06T15:16:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Machine Learning in Action》学习笔记二：朴素贝叶斯">
<meta name="twitter:description" content="朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。 假设有i个分类，我们需要比较的其实是后验概率 P(Y=c~k~|X=x) 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>《Machine Learning in Action》学习笔记二：朴素贝叶斯</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

<div id="particles-js"></div>
<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Articles</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2017/10/19/image-classification-note/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2017/10/15/machine-learning-in-action-note1/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&text=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&is_video=false&description=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=《Machine Learning in Action》学习笔记二：朴素贝叶斯&body=Check out this article: http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&name=《Machine Learning in Action》学习笔记二：朴素贝叶斯&description=&lt;p&gt;朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。&lt;/p&gt;
&lt;p&gt;假设有i个分类，我们需要比较的其实是后验概率 &lt;strong&gt;P(Y=c~k~|X=x)&lt;/strong&gt; 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。&lt;/p&gt;
&lt;p&gt;那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：&lt;br&gt;$$&lt;br&gt;P(A|B) = P(A)\frac{P(B|A)}{P(B)}&lt;br&gt;$$&lt;br&gt;让我们来代入一下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}&lt;br&gt;$$&lt;br&gt;给一个训练集，&lt;strong&gt;P(Y=c~i~)&lt;/strong&gt;是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：&lt;br&gt;$$&lt;br&gt;P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)&lt;br&gt;$$&lt;br&gt;由于分母 &lt;strong&gt;P(X=x)&lt;/strong&gt; 对所有c~i~都没差，那我们大可不必计算出这个值。&lt;/p&gt;
&lt;h3 id=&#34;朴素贝叶斯学习与分类的算法过程：&#34;&gt;&lt;a href=&#34;#朴素贝叶斯学习与分类的算法过程：&#34; class=&#34;headerlink&#34; title=&#34;朴素贝叶斯学习与分类的算法过程：&#34;&gt;&lt;/a&gt;朴素贝叶斯学习与分类的算法过程：&lt;/h3&gt;&lt;p&gt;输入：数据集 $T = {(x_1,y_1), (x_2,y_2), … ,(x_n, y_n)}$，其中$x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j$是第 i 个样本的第j个特征；实例x&lt;/p&gt;
&lt;p&gt;输出：实例x的分类&lt;/p&gt;
&lt;p&gt;1) 计算先验概率和条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算&lt;br&gt;$$&lt;br&gt;P(Y=c_k)\prod_{j=1}^{n}P(X_j = x_j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;3) 确定实例x的类&lt;br&gt;$$&lt;br&gt;y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#朴素贝叶斯学习与分类的算法过程："><span class="toc-number">1.</span> <span class="toc-text">朴素贝叶斯学习与分类的算法过程：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一个栗子：邮件分类问题"><span class="toc-number">2.</span> <span class="toc-text">一个栗子：邮件分类问题</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        《Machine Learning in Action》学习笔记二：朴素贝叶斯
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Colorjam</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2017-10-18T00:52:21.000Z" itemprop="datePublished">2017-10-18</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p>
<p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p>
<p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p>
<p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母 <strong>P(X=x)</strong> 对所有c~i~都没差，那我们大可不必计算出这个值。</p>
<h3 id="朴素贝叶斯学习与分类的算法过程："><a href="#朴素贝叶斯学习与分类的算法过程：" class="headerlink" title="朴素贝叶斯学习与分类的算法过程："></a>朴素贝叶斯学习与分类的算法过程：</h3><p>输入：数据集 $T = {(x_1,y_1), (x_2,y_2), … ,(x_n, y_n)}$，其中$x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j$是第 i 个样本的第j个特征；实例x</p>
<p>输出：实例x的分类</p>
<p>1) 计算先验概率和条件概率</p>
<p>$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$</p>
<p>$$<br>P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}<br>$$</p>
<p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p>
<p>2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算<br>$$<br>P(Y=c_k)\prod_{j=1}^{n}P(X_j = x_j|Y=c_k) ,  k=1,2,…K<br>$$</p>
<p>3) 确定实例x的类<br>$$<br>y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p>
<a id="more"></a>
<h3 id="一个栗子：邮件分类问题"><a href="#一个栗子：邮件分类问题" class="headerlink" title="一个栗子：邮件分类问题"></a>一个栗子：邮件分类问题</h3><p>公式都有了，到底要如何实现呢？我们通过邮件分类问题来引入解决办法。</p>
<p>问题：假设有很多个邮件，标记为<strong>1-垃圾邮件</strong>，<strong>0-正常邮件</strong>，给定一个实例判断它属于哪一类。</p>
<p>思路：首先在啥都没有的情况下，我们要处理邮件，将其转化为python可以理解的list，并为出现的所有单词构建一个词汇表，每封邮件对应为词汇表上的一个向量，所有的邮件构成一个矩阵，利用矩阵计算出先验概率和条件概率，将给定实例转化为向量，通过比较大小确定实例的类。</p>
<ol>
<li><p>处理邮件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</div><div class="line">        wordList = textParse(open(<span class="string">'email/spam/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>,<span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList) <span class="comment"># 文档合集</span></div><div class="line">        fullText.extend(wordList) <span class="comment"># 单词合集</span></div><div class="line">        classList.append(<span class="number">1</span>)</div><div class="line">        wordList = textParse(open(<span class="string">'email/ham/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>, <span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">0</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>构建词汇表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></div><div class="line">    vocabSet = set([])</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</div><div class="line">        vocabSet = vocabSet | set(document) <span class="comment"># 不重复的单词</span></div><div class="line">    <span class="keyword">return</span> list(vocabSet)</div><div class="line"> </div><div class="line">vocabList = vocabSet(docList)</div></pre></td></tr></table></figure>
</li>
<li><p>将所有邮件分为训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">trainIndex = list(range(<span class="number">50</span>)); testIndex = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    randIndex = int(random.uniform(<span class="number">0</span>, len(trainIndex)))</div><div class="line">    testIndex.append(trainIndex[randIndex])</div><div class="line">    <span class="keyword">del</span>(trainIndex[randIndex])</div></pre></td></tr></table></figure>
</li>
<li><p>将所有邮件转换为矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></div><div class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabList:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span> <span class="comment"># 转换为词汇表对应向量</span></div><div class="line">    <span class="keyword">return</span>  returnVec</div><div class="line"></div><div class="line">trainMat = []; trainClasses = []</div><div class="line"><span class="keyword">for</span> docIndex <span class="keyword">in</span> trainIndex:</div><div class="line">	trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) </div><div class="line">    trainClasses.append(classList[docIndex])</div></pre></td></tr></table></figure>
</li>
<li><p>计算先验概率和条件概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></div><div class="line">    numTrainDocs = len(trainMatrix)</div><div class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</div><div class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs) <span class="comment"># 先验概率 p(class=1)</span></div><div class="line"></div><div class="line">    <span class="comment"># 1/ Initialize probabilities</span></div><div class="line">    <span class="comment"># 拉普拉斯平滑处理</span></div><div class="line">    p0Num = ones(numWords) <span class="comment"># p(xi|c0)</span></div><div class="line">    p1Num = ones(numWords) <span class="comment"># p(xi|c1)</span></div><div class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</div><div class="line"></div><div class="line">        <span class="comment"># 2 / Vector addition</span></div><div class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</div><div class="line">            p1Num += trainMatrix[i]</div><div class="line">            p1Denom += sum(trainMatrix[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            p0Num += trainMatrix[i]</div><div class="line">            p0Denom += sum(trainMatrix[i])</div><div class="line"></div><div class="line">    <span class="comment"># 3/ Element-wise division</span></div><div class="line">    <span class="comment"># 条件概率</span></div><div class="line">    p1Vect = log(p1Num / p1Denom)</div><div class="line">    p0Vect = log(p0Num / p0Denom)</div><div class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</div><div class="line"></div><div class="line">p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</div></pre></td></tr></table></figure>
<p>让我们把条件概率用普通话稍微翻译一下：<br>$$<br>P(X=x_j|c_k) = \frac{单词x_j出现次数}{c_k类文档总单词数}<br>$$<br>把所有的 $P(X=x_i|c_k)$ 捋到一起就是一个向量 piVcet。</p>
<p>在上面的代码中我们其实处理了两种极端情况：</p>
<ol>
<li>早些时候这个代码是<code>p0Num = zeros(numWords); p0Denom =0</code>（p1同理）。如果某个单词没有出现，当计算乘积 ΠP(X=x~i~|c~k~)时，结果就会为0，这显然就没有办法进行预测了嘛。拉普拉斯就提出用加1的方法估计没有出现过的现象的概率。</li>
<li>还有一个是下溢出问题。在计算乘积  \(\prod P(X=x_i|c_k)\)时，由于概率都是很小的数值，程序有可能下溢出而得不到正确答案。解决办法就是对乘积取自然对数。</li>
</ol>
</li>
<li><p>交叉验证</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></div><div class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</div><div class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1</span>-pClass1)</div><div class="line">    <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    </div><div class="line">    errorCount = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testIndex:</div><div class="line">        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])</div><div class="line">		classifyResult = classifyNB(array(wordVector), p0V, p1V, pSpam)</div><div class="line">	    <span class="keyword">if</span>  classifyResult != classList[docIndex]:</div><div class="line">   			errorCount += <span class="number">1</span></div><div class="line">	print(<span class="string">"the error rate is: "</span>, float(errorCount)/len(testIndex))</div></pre></td></tr></table></figure>
</li>
</ol>
<p>这里我们终于要计算 $prod P(X=x_i|c_k)$ 了。<code>vec2Classify _ piVec</code> 表示先找出实例有的单词，由于\(\ln(a_b) = ln(a)+ln(b)\)，条件概率的相乘便转换为矩阵元素相加。</p>
<p>🙂 适用于量少的数据集／可以处理多类别问题</p>
<p>🙁 对输入数据的准备方式较敏感</p>
<p>🛠 标称型数据</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Articles</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#朴素贝叶斯学习与分类的算法过程："><span class="toc-number">1.</span> <span class="toc-text">朴素贝叶斯学习与分类的算法过程：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一个栗子：邮件分类问题"><span class="toc-number">2.</span> <span class="toc-text">一个栗子：邮件分类问题</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&text=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&is_video=false&description=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=《Machine Learning in Action》学习笔记二：朴素贝叶斯&body=Check out this article: http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&title=《Machine Learning in Action》学习笔记二：朴素贝叶斯"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2017/10/18/machine-learning-in-action-note2/&name=《Machine Learning in Action》学习笔记二：朴素贝叶斯&description=&lt;p&gt;朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。&lt;/p&gt;
&lt;p&gt;假设有i个分类，我们需要比较的其实是后验概率 &lt;strong&gt;P(Y=c~k~|X=x)&lt;/strong&gt; 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。&lt;/p&gt;
&lt;p&gt;那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：&lt;br&gt;$$&lt;br&gt;P(A|B) = P(A)\frac{P(B|A)}{P(B)}&lt;br&gt;$$&lt;br&gt;让我们来代入一下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}&lt;br&gt;$$&lt;br&gt;给一个训练集，&lt;strong&gt;P(Y=c~i~)&lt;/strong&gt;是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：&lt;br&gt;$$&lt;br&gt;P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)&lt;br&gt;$$&lt;br&gt;由于分母 &lt;strong&gt;P(X=x)&lt;/strong&gt; 对所有c~i~都没差，那我们大可不必计算出这个值。&lt;/p&gt;
&lt;h3 id=&#34;朴素贝叶斯学习与分类的算法过程：&#34;&gt;&lt;a href=&#34;#朴素贝叶斯学习与分类的算法过程：&#34; class=&#34;headerlink&#34; title=&#34;朴素贝叶斯学习与分类的算法过程：&#34;&gt;&lt;/a&gt;朴素贝叶斯学习与分类的算法过程：&lt;/h3&gt;&lt;p&gt;输入：数据集 $T = {(x_1,y_1), (x_2,y_2), … ,(x_n, y_n)}$，其中$x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j$是第 i 个样本的第j个特征；实例x&lt;/p&gt;
&lt;p&gt;输出：实例x的分类&lt;/p&gt;
&lt;p&gt;1) 计算先验概率和条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算&lt;br&gt;$$&lt;br&gt;P(Y=c_k)\prod_{j=1}^{n}P(X_j = x_j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;3) 确定实例x的类&lt;br&gt;$$&lt;br&gt;y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Colorjam
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Articles</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- particles -->
<script src="/lib/particles/particles.min.js"></script>
<script type="text/javascript">
    particlesJS('particles-js', {
        "particles": {
            "number": {
            "value": 60,
            "density": {
                "enable": true,
                "value_area": 1200
            }
            },
            "color": {
            "value": "#d25e5e"
            },
            "shape": {
            "type": "circle",
            "stroke": {
                "width": 0,
                "color": "#000000"
            },
            "polygon": {
                "nb_sides": 3
            },
            "image": {
                "src": "img/github.svg",
                "width": 100,
                "height": 100
            }
            },
            "opacity": {
            "value": 0.35,
            "random": false,
            "anim": {
                "enable": false,
                "speed": 1,
                "opacity_min": 0.1,
                "sync": false
            }
            },
            "size": {
            "value": 2,
            "random": true,
            "anim": {
                "enable": false,
                "speed": 40,
                "size_min": 0.1,
                "sync": false
            }
            },
            "line_linked": {
            "enable": true,
            "distance": 150,
            "color": "#e88181",
            "opacity": 0.352750653390415,
            "width": 0.9620472365193136
            },
            "move": {
            "enable": true,
            "speed": 6,
            "direction": "none",
            "random": false,
            "straight": false,
            "out_mode": "out",
            "bounce": false,
            "attract": {
                "enable": false,
                "rotateX": 600,
                "rotateY": 1200
            }
            }
        },
        "interactivity": {
            "detect_on": "canvas",
            "events": {
            "onhover": {
                "enable": true,
                "mode": "repulse"
            },
            "onclick": {
                "enable": true,
                "mode": "push"
            },
            "resize": true
            },
            "modes": {
            "grab": {
                "distance": 400,
                "line_linked": {
                "opacity": 1
                }
            },
            "bubble": {
                "distance": 400,
                "size": 40,
                "duration": 2,
                "opacity": 8,
                "speed": 3
            },
            "repulse": {
                "distance": 200,
                "duration": 0.4
            },
            "push": {
                "particles_nb": 4
            },
            "remove": {
                "particles_nb": 2
            }
            }
        },
        "retina_detect": true
        });
</script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
