<!DOCTYPE html>
<html lang=en>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络方法小结">
<meta property="og:url" content="http://yoursite.com/2018/01/21/neural-nets-note/index.html">
<meta property="og:site_name" content="Colorjam">
<meta property="og:description" content="在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/data_preprocessing1.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/data_preprocessing2.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/svmvssoftmax.png">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/gradientdescent.png">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/opt2.gif">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/loss.png">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/accuracies.jpeg">
<meta property="og:image" content="http://yoursite.com/2018/01/21/neural-nets-note/firstlayer.png">
<meta property="og:updated_time" content="2018-01-23T09:12:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络方法小结">
<meta name="twitter:description" content="在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。">
<meta name="twitter:image" content="http://yoursite.com/2018/01/21/neural-nets-note/data_preprocessing1.jpeg">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>神经网络方法小结</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>


<div id="particles-js"></div>
<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/01/23/cnn/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2018/01/19/FullyConnectedNets-train/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2018/01/21/neural-nets-note/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2018/01/21/neural-nets-note/&text=神经网络方法小结"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2018/01/21/neural-nets-note/&is_video=false&description=神经网络方法小结"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=神经网络方法小结&body=Check out this article: http://yoursite.com/2018/01/21/neural-nets-note/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2018/01/21/neural-nets-note/&name=神经网络方法小结&description=&lt;p&gt;在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据预处理"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参数初始化"><span class="toc-number">2.</span> <span class="toc-text">参数初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数（Loss-Functions）"><span class="toc-number">3.</span> <span class="toc-text">损失函数（Loss Functions）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化（Regularization）"><span class="toc-number">4.</span> <span class="toc-text">正则化（Regularization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优化（Optimization）"><span class="toc-number">5.</span> <span class="toc-text">最优化（Optimization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#超参数调优"><span class="toc-number">6.</span> <span class="toc-text">超参数调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#检查学习过程"><span class="toc-number">7.</span> <span class="toc-text">检查学习过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型集成"><span class="toc-number">8.</span> <span class="toc-text">模型集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考链接"><span class="toc-number">9.</span> <span class="toc-text">参考链接</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        神经网络方法小结
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Colorjam</span>
      </span>
      
    <div class="postdate">
        <time datetime="2018-01-21T03:02:06.000Z" itemprop="datePublished">2018-01-21</time>
    </div>


      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。</p>
<a id="more"></a>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>机器学习本质上是数据工程，数据的分布对于算法学习过程是有影响的。所以在整个学习开始前，需要对<strong>原始数据</strong>进行预处理。通常的方法有：</p>
<ul>
<li><p><strong>均值减法（Mean subtraction）</strong>：使数据零均值化<code> X -= np.mean(X)</code></p>
</li>
<li><p><strong>归一化（Normalization）</strong>：使数据范围近似相等<code>X /= np.std(X, axis=0)</code></p>
<p><img src="/2018/01/21/neural-nets-note/data_preprocessing1.jpeg" alt=""></p>
</li>
<li><p><strong>主成分分析（Principal Component Analysis，PCA）</strong>：降低数据维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></div><div class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></div><div class="line">U,S,V = np.linalg.svd(cov) <span class="comment"># 奇异值分解</span></div><div class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></div><div class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># 降维</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>白化（Whitening）</strong>：降低数据的冗余度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 除以特征值 </span></div><div class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</div></pre></td></tr></table></figure>
<p><img src="/2018/01/21/neural-nets-note/data_preprocessing2.jpeg" alt=""></p>
</li>
</ul>
<p>❗️在进行预处理的过程中，对所有数据统一处理后再划分训练集/验证集/测试集的做法是<strong>错误的</strong>。正确做法是：先将数据划分为训练集/验证集/测试集，对<strong>训练集</strong>进行操作后，将其运用于验证集和测试集。</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>除了数据X，神经网络还有一些很重要的数值就是<strong>权重W</strong>和<strong>偏置b</strong>。</p>
<p><strong>初始化权重</strong>有以下几种方法：</p>
<ul>
<li><p><strong>小随机数初始化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</div></pre></td></tr></table></figure>
</li>
<li><p><strong>使用1/sqrt(n)校准方差</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = np.random.randn(n) / sqrt(n)</div><div class="line">W = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n) <span class="comment"># ReLU神经元的特殊初始化</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>稀疏初始化（Sparse initialization）</strong></p>
</li>
</ul>
<p><strong>初始化偏置</strong>：通常初始为全0。</p>
<h3 id="损失函数（Loss-Functions）"><a href="#损失函数（Loss-Functions）" class="headerlink" title="损失函数（Loss Functions）"></a>损失函数（Loss Functions）</h3><p><strong>数据损失</strong>是一个有监督学习问题，用于衡量分类算法的预测结果和真实标签结果的一致性。我们要计算所有样本的平均数据损失，向减小这个数值的方向不断努力。可以说这就是深度学习的根本目的了。因此也就产生了一系列计算数据损失的损失函数：</p>
<ul>
<li><p><strong>Multiclass Support Vector Machine loss（SVM）</strong>：SVM分类器的损失函数想要SVM在正确分类的得分比不正确分类的得分来得高，且至少高出一个边界值𝚫。<br>$$<br>L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)<br>$$<br>这里要提一下<strong>折页损失（hinge loss）</strong>，即\(max(0, —)\)函数，它常用于“maximum-margin”的算法。因为SVM分类器就属于这类算法，所以我们用名称SVM来代表这类损失函数。</p>
</li>
<li><p><strong>Softmax</strong>：Softmax分类器将评分值视为每个分类的未归一化的对数概率（\(e^{f_{y_{i}}}\)，把折页损失替换成了<strong>交叉熵损失（cross-entropy loss）</strong>，它所做的就是最小化在估计分类概率和“真实”分布之间的交叉熵：<br>$$<br>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)<br>$$</p>
</li>
</ul>
<p>❗️在实际编程实现Softmax的过程中，中间项 \(e^{f_{y_i}}\) 和 \(\sum_j e^{f_j}\) 因为存在指数函数，数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化的小技巧非常重要。通常将𝐶设为\(\log C = -\max_j f_j\)<br>$$<br>\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}<br>= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}<br>= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}<br>$$<br><img src="/2018/01/21/neural-nets-note/svmvssoftmax.png" alt="svmvssoftmax"></p>
<h3 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h3><p>除了数据损失部分，正则项也是在损失函数很重要的一部分。如果模型太复杂即出现过拟合（overfitting），我们就需要加入一些正则项来解决这个问题。</p>
<ul>
<li><p><strong>L2正则化</strong>：\(R(W) = \sum_k\sum_lW_{k,l}^2\) </p>
</li>
<li><p><strong>L1正则化</strong>：\(R(W) = \sum_k\sum_l|W_{k,l}|\) </p>
</li>
<li><p><strong>最大范式约束（Max norm constraints）</strong>：给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。</p>
</li>
<li><p><strong>随机失活（Dropout）</strong>：让神经元以超参数𝑝的概率被激活或者被设置为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line">U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 随机失活遮罩</span></div><div class="line">H1 *= U1 <span class="comment"># drop!</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>批量归一化（Batch Normalization）</strong>：为了使神经网络的每一层输入数据都满足正态分布，除了一开始的数据预处理，科学家们引入了批量归一化。就是在使用每层的激活函数前，对采样的批数据进行归一化处理。</p>
</li>
</ul>
<p>❗️在开始最优化前我们最好做一些<strong>合理性检查</strong>：</p>
<ul>
<li>寻找特定情况的正确损失值</li>
<li>提高正则化强度时导致损失值变大</li>
<li>对小数据子集过拟合</li>
</ul>
<h3 id="最优化（Optimization）"><a href="#最优化（Optimization）" class="headerlink" title="最优化（Optimization）"></a>最优化（Optimization）</h3><p>机器学习的过程不断修改权重W，使Loss能够减小。最常见的方法就是进行<strong>反向传播（Backpropagation）</strong>。通过微分公式和链式法则计算出<strong>解析梯度</strong>，沿着梯度的负方向更新权重。但解析梯度非常容易算错，所以我们可以计算<strong>数值梯度</strong>来进行<strong>梯度检查</strong>，有两种比较方式：</p>
<ul>
<li><strong>使用中心化公式</strong></li>
</ul>
<p>$$<br>\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in}<br>$$</p>
<ul>
<li><strong>使用相对误差</strong><br>$$<br>\frac{\mid f’_a - f’_n \mid}{\max(\mid f’_a \mid, \mid f’_n \mid)}<br>$$</li>
</ul>
<p>不断迭代计算梯度并更新权重的最优化方法就是<strong>梯度下降法（Gradient descent）</strong>。迭代的思路主要有两种：</p>
<ul>
<li><strong>批量梯度下降（Batch Gradient Descent，BGD）</strong>：每次使用所有数据计算梯度</li>
<li><strong>随机梯度下降（Stochastic Gradient Descent，SGD）</strong>：每次只使用一个数据计算梯度</li>
<li><strong>小批量数据梯度下降（Mini-batch gradient descent）</strong>：每次使用一个小批量数据计算</li>
</ul>
<p>❗️即使SGD在技术上是指每次使用1个数据来计算梯度，但人们会使用SGD来指代小批量数据梯度下降</p>
<p>因为深度学习的参数通常处于高维空间，不同的更新方法会导致不同的优化速度。</p>
<ul>
<li><p><strong>普通更新</strong>：沿着负梯度方向改变参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure>
</li>
<li><p><strong>动量更新</strong>：从物理角度出发，负梯度与质点的加速度是成比例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></div><div class="line">x += v <span class="comment"># 与位置融合</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>Nesterov动量</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">v_prev = v <span class="comment"># 存储备份</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></div></pre></td></tr></table></figure>
<p><img src="/2018/01/21/neural-nets-note/gradientdescent.png" alt="loss"></p>
<p>​</p>
</li>
</ul>
<p>第二类常用的最优化方法是基于牛顿法的。在这些方法中最流行的是<strong>L-BFGS</strong>。但在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p>
<p>上面这些更新方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。因此就有人提出了适应性学习率算法：</p>
<ul>
<li><p><strong>Adagrad</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure>
</li>
<li><p><strong>RMSprop</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure>
</li>
<li><p><strong>Adam</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</div><div class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</div><div class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</div></pre></td></tr></table></figure>
<p><img src="/2018/01/21/neural-nets-note/opt2.gif" alt="loss"></p>
</li>
</ul>
<h3 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h3><p>整个训练过程我们会遇到很多超参数，掌握一些小技巧对于超参数初的始化和调整是必不可少的。</p>
<ul>
<li><strong>超参数范围</strong>：在对数尺度上进行超参数搜索。一个典型的学习率应该看起来是这样：learning_rate = 10 ** uniform(-6, 1)。对于正则化强度，可以采用同样的策略。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。</li>
<li><strong>随机搜索优于网格搜索</strong>：通常，有些超参数比其余的更重要。通过随机搜索，可以更精确地发现那些比较重要的超参数的好数值。</li>
<li><strong>小心边界上的最优值</strong>：假设我们使用learning_rate = 10 ** uniform(-6,1)来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</li>
<li><strong>从粗到细地分阶段搜索</strong>：在实践中，先进行初略范围搜索，让模型训练一个周期就可以了；第二个阶段就是根据好结果出现的地方，缩小范围进行搜索，这时可以让模型运行5个周期；最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</li>
<li><strong>贝叶斯超参数最优化</strong>：主要是研究在超参数空间中更高效的导航算法。</li>
</ul>
<h3 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h3><p>可以通过可视化的方式来检查我们的损失值、精确值等是否符合预期。</p>
<ul>
<li><p><strong>损失值</strong>：损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时，震荡就会最小。</p>
<p><img src="/2018/01/21/neural-nets-note/loss.png" alt="loss"></p>
<p>​</p>
</li>
<li><p><strong>训练集和测试集的准确率</strong>：在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度</p>
<p><img src="/2018/01/21/neural-nets-note/accuracies.jpeg" alt="accuracies"> </p>
</li>
<li><p><strong>权重更新比例</strong>：权重中更新值的数量和全部值的数量之间的比例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></div><div class="line">param_scale = np.linalg.norm(W.ravel())</div><div class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></div><div class="line">update_scale = np.linalg.norm(update.ravel())</div><div class="line">W += update <span class="comment"># 实际更新</span></div><div class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></div></pre></td></tr></table></figure>
</li>
<li><p><strong>每层的激活数据及梯度分布</strong>：可以输出网络中所有层的激活数据和梯度分布的柱状图。如果看到任何奇怪的分布情况，那都不是好兆头。</p>
</li>
<li><p><strong>权重可视化</strong>：如果数据是图像像素数据，可以对第一层权重进行可视化，观察特征分布。</p>
<p><img src="/2018/01/21/neural-nets-note/firstlayer.png" alt="accuracies"></p>
</li>
</ul>
<h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>有时候单个模型并不能满足需求，可以进行模型集成来获得额外的性能提高。进行集成有以下几种方法：</p>
<ul>
<li><strong>同一个模型，不同的初始化</strong>：使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。</li>
<li><strong>在交叉验证中发现最好的模型</strong>：使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。</li>
<li><strong>一个模型设置多个记录点</strong>：如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。</li>
<li><strong>在训练的时候跑参数的平均值</strong>：在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。</li>
</ul>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol>
<li><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-2/</a></p>
</li>
<li><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a></p>
</li>
<li><p><a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">Loss function</a></p>
<p>​</p>
</li>
</ol>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据预处理"><span class="toc-number">1.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参数初始化"><span class="toc-number">2.</span> <span class="toc-text">参数初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数（Loss-Functions）"><span class="toc-number">3.</span> <span class="toc-text">损失函数（Loss Functions）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#正则化（Regularization）"><span class="toc-number">4.</span> <span class="toc-text">正则化（Regularization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优化（Optimization）"><span class="toc-number">5.</span> <span class="toc-text">最优化（Optimization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#超参数调优"><span class="toc-number">6.</span> <span class="toc-text">超参数调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#检查学习过程"><span class="toc-number">7.</span> <span class="toc-text">检查学习过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型集成"><span class="toc-number">8.</span> <span class="toc-text">模型集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考链接"><span class="toc-number">9.</span> <span class="toc-text">参考链接</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2018/01/21/neural-nets-note/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2018/01/21/neural-nets-note/&text=神经网络方法小结"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2018/01/21/neural-nets-note/&is_video=false&description=神经网络方法小结"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=神经网络方法小结&body=Check out this article: http://yoursite.com/2018/01/21/neural-nets-note/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2018/01/21/neural-nets-note/&title=神经网络方法小结"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2018/01/21/neural-nets-note/&name=神经网络方法小结&description=&lt;p&gt;在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2018 Colorjam
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- particles -->
<script src="/lib/particles/particles.min.js"></script>
<script type="text/javascript">
    particlesJS('particles-js', {
        "particles": {
            "number": {
            "value": 60,
            "density": {
                "enable": true,
                "value_area": 1200
            }
            },
            "color": {
            "value": "#d25e5e"
            },
            "shape": {
            "type": "circle",
            "stroke": {
                "width": 0,
                "color": "#000000"
            },
            "polygon": {
                "nb_sides": 3
            },
            "image": {
                "src": "img/github.svg",
                "width": 100,
                "height": 100
            }
            },
            "opacity": {
            "value": 0.35,
            "random": false,
            "anim": {
                "enable": false,
                "speed": 1,
                "opacity_min": 0.1,
                "sync": false
            }
            },
            "size": {
            "value": 2,
            "random": true,
            "anim": {
                "enable": false,
                "speed": 40,
                "size_min": 0.1,
                "sync": false
            }
            },
            "line_linked": {
            "enable": true,
            "distance": 150,
            "color": "#e88181",
            "opacity": 0.352750653390415,
            "width": 0.9620472365193136
            },
            "move": {
            "enable": true,
            "speed": 6,
            "direction": "none",
            "random": false,
            "straight": false,
            "out_mode": "out",
            "bounce": false,
            "attract": {
                "enable": false,
                "rotateX": 600,
                "rotateY": 1200
            }
            }
        },
        "interactivity": {
            "detect_on": "canvas",
            "events": {
            "onhover": {
                "enable": true,
                "mode": "repulse"
            },
            "onclick": {
                "enable": true,
                "mode": "push"
            },
            "resize": true
            },
            "modes": {
            "grab": {
                "distance": 400,
                "line_linked": {
                "opacity": 1
                }
            },
            "bubble": {
                "distance": 400,
                "size": 40,
                "duration": 2,
                "opacity": 8,
                "speed": 3
            },
            "repulse": {
                "distance": 200,
                "duration": 0.4
            },
            "push": {
                "particles_nb": 4
            },
            "remove": {
                "particles_nb": 2
            }
            }
        },
        "retina_detect": true
        });
</script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


