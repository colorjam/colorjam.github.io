<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Colorjam's Blog</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Colorjam's Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/pinkladies" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/01/23/cnn/" class="post-title-link">卷积神经网络学习笔记</a></h2><div class="post-info">Jan 23, 2018</div><div class="post-content"><p>卷积神经网络和普通的神经网络很相似，但是明确表示了输入是图像，并且允许我们编码一些特征，因此就使得提升了网络前向传播的效率，并且大大减少了网络中的参数。下面就让我们一起瞅瞅CNN加入了哪些神奇的东西吧 :)</p></div><a href="/2018/01/23/cnn/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/01/21/neural-nets-note/" class="post-title-link">神经网络方法小结</a></h2><div class="post-info">Jan 21, 2018</div><div class="post-content"><p>在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。</p></div><a href="/2018/01/21/neural-nets-note/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/01/19/FullyConnectedNets-train/" class="post-title-link">训练全连接网络</a></h2><div class="post-info">Jan 19, 2018</div><div class="post-content"></div><a href="/2018/01/19/FullyConnectedNets-train/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/11/10/cs231n-assignment1/" class="post-title-link">cs231n assignment1 学习笔记</a></h2><div class="post-info">Nov 10, 2017</div><div class="post-content"><p>作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 <a href="https://matplotlib.org/faq/osx_framework.html#osxframework-faq" target="_blank" rel="external">Working with Matplotlib on OSX</a>，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。</p>
<h3 id="k-Nearest-Neighbor-kNN-exercise"><a href="#k-Nearest-Neighbor-kNN-exercise" class="headerlink" title="k-Nearest Neighbor (kNN) exercise"></a>k-Nearest Neighbor (kNN) exercise</h3><p>点击「run」执行每个框框，<code>dists = classifier.compute_distances_two_loops(X_test)</code> 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。</p></div><a href="/2017/11/10/cs231n-assignment1/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/11/05/machine-learning-in-action-note5/" class="post-title-link">《Machine Learning in Action》学习笔记五：自适应增强算法</a></h2><div class="post-info">Nov 5, 2017</div><div class="post-content"><p>AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：</p>
<ul>
<li><p>Bagging（bootstrap aggregating）</p>
<p>Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。</p>
</li>
<li><p>Boosting</p>
<p>Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>根据<a href="https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8" target="_blank" rel="external">LightGBM介绍视频</a>对Bossting做一些补充：本质上来说，Boosting的方法都是在训练好一个子模型后，统计一下现有复合模型的拟合情况，从而调节接下来学习任务的setting，使得接下来加入复合模型的子模型符合降低整体loss的目标。</p>
</li>
</ul>
<p>Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。</p></div><a href="/2017/11/05/machine-learning-in-action-note5/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/22/machine-learning-in-action-note4/" class="post-title-link">《Machine Learning in Action》学习笔记四：支持向量机</a></h2><div class="post-info">Oct 22, 2017</div><div class="post-content"><p>支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。</p>
<p>上一章学习的「对数几率函数」中，我们提到了</p>
<blockquote>
<p>利用线性回归模型的预测结果去逼近真实标记的对数几率</p>
</blockquote>
<p>标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。</p>
<p>上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。</p>
<p><img src="/2017/10/22/machine-learning-in-action-note4/svmpic.jpg" alt="svmpic"></p></div><a href="/2017/10/22/machine-learning-in-action-note4/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/20/machine-learning-in-action-note3/" class="post-title-link">《Machine Learning in Action》学习笔记三：对数几率回归</a></h2><div class="post-info">Oct 20, 2017</div><div class="post-content"><p>首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。</p>
<p>「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：<br>$$<br>f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i<br>$$<br>有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>$$<br>\ln y = w^Tx + b<br>$$<br>这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）<br>$$<br>y = g^{-1}(w^Tx + b)<br>$$<br>其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。</p></div><a href="/2017/10/20/machine-learning-in-action-note3/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/19/image-classification-note/" class="post-title-link">image-classification-note</a></h2><div class="post-info">Oct 19, 2017</div><div class="post-content"><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems:"></a>Problems:</h3><ol>
<li>Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.</li>
<li>Viewpoint variation: All pixels change when the camera moves</li>
<li>Illumination: There can be lighting conditions going on in the scene</li>
<li>Deformation: Cats can assume a lot of different, varied poses and positions.</li>
<li>Occlusion: You might only see a part of a cat.</li>
<li>Background Clutter: The foreground of the cat look similar in appearance</li>
<li>Intraclass variation: Cats can come in different shapes and sizes and colors and ages</li>
</ol></div><a href="/2017/10/19/image-classification-note/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/18/machine-learning-in-action-note2/" class="post-title-link">《Machine Learning in Action》学习笔记二：朴素贝叶斯</a></h2><div class="post-info">Oct 18, 2017</div><div class="post-content"><p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p>
<p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p>
<p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p>
<p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母 <strong>P(X=x)</strong> 对所有c~i~都没差，那我们大可不必计算出这个值。</p>
<h3 id="朴素贝叶斯学习与分类的算法过程："><a href="#朴素贝叶斯学习与分类的算法过程：" class="headerlink" title="朴素贝叶斯学习与分类的算法过程："></a>朴素贝叶斯学习与分类的算法过程：</h3><p>输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x</p>
<p>输出：实例x的分类</p>
<p>1) 计算先验概率和条件概率</p>
<p>$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$</p>
<p>$$<br>P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}<br>$$</p>
<p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p>
<p>2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算<br>$$<br>P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p>
<p>3) 确定实例x的类<br>$$<br>y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p></div><a href="/2017/10/18/machine-learning-in-action-note2/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/15/machine-learning-in-action-note1/" class="post-title-link">《Machine Learning in Action》学习笔记一：kNN和决策树</a></h2><div class="post-info">Oct 15, 2017</div><div class="post-content"><p>正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。</p>
<p>算法的一般流程为：收集数据 -&gt; 准备数据 -&gt; 分析数据 -&gt; 训练算法 -&gt; 测试算法 -&gt; 使用算法</p>
<p>第一个笔记本包括kNN分类算法和决策树算法。</p>
<h1 id="kNN分类算法"><a href="#kNN分类算法" class="headerlink" title="kNN分类算法"></a>kNN分类算法</h1><p>kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。</p>
<h3 id="kNN的一般流程"><a href="#kNN的一般流程" class="headerlink" title="kNN的一般流程"></a>kNN的一般流程</h3><ol>
<li>收集数据</li>
<li>准备数据：最好使用结构化数据格式，因为计算距离需要数值。</li>
<li>分析数据</li>
<li>训练算法：此步骤不适用于kNN算法</li>
<li>测试算法：计算错误率</li>
<li>使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。</li>
</ol></div><a href="/2017/10/15/machine-learning-in-action-note1/" class="read-more">...more</a></article></li></ul></main><footer><div class="paginator"><a href="/page/2/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2018 <a href="http://yoursite.com">Colorjam</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>