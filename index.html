<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Colorjam's Blog</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="Colorjam's Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/pinkladies" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/22/machine-learning-in-action-note4/" class="post-title-link">《Machine Learning in Action》学习笔记四：支持向量机</a></h2><div class="post-info">Oct 22, 2017</div><div class="post-content"><p>支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。</p>
<p>上一章学习的「对数几率函数」中，我们提到了</p>
<blockquote>
<p>利用线性回归模型的预测结果去逼近真实标记的对数几率</p>
</blockquote>
<p>标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。</p>
<p>上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。</p>
<img src="/2017/10/22/machine-learning-in-action-note4/svmpic.jpg" alt="svmpic.jpg" title=""></div><a href="/2017/10/22/machine-learning-in-action-note4/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/20/machine-learning-in-action-note3/" class="post-title-link">《Machine Learning in Action》学习笔记三：对数几率回归</a></h2><div class="post-info">Oct 20, 2017</div><div class="post-content"><p>首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。</p>
<p>「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：<br>$$<br>f(x_i) = wx_i+b，使得f(x_i) \approx y_i<br>$$<br>有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>$$<br>\ln y = w^Tx + b<br>$$<br>这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）<br>$$<br>y = g^{-1}(w^Tx + b)<br>$$<br>其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。</p></div><a href="/2017/10/20/machine-learning-in-action-note3/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/19/image-classification-note/" class="post-title-link">image-classification-note</a></h2><div class="post-info">Oct 19, 2017</div><div class="post-content"><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems:"></a>Problems:</h3><ol>
<li>Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.</li>
<li>Viewpoint variation: All pixels change when the camera moves</li>
<li>Illumination: There can be lighting conditions going on in the scene</li>
<li>Deformation: Cats can assume a lot of different, varied poses and positions.</li>
<li>Occlusion: You might only see a part of a cat.</li>
<li>Background Clutter: The foreground of the cat look similar in appearance</li>
<li>Intraclass variation: Cats can come in different shapes and sizes and colors and ages</li>
</ol></div><a href="/2017/10/19/image-classification-note/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/18/machine-learning-in-action-note2/" class="post-title-link">《Machine Learning in Action》学习笔记二：朴素贝叶斯</a></h2><div class="post-info">Oct 18, 2017</div><div class="post-content"><p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p>
<p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p>
<p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p>
<p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c<em>k) = \prod</em>{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母<strong>P(X=x)</strong>对所有c~i~都没差，那我们大可不必计算出这个值。</p>
<p>###朴素贝叶斯学习与分类的算法过程：</p>
<p>输入：数据集T = {(x~1~,y~1~), (x~2~,y~2~), … ,(x~n~, y~n~)}，其中x~i~ = (x~i~^1^, x~i~^2^, … , x~i~^n^)^T^，x~i~^j^ 是第i个样本的第j个特征；实例x</p>
<p>输出：实例x的分类</p>
<ol>
<li><p>计算先验概率和条件概率</p>
</li>
<li><p>$$<br>P(Y=c<em>k) = \frac{\sum</em>{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K<br>$$</p>
</li>
</ol>
<p>$$<br>P(X^j = a_{jl} | Y=c<em>k) = \frac{\sum</em>{i=1}^NI(x<em>i^j = a</em>{jl},y_i=c<em>k)}{\sum</em>{i=1}^NI(y_i)=c_k}<br>$$</p>
<p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p>
<ol>
<li>对于给定实例x=(x^1^, x^2^, … , x^n^)，计算</li>
</ol>
<p>$$<br>P(Y=c<em>k)\prod</em>{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p>
<ol>
<li>确定实例x的类<br>$$<br>y=maxP(Y=c<em>k)\prod</em>{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</li>
</ol></div><a href="/2017/10/18/machine-learning-in-action-note2/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/10/15/machine-learning-in-action-note1/" class="post-title-link">《Machine Learning in Action》学习笔记一：kNN和决策树</a></h2><div class="post-info">Oct 15, 2017</div><div class="post-content"><p>正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。</p>
<p>算法的一般流程为：收集数据 -&gt; 准备数据 -&gt; 分析数据 -&gt; 训练算法 -&gt; 测试算法 -&gt; 使用算法</p>
<p>第一个笔记本包括kNN分类算法和决策树算法。</p>
<h1 id="kNN分类算法"><a href="#kNN分类算法" class="headerlink" title="kNN分类算法"></a>kNN分类算法</h1><p>kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。</p>
<h3 id="kNN的一般流程"><a href="#kNN的一般流程" class="headerlink" title="kNN的一般流程"></a>kNN的一般流程</h3><ol>
<li>收集数据</li>
<li>准备数据：最好使用结构化数据格式，因为计算距离需要数值。</li>
<li>分析数据</li>
<li>训练算法：此步骤不适用于kNN算法</li>
<li>测试算法：计算错误率</li>
<li>使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。</li>
</ol></div><a href="/2017/10/15/machine-learning-in-action-note1/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/07/25/china-map/" class="post-title-link">来生成一个中国地图吧</a></h2><div class="post-info">Jul 25, 2017</div><div class="post-content"><p>第一部分应该是根据作者2012年的文章进行的，第二部分根据作者2016年新的文章，在命令行显示人口密度。</p></div><a href="/2017/07/25/china-map/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/06/20/Tensorflow-test/" class="post-title-link">Tensorflow初体验</a></h2><div class="post-info">Jun 20, 2017</div><div class="post-content"></div><a href="/2017/06/20/Tensorflow-test/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/06/16/machine-learning-ex8/" class="post-title-link">Machine Learning ex8</a></h2><div class="post-info">Jun 16, 2017</div><div class="post-content"><p>以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。</p>
<p>这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。</p></div><a href="/2017/06/16/machine-learning-ex8/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/06/13/machine-learning-ex7/" class="post-title-link">Mmachine Learning ex7</a></h2><div class="post-info">Jun 13, 2017</div><div class="post-content"><p>这一章主要学习了 <strong>K-均值算法</strong> 和 <strong>PCA 算法</strong>，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。</p></div><a href="/2017/06/13/machine-learning-ex7/" class="read-more">...more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2017/06/10/machine-learning-ex6/" class="post-title-link">Machine Learning ex6</a></h2><div class="post-info">Jun 10, 2017</div><div class="post-content"><p>终于来到支持向量机（SVM）了！这一章我们要学习如何使用高斯核SVM来建立一个垃圾邮件分类器。</p></div><a href="/2017/06/10/machine-learning-ex6/" class="read-more">...more</a></article></li></ul></main><footer><div class="paginator"><a href="/page/2/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2017 <a href="http://yoursite.com">Colorjam</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>