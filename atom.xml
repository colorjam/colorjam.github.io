<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-05-24T03:41:39.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>那些年使用GPR的超分辨率重建技术</title>
    <link href="http://yoursite.com/2018/05/24/sr-gpr/"/>
    <id>http://yoursite.com/2018/05/24/sr-gpr/</id>
    <published>2018-05-24T01:06:00.000Z</published>
    <updated>2018-05-24T03:41:39.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Single-Image-Super-Resolution-using-Gaussian-Process-Regression"><a href="#1-Single-Image-Super-Resolution-using-Gaussian-Process-Regression" class="headerlink" title="1.Single Image Super-Resolution using Gaussian Process Regression"></a>1.<a href="https://hhexiy.github.io/docs/papers/srgpr.pdf" target="_blank" rel="external">Single Image Super-Resolution using Gaussian Process Regression</a></h3><p>这是最早将GPR引入超分辨率重建问题的论文。令 $y$ 表示一个观测值，$\epsilon$ 为高斯噪声，高斯过程回归模型表示如下：<br>$$<br>y = f(x) + \epsilon, \epsilon \sim \mathcal{N}(n, \sigma^2_n)<br>$$<br>均值函数为零的观测值 $\mathbf { y }$ 和输出 $f_*$ 的联合分布为：<br>$$<br>\left[\begin{array}{c} \mathbf { y } \\ f_*\end{array}\right] \sim \mathcal{N}\left( \begin{array}{cc}0, \left[\begin{array}{cc} K_y \triangleq K(X,X)+\sigma^2_nI &amp; K(X,X_*)\\K(X_*,X) &amp; K(X_*,X_*)\end{array} \right]\end{array} \right)<br>$$<br>其中$X$代表训练集，$X_*$代表测试集。我们可以推导出条件分布：<br>$$<br>f_*|X,y,X_* \sim \mathcal{N}(\bar{f}_*, V(f_*))<br>$$<br>其中，<br>$$<br>\bar{f}_* = K(X_*, X)K_y^{-1}\mathbf { y } = K(X_*, X)\alpha<br>$$</p><p>$$<br>V(f_*) = K(X_*, X_*) -K(X_*, X)K_y^{-1}K(X,X_*)<br>$$</p><p>下面3篇文章可以说是一脉相承，毕竟是出自同一个作者，就来学习一下如何可以新(shui)突(lun)破(wen)吧～</p><h3 id="2-Single-Image-Super-Resolution-Using-Active-Sampling-Gaussian-Process-Regression"><a href="#2-Single-Image-Super-Resolution-Using-Active-Sampling-Gaussian-Process-Regression" class="headerlink" title="2.Single-Image Super-Resolution Using Active-Sampling Gaussian Process Regression"></a>2.<a href="https://ieeexplore.ieee.org/document/7364246/" target="_blank" rel="external">Single-Image Super-Resolution Using Active-Sampling Gaussian Process Regression</a></h3><p>本文使用了<strong>主动采样（Active-samplng）</strong>，即从训练集中提取含有较多信息的图像对构成子集，利用子集构建GPR模型，以减少计算复杂度。</p><p>算法的可视化过程如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frm6jky61bj30uw0fetbj.jpg" alt=""></p><p><strong>输入</strong>包括低分辨率测试图像 $I$，放大倍率 $s$，训练图像对$\left\{ I_q, H_q\right\}_{q=1}^m$。训练图像对包括经过差值放大的$LR$图像和$HR$图像。<strong>输出</strong>为重建结果$S_H$。</p><p><strong>训练阶段：</strong>随机从$\left\{ I_q, H_q\right\}_{q=1}^m$中选择$n$对样本图像$\left\{P_l^{(I)},P_l^{(H)} \right\}_{l=1}^n$，利用中心像素值计算$D \triangleq \left\{ &lt; \mathbf { x } _ { l } ,y _ { l } &gt; \right\} = \left\{ &lt; P _ { l } ^ { ( I ) } ,\operatorname{cen} \left( P _ { l } ^ { ( H ) } \right) - \operatorname{cen} \left( P _ { l } ^ { ( I ) } \right) &gt; \right\}$；使用主动采样获取样本子集 $D ^ { \prime } = \left\{ &lt; P _ { u } ^ { ( I ) } ,y _ { u } &gt; \right\} _ { u = 1} ^ { r }$ ；利用$D ^ { \prime }$训练GPR模型$f$，同时事先计算出投影矩阵$P \triangleq K _ { y } ^ { - 1} y$  。</p><p><strong>测试阶段：</strong>计算LR的差值图像$S_I$，令$S_H=S_I$，循环$S_I$中的所有样本像素计算：<br>$$<br>\begin{align}&amp; y _ { k } ^ { * } = K \left( \mathbf { x } _ { k } ^ { * } ,X \right) * P ,\text{ where } X \triangleq \left\{ P _ { u } ^ { ( I ) } \right\} _ { u = 1} ^ { r } \\  &amp;y _ { k } ^ { * } =  y _ { k } ^ { * }  + cen( x _ { k } ^ { * })  \\&amp;更新对应的S_H的像素值\end{align}<br>$$<br>最后用IBP算法重建出高分辨率图像。</p><p>在进行主动采样时，定义了两个采样标准：代表性（representativeness）和多样性（diversity） 。</p><p>对于一个样本$s_i \triangleq &lt; \mathbf { x } _ { i } ,y _ { i } &gt; \in D$ ，其中$D$为原始训练样本集。 </p><p><strong>代表性</strong>可以表示为：<br>$$<br>R \left( s _ { i } \right) = \frac { 1} { | \mathcal { N } _ { i } | } \sum _ { j \in \mathcal { N } _ { i } } \exp \left( - | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} / 2\sigma _ { R } ^ { 2} \right)<br>$$<br>其中，$\sigma _ { R } ^ { 2}$为高斯核的带宽，可以表示为：<br>$$<br>\sigma _ { R } ^ { 2} = \rho \times \left( \text{ medians } _ { i } ,s _ { j } \in D ,i \neq j | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} \right)<br>$$<br><strong>多样性</strong>可以表示为：<br>$$<br>D \left( \mathbf { s } _ { i } \right) = \min _ { \mathbf { s } _ { j } \in \mathcal { S } } \left[ - \exp \left( - | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} / 2\sigma _ { R } ^ { 2} \right) \right]<br>$$<br>其中$S$为挑选出的信息样本。对于自然图像，边缘和纹理可以表现多样性特性。</p><p>使用凸组合在剩余的候选样本集$C=D-S$中，重复选择出具有较多信息的样本。<br>$$<br>argmax _ { s _ { i } \in \mathcal { C } } \left( \lambda R \left( \mathbf { s } _ { i } \right) + ( 1- \lambda ) D \left( \mathbf { s } _ { i } \right) \right)<br>$$</p><p>## </p><h2 id="3-Fast-single-image-super-resolution-using-sparse-Gaussian-process-regression"><a href="#3-Fast-single-image-super-resolution-using-sparse-Gaussian-process-regression" class="headerlink" title="3.Fast single image super-resolution using sparse Gaussian process regression"></a>3.<a href="https://www.sciencedirect.com/science/article/pii/S0165168416302973" target="_blank" rel="external">Fast single image super-resolution using sparse Gaussian process regression</a></h2><p>本文利用主动采样生成训练子集，使用稀疏高斯过程框架<a href="https://pdfs.semanticscholar.org/01ae/ae41bb2d49740826b06bc8668372c3fe778d.pdf" target="_blank" rel="external">GPLasso</a>来产生稀疏投影向量。GPLasso降低GPR复杂度主要是通过稀疏均值函数中的变量$\alpha$。GPLasso利用相对熵和L1正则项定义$\alpha$的损失函数：<br>$$<br>Q \left( \alpha ^ { \star } \right) = 2K L \left( p ( f | \mathcal { D } ) | _ { H } ( f ) \right) + \lambda | \alpha ^ { \star } | _ { 1}<br>$$<br>基于最小角回归（LAR）算法来最小化损失函数。</p><h2 id="4-Single-Image-Super-Resolution-Using-Gaussian-Process-Regression-With-Dictionary-Based-Sampling-and-Student-tLikelihood"><a href="#4-Single-Image-Super-Resolution-Using-Gaussian-Process-Regression-With-Dictionary-Based-Sampling-and-Student-tLikelihood" class="headerlink" title="4.Single Image Super-Resolution Using Gaussian Process Regression With Dictionary-Based Sampling and Student- tLikelihood"></a>4.<a href="https://ieeexplore.ieee.org/document/7918590/" target="_blank" rel="external">Single Image Super-Resolution Using Gaussian Process Regression With Dictionary-Based Sampling and Student- tLikelihood</a></h2><h3 id="3-NGPR"><a href="#3-NGPR" class="headerlink" title="3.NGPR"></a>3.NGPR</h3><p><a href="https://www.sciencedirect.com/science/article/pii/S0925231216002332" target="_blank" rel="external">Image super-resolution using non-local Gaussian process regression</a></p><p>本文作者提出了一种非局部的基于GPR模型的NGPR超分辨率重建方法。该方法是在LR层面学习NGPR模型用于细节合成，在HR层面利用学习到的模型预测高频细节。之前也整理过论文啦～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-Single-Image-Super-Resolution-using-Gaussian-Process-Regression&quot;&gt;&lt;a href=&quot;#1-Single-Image-Super-Resolution-using-Gaussian-Process-
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>读论文《Image super-resolution using non-local Gaussian process regression》</title>
    <link href="http://yoursite.com/2018/05/21/paper-ngpr/"/>
    <id>http://yoursite.com/2018/05/21/paper-ngpr/</id>
    <published>2018-05-21T03:02:06.000Z</published>
    <updated>2018-05-24T02:08:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>大部分使用GPR的SR方法是局部采样的，这些方法将图像分成几个固定大小（e.g  $45\times45$）并且有一小部分重叠的大块，对每个大块，只使用到其中小块的自相似性（e.g $3\times3$）来建立GPR模型，因此很难完全利用到自然图像中的自相似性。本文作者提出了一种非局部的基于GPR模型的NGPR超分辨率重建方法。</p><h2 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h2><p>该系统包括两个阶段：</p><ol><li>学习阶段：在LR层面学习NGPR模型用于细节合成</li><li>预测阶段：在HR层面预测高频细节用于SR估计</li></ol><p>系统流程图如下所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1frj17sdk7qj31180iidqf.jpg" alt=""></p><p>具体地，在学习阶段可以分成两个主要部分：1）生成训练图像对；2）学习NGPR模型。</p><p>首先获取由原始图像 $I$ 进行上采样再下采样生成的辅助差值图像 $I_I$ ，将 $I$ 和  $I_I$ 分别减去均值图像 $I_M$，提取出高频特征 $I’$ 和 $I’_I$ 。其中 $I_M$ 是用 $3\times3$ 的平均过滤器在 $I_I$ 上的过滤结果。接下来提取出 $I’_I$ 中图像块的邻近像素序列 $\left\{ {x_i}\right\}^n_{i=1}$ ，以及 $I’$ 中对应图像块的中心像素序列 $\left\{ {y_i}\right\}^n_{i=1}$ 作为训练集，用于学习NGPR模型的映射关系。</p><p>在预测阶段，利用插值法获得上采样图像 $S_I$，减去均值图像 $S_M$ 以提取图像的高频特征 $S’_I$ 。利用学习阶段学习出的模型，将 $S’_I$ 中图像块的邻近像素序列 $\left\{ x _ { j * } \right\} _ { j = 1} ^ { m } $ 映射为缺失的高频细节 $\left\{ { f(x_{j*}) } \right\}^m_{j=1}$。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>作者提出了在特征提取过程中，使用到的三个方法。</p><h4 id="1）非局部采样"><a href="#1）非局部采样" class="headerlink" title="1）非局部采样"></a>1）非局部采样</h4><p>在生成训练图像对的时候，采样间隔为4，将训练集大小降低为原来的1/16。在提高重建效率的同时保持重建图像的质量。</p><h4 id="2）块剪枝"><a href="#2）块剪枝" class="headerlink" title="2）块剪枝"></a>2）块剪枝</h4><p>在训练样本中排除了标准差接近0的图像块，避免一些不含有信息的图像块影响学习过程。</p><h4 id="3）块标准化"><a href="#3）块标准化" class="headerlink" title="3）块标准化"></a>3）块标准化</h4><p>在训练前对训练图像对进行标准化处理：<br>$$<br>&lt; x _ { i } ,y _ { i } &gt; \leftarrow &lt; x _ { i } / || x _ { i } || ,y _ { i } / || y _ { i } || &gt; ,\quad \forall i = 1,2,\dots ,n<br>$$</p><p>同时也对测试样本$\left\{ x _ { j * } \right\} _ { j = 1} ^ { m } $ 进行标准化处理，并且在预测后进行恢复：</p><p>$$<br>f \left( \mathbf { x } _ { j * } \right) \leftarrow f \left( \mathbf { x } _ { j * } \right) \times || \mathbf { x } _ { j * } || ,\quad \forall j = 1,2,\dots ,m<br>$$</p><h2 id="NGPR模型"><a href="#NGPR模型" class="headerlink" title="NGPR模型"></a>NGPR模型</h2><p>作者提到了三个核函数：</p><h4 id="1-径向基函数核（RBF-kernel）"><a href="#1-径向基函数核（RBF-kernel）" class="headerlink" title="1)  径向基函数核（RBF kernel）"></a>1)  径向基函数核（RBF kernel）</h4><p>$$<br>k _ { S E i so} \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \sigma _ { f } ^ { 2} \exp \left( - \frac { 1} { 2l ^ { 2} } \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right) ^ { T } \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right) \right)<br>$$</p><p>其中，$ \sigma _ { f } ^ { 2}$  代表信号方差， $l$ 代表长度尺度。RBF核是各向同性的，可以用来描述具有较小欧拉距离的样本。但是各向同性会忽视图像块的结构信息，导致协方差矩阵产生线性依赖。</p><h4 id="2）加性测量噪声核（Additive-measurement-noise-kernel）"><a href="#2）加性测量噪声核（Additive-measurement-noise-kernel）" class="headerlink" title="2）加性测量噪声核（Additive measurement noise kernel）"></a>2）加性测量噪声核（Additive measurement noise kernel）</h4><p>$$<br>k _ { N o i s e } \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \sigma _ { n } ^ { 2} \delta \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right)<br>$$</p><p>其中，$\sigma _ { n } ^ { 2}$ 代表噪声方差，$\delta$ 代表克罗内克函数。</p><h4 id="3）线性核（Linear-kernel）"><a href="#3）线性核（Linear-kernel）" class="headerlink" title="3）线性核（Linear kernel）"></a>3）线性核（Linear kernel）</h4><p>$$<br>k _ { L I N } \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \mathbf { x} ^ { T } \mathbf { x} ^ { \prime }<br>$$</p><p>线性核是对称但是各向异性的，可以用来测量结构信息。RBF核的协方差总是大于0，忽略了负协方差也可获得的事实。线性核比RBF核更有利于反向相关的训练样本。因此线性核比RBF核能揭示更内在的结构。</p><p>最后作者将三个核函数进行组合，提出了一个新的核函数：<br>$$<br>k = k_{SEiso} + k_{Noise} + c \times k_{LIN}<br>$$<br>其中 $c$ 是一个常数，用来调整线性核的重要性。</p><h2 id="超参数确定"><a href="#超参数确定" class="headerlink" title="超参数确定"></a>超参数确定</h2><h4 id="1）噪声标准差-sigma-n"><a href="#1）噪声标准差-sigma-n" class="headerlink" title="1）噪声标准差 $\sigma _ { n }$"></a>1）噪声标准差 $\sigma _ { n }$</h4><p>由于 $I’_I$ 可以看作 $I’$ 的估计，那么 $I’-I’_I$ 可以看作噪声的估计。因此我们利用 $I’-I’_I$ 的标准差对 $\sigma _ { n }$进行初始化。<br>$$<br>\sigma _ { n } ^ { 2} = \frac { 1} { R _ { L R } \times C _ { L R } - 1} \sum _ { i = 1} ^ { R _ { 1k } \times C _ { 18} } \left[ \left( I ^ { \prime } ( i ) - I _ { I } ^ { \prime } ( i ) \right) - \left( \overline { I } ^ { \prime } - \overline { I } _ { I } ^ { \prime } \right) \right] ^ { 2}<br>$$</p><h4 id="2）长度尺度-l"><a href="#2）长度尺度-l" class="headerlink" title="2）长度尺度 $l$"></a>2）长度尺度 $l$</h4><p>$l$ 是RBF核的标准差，可以通过输入训练样本之间距离矩阵的标准差来进行初始化。<br>$$<br>l ^ { 2} = \frac { 1} { (R _ { L R } \times C _ { L R })^2 - 1} \sum _ { i, j=1} ^{ { R _ { L R } \times C _ { L R } }}(||P(i)-P(j)|| - m_d)^2<br>$$</p><p>$$<br>m_d = \frac { 1} { (R _ { L R } \times C _ { L R })^2 } \sum _ { i, j=1} ^{ { R _ { L R } \times C _ { L R } }}(||P_E(i)-P_E(j)||)<br>$$</p><h4 id="3）-信号标准差-sigma-f"><a href="#3）-信号标准差-sigma-f" class="headerlink" title="3） 信号标准差 $ \sigma _ { f }$"></a>3） 信号标准差 $ \sigma _ { f }$</h4><p>通过从 $I’$ 中提取的样本的标准差来进行初始化。<br>$$<br>\sigma _ { n } ^ { 2} = \frac { 1} { R _ { L R } \times C _ { L R } - 1} \sum _ { i = 1} ^{ { R _ { L R } \times C _ { L R } }}(I’(i) -\overline { I’ })^2<br>$$</p><h4 id="4-常数-c"><a href="#4-常数-c" class="headerlink" title="4) 常数 $c$"></a>4) 常数 $c$</h4><p>$$<br>c = \tau \sigma_f^2<br>$$</p><p>其中，$R_{LR}$ ，$ C_{LR}$ 分别代表 $I’$ 的行数和列数。$P(i)$ 代表 $I’_I$ 的 $i$ 个块。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;大部分使用GPR的SR方法是局部采样的，这些方法将图像分成几个固定大小（e.g  $45\times45$）并且有一小部分重叠的大块，对每个大块，只使用到其中小块的自相似性（e.g $3\times3$）来建立GPR模型，因此很难完全利用到自然图像中的自相似性。本文作者提出
      
    
    </summary>
    
    
      <category term="SSIR" scheme="http://yoursite.com/tags/SSIR/"/>
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>读论文《Single Image Super-Resolution using Gaussian Process Regression》</title>
    <link href="http://yoursite.com/2018/05/16/paper-ssir-using-gpr/"/>
    <id>http://yoursite.com/2018/05/16/paper-ssir-using-gpr/</id>
    <published>2018-05-16T07:01:06.000Z</published>
    <updated>2018-05-24T02:45:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文的作者提出了利用高斯回归过程来实现超分辨率重建的方法。该方法只需要输入低分辨率图像，而不需要额外的数据集和样本图像。</p><p>作者的灵感来自于自然图像中的结构冗余。两个像素之间的相似性可以定义为它们局部几何结构的差异。一个像素的邻近像素可以表明该像素的局部特征（比如平滑或边缘区域），因此可以预测基于回归的模型。</p><h2 id="高斯过程回归"><a href="#高斯过程回归" class="headerlink" title="高斯过程回归"></a>高斯过程回归</h2><p>高斯过程（GP）定义了函数 $f$ 的分布，$f$ 将输入空间从 𝒳 映射到 ℛ。对于 𝒳 的任意有限子集，其边缘分布 $P(f(x_1), f(x_2)…f(x_n))$ 是一个多元高斯分布，其中 $x$ 代表一个输入向量。通过均值函数 $m(x)$ 和协方差函数 $k(x_i, x_j)$ 我们有：<br>$$<br>f(x) \sim \mathcal{GP}(m(x), k(x_i,x_j))<br>$$<br>其中矩阵 $X​$ 的每一行为输入向量，$f​$ 是函数值的向量，$K(X, X)​$ 代表 $n \times n​$ 的协方差矩阵，其中 $K_(i,j) = k(x_i, x_j)​$</p><p>高斯过程回归（GPR）即我们假设了目标函数为高斯过程的先验分布。令 $y$ 表示一个观测值，$\epsilon$ 为高斯噪声，高斯过程回归模型表示如下：<br>$$<br>y = f(x) + \epsilon, \epsilon \sim \mathcal{N}(n, \sigma^2_n)<br>$$<br>均值函数为零的观测值 $\mathbf { y }$ 和输出 $f_*$ 的联合分布为：<br>$$<br>\left[\begin{array}{c} \mathbf { y } \\ f_*\end{array}\right] \sim \mathcal{N}\left( \begin{array}{cc}0, \left[\begin{array}{cc}K(X,X)+\sigma^2_nI &amp; K(X,X_*)\\K(X_*,X) &amp; K(X_*,X_*)\end{array} \right]\end{array} \right)<br>$$<br>其中$X$代表训练集，$X_*$代表测试集。我们可以推导出条件分布：<br>$$<br>f_*|X,y,X_* \sim \mathcal{N}(\bar{f}_*, V(f_*))<br>$$<br>其中，<br>$$<br>\bar{f}_* = K(X_*, X)[K(X,X) + \sigma^2_nI]^{-1}\mathbf { y }<br>$$</p><p>$$<br>V(f_*) = K(X_*, X_*) -K(X_*, X)[K(X,X) + \sigma^2_nI]^{-1}K(X,X_*)<br>$$</p><h2 id="单帧图像重建"><a href="#单帧图像重建" class="headerlink" title="单帧图像重建"></a>单帧图像重建</h2><p>高分辨率图像块由对应的低分辨率图像块逐像素地预测得出。取 $ 3\times3$ 大小的低分辨率图像块，观测值 $\mathbf { y }$ 为图像块的中心像素，$x$ 是一个8维向量，代表观测值的临近像素。</p><p>高分辨率图像的预测由两个由粗到细的阶段组成。</p><ol><li>上采样阶段：首先利用双线性插值法获得差值后的图像 $H_b$。然后将低分辨率图像 $L$ 分块，利用每个块的中心像素 $\mathbf { y }$ （目标像素）和它的临近像素 $X_{NL}$ 训练GPR模型 $M$ 。将 $H_b$ 中每个像素的临近像素$X_{NH_b}$ 输入 $M$ 计算出预测值 $p_\tilde{H}$。最后通过 $p_\tilde{H}$获得大致的上采样图像 $\tilde{H}$。</li><li>去模糊阶段：计算 $\tilde{H}$  的下采样图像 $\tilde{L}$ ，并对其进行分块（同上采样阶段 $L$ 的分块操作）。利用每个块的目标像素 $\mathbf { y }$ 和与其对应的 $\tilde{L}$ 中的临近像素 $X_{N\tilde{L}}$ 训练GPR模型 $M$。将 $\tilde{H}$ 中每个像素的临近像素$X_{N\tilde{H}}$ 输入 $M$ 计算出预测值 $p_H$。最后通过 $ p_H$获得最终的重建图像 $H$ 。</li></ol><p>完整的算法描述过程如下：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fre5esj0u5j30l00riaf8.jpg" alt=""></p><h2 id="协方差函数"><a href="#协方差函数" class="headerlink" title="协方差函数"></a>协方差函数</h2><p>在高斯过程回归中，协方差函数通过定义函数的相似性，编码了潜在的预测过程，因此是相当重要的。作者选择了平方指数协方差函数：</p><p>$$<br>k(x_i, x_j) = \sigma_f^2 exp(-\frac{1}{2}\frac{(x_i-x_j)’(x_i-x_j)}{ℓ^2})<br>$$<br>其中 $\sigma^2_f$ 代表信号方差(signal variance)，$ℓ$ 代表特征长度尺度(characteristic length scale)。由于目标像素周围强度值(intensity values)的差异表明了其所处位置，因此相似性的计算基于两个8维临近像素向量紧张值的欧拉距离。</p><p>通过可视化协方差矩阵 $K$， 作者表示平方指数协方差函数能很好地捕捉到图像块之间局部的以及全局的相似性。</p><p>在这个过程中我们主要需要学习的参数有：</p><ol><li>信号方差 $\sigma^2_f$</li><li>典型长度倍数 $ℓ$ ：可以看作是控制一个级别 $u$ 在一个1维高斯过程中向上交叉数量的参数</li><li>噪声方差 $\sigma^2_n$</li></ol><p>在贝叶斯回归模型中，超参数 $\theta$ 的后验概率可以表示为：<br>$$<br>p ( \mathbf { \theta } | \mathbf { X } ,\mathbf { y } ,\mathcal { H } ) = \frac { p ( \mathbf { y } | \mathbf { X } ,\theta ,\mathcal { H } ) p ( \mathbf { \theta } | \mathcal { H } ) } { \int p ( \mathbf { y } | \mathbf { X } ,\theta ,\mathcal { H } ) p ( \theta | \mathcal { H } ) d \theta }<br>$$<br>但是这个后验概率很难求解。在超分辨率问题中，可以通过最大化边际似然(marginal likelihood)来解决这个问题。在我们的模型中，边际似然可以表示为：<br>$$<br>p ( \mathbf { y } | \mathbf { X } ) = \int p ( \mathbf { y } | \mathbf { f } ,\mathbf { X } ) p ( \mathbf { f } | \mathbf { X } ) d \mathbf { f }<br>$$<br>已知 $\mathbf { y } | \mathbf { f } \sim \mathcal { N } \left( \mathbf { f } ,\sigma _ { n } ^ { 2} I \right)$ 和高斯过程先验分布 $f(x) \sim \mathcal{GP}(m(x), k(x_i,x_j))$，可以将上式转化为<br>$$<br>\log p ( \mathbf { y } | \mathbf { X } ,\theta ) = - \frac { 1} { 2} \mathbf { y } ^ { T } \mathbf { K } _ { y } ^ { - 1} \mathbf { y } - \frac { 1} { 2} \log | \mathbf { K } _ { y } | - \frac { n } { 2} \log 2\pi<br>$$<br>其中$ K _ { y } = K ( \mathbf { X } ,\mathbf { X } ) + \sigma _ { n } ^ { 2} I $，对参数求偏导得到：<br>$$<br>\frac { \partial \mathcal { L } } { \partial \theta _ { i } } = \frac { 1} { 2} \mathbf { y } ^ { T } \mathbf { K } ^ { - 1} \frac { \partial \mathbf { K } } { \partial \theta _ { i } } \mathbf { K } ^ { - 1} \mathbf { y } - \frac { 1} { 2} \operatorname{tr} \left( \mathbf { K } ^ { - 1} \frac { \partial \mathbf { K } } { \partial \theta _ { i } } \right)<br>$$<br>然后我们就可以利用梯度下降法来求解最优参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文的作者提出了利用高斯回归过程来实现超分辨率重建的方法。该方法只需要输入低分辨率图像，而不需要额外的数据集和样本图像。&lt;/p&gt;
&lt;p&gt;作者的灵感来自于自然图像中的结构冗余。两个像素之间的相似性可以定义为它们局部几何结构的差异。一个像素的邻近像素可以表明该像素的局部特征（比
      
    
    </summary>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="SISR" scheme="http://yoursite.com/tags/SISR/"/>
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-gan</title>
    <link href="http://yoursite.com/2018/03/14/cs231n-assignment3-gan/"/>
    <id>http://yoursite.com/2018/03/14/cs231n-assignment3-gan/</id>
    <published>2018-03-14T11:44:48.000Z</published>
    <updated>2018-05-16T02:13:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>最后一份作业来会一会生成式对抗网络（Generative Adversarial Networks, GAN）。网络中有两个模型，生成模型 \(G\)（Generator）和判别模型 \(D\)（Discriminator）。\(D\) 的目标是判断一个图像是真的还是假的，\(G\) 的目标是欺骗 \(D\) 使其相信它生成的图像是真的。 <a href="https://www.msra.cn/zh-cn/news/features/gan-20170511" target="_blank" rel="external">到底什么是生成式对抗网络GAN？</a> 其中以男女朋友做比喻还蛮可爱的。 </p><a id="more"></a><p><img src="/2018/03/14/cs231n-assignment3-gan/model.png" alt="cross-validation"></p><p>整个训练过程其实是 \(G\) 和 \(D\) 之间的博弈，在这个博弈中有两个场景。第一个场景的输入是真实图像 <strong>x</strong>，输出概率 \(D(x)\) 指一个图片是真实图片的概率，在这个场景中 \(D(x)\) 努力接近1。第二场景的输入是噪音 <strong>z</strong>， \(G\) 生成一个新的图像 \(G(z)\) 努力欺骗 \(D\) 使 \(D(G(z))\) 接近1， \(D\) 努力让 \(D(G(z))\) 接近0。 </p><p>在本次作用中我们采用以下的更新规则：</p><ol><li>更新生成模型 (\(G\)) ，最大化判别模型对于生成的图像，作出<strong>错误选择</strong>的概率：<br>$$\underset{G}{\text{maximize}}\;  \mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$</li><li>更新判别模型 (\(D\))，最大化判别模型对于真实和生成的图像，作出<strong>正确选择</strong>的概率：<br>$$\underset{D}{\text{maximize}}\; \mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] + \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$</li></ol><h3 id="Vanilla-GAN"><a href="#Vanilla-GAN" class="headerlink" title="Vanilla GAN"></a>Vanilla GAN</h3><p>图像判别模型的损失函数：</p><p>$$ \ell_D = -\mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] - \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$</p><p>图像生成模型G（Generator）的损失函数：</p><p>$$\ell_G  =  -\mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$</p><p>我们可以通过计算二元交叉熵损失（binary cross entropy loss）来计算logits的对数概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_loss</span><span class="params">(input, target)</span>:</span></div><div class="line">    neg_abs = - input.abs()</div><div class="line">    loss = input.clamp(min=<span class="number">0</span>) - input * target + (<span class="number">1</span> + neg_abs.exp()).log()</div><div class="line">    <span class="keyword">return</span> loss.mean()</div></pre></td></tr></table></figure><p>交叉熵用于衡量两个取值为正数的函数的相似性，取值越小差异越小，因此最小化损失函数相当于最小化交叉熵。</p><p>对应上面的两个场景，<strong>D</strong> 判断真实图片的labels都为1，生成图片的labels都为0，可以得到下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_loss</span><span class="params">(logits_real, logits_fake)</span>:</span></div><div class="line">    N = logits_real.shape[<span class="number">0</span>]</div><div class="line">    real_labels = Variable(torch.ones(N)).type(dtype)</div><div class="line">    fake_labels = Variable(torch.zeros(N)).type(dtype)</div><div class="line">    loss = bce_loss(logits_real, real_labels) + bce_loss(logits_fake, fake_labels)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>上面我们说到 <strong>G</strong> 努力欺骗 <strong>D</strong> 使 <strong>D(G(z))</strong> 接近1，因此要将labels都设置为1，得到下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_loss</span><span class="params">(logits_fake)</span>:</span></div><div class="line">    N = logits_fake.shape[<span class="number">0</span>]</div><div class="line">    fake_labels = Variable(torch.ones(N)).type(dtype)</div><div class="line">    loss = bce_loss(logits_fake, fake_labels)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>最终训练结果：</p><p><img src="/2018/03/14/cs231n-assignment3-gan/vanilla_result.png" alt="cross-validation"></p><h3 id="Least-Square-GAN"><a href="#Least-Square-GAN" class="headerlink" title="Least Square GAN"></a>Least Square GAN</h3><p>图片生成模型G（Generator）的损失函数：</p><p>$$\ell_G  =  \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_generator_loss</span><span class="params">(scores_fake)</span>:</span></div><div class="line">    loss = ((scores_fake<span class="number">-1</span>)**<span class="number">2</span>).mean()</div><div class="line">    <span class="keyword">return</span> loss / <span class="number">2</span></div></pre></td></tr></table></figure><p>图像判别模型的损失函数：<br>$$ \ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_discriminator_loss</span><span class="params">(scores_real, scores_fake)</span>:</span></div><div class="line">    loss = ((scores_real<span class="number">-1</span>)**<span class="number">2</span>).mean() + (scores_fake**<span class="number">2</span>).mean()</div><div class="line">    <span class="keyword">return</span> loss / <span class="number">2</span></div></pre></td></tr></table></figure><p><img src="/2018/03/14/cs231n-assignment3-gan/ls_result.png" alt="cross-validation"></p><p>作业中提出了几个问题</p><blockquote><p>Describe how the visual quality of the samples changes over the course of training. Do you notice anything about the distribution of the samples? How do the results change across different training runs?</p></blockquote><p>可以看到通过训练，生成图像从噪音逐渐变为能辨认出来的数字。生成的图像中似乎1、3、9比较多。</p><h3 id="Deeply-Convolutional-GANs"><a href="#Deeply-Convolutional-GANs" class="headerlink" title="Deeply Convolutional GANs"></a>Deeply Convolutional GANs</h3><p>前面的网络结构都是使用全连接网络，搞事的科学家们加上了几层卷积网络，虽然训练速度很慢，但生成的图片可骗人了。。</p><p><img src="/2018/03/14/cs231n-assignment3-gan/dc_result.png" alt="cross-validation"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最后一份作业来会一会生成式对抗网络（Generative Adversarial Networks, GAN）。网络中有两个模型，生成模型 \(G\)（Generator）和判别模型 \(D\)（Discriminator）。\(D\) 的目标是判断一个图像是真的还是假的，\(G\) 的目标是欺骗 \(D\) 使其相信它生成的图像是真的。 &lt;a href=&quot;https://www.msra.cn/zh-cn/news/features/gan-20170511&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;到底什么是生成式对抗网络GAN？&lt;/a&gt; 其中以男女朋友做比喻还蛮可爱的。 &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-styletransfer</title>
    <link href="http://yoursite.com/2018/03/14/cs231n-assignment3-styletransfer/"/>
    <id>http://yoursite.com/2018/03/14/cs231n-assignment3-styletransfer/</id>
    <published>2018-03-14T01:42:13.000Z</published>
    <updated>2018-04-12T00:55:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>用神经网络基于两张画来生成一副新的画，一张画表现内容，一张画代表风格，科学家的脑洞也是大！作业里就实现了这种技术，下面一起来学习一下吧～</p><p><img src="/2018/03/14/cs231n-assignment3-styletransfer/style_content.png" alt=""></p><a id="more"></a><p>我们知道神经网络的损失函数非常重要，在图像风格转换的技术中，损失函数由三个部分组成：<strong>content loss + style loss + total variation loss</strong></p><h3 id="内容损失（Content-loss）"><a href="#内容损失（Content-loss）" class="headerlink" title="内容损失（Content loss）"></a>内容损失（Content loss）</h3><p>内容损失代表生成图的特征图(feature map)和原图的特征图的差异。我们仅关注网络的一层\(\ell\)，它的特征图为 \(A^\ell \in \mathbb{R}^{1 \times C_\ell \times H_\ell \times W_\ell}\)。我们会将特征图在空间上合并到一个维度，并利用这个变形后的版本。用 \(F^\ell \in \mathbb{R}^{N_\ell \times M_\ell}\) 代表当前图像的特征图，\(P^\ell \in \mathbb{R}^{N_\ell \times M_\ell}\) 代表原图的特征图，其中 \(M_\ell=H_\ell\times W_\ell\)。\(F^\ell\) 和 \(P^\ell\) 的每一行代表特定滤波器的激活向量，是由图像的所有位置卷积而成的。\(w_c\) 是内容损失的权重。</p><p>内容损失用公式表示为：</p><p>\(L_c = w_c \times \sum_{i,j} (F_{ij}^{\ell} - P_{ij}^{\ell})^2\)</p><p>不太理解作业里关于特征图的解释，因此结合之前整理过的CNN学习笔记，说一下自己理解的特征图。我觉得特征图其实就是卷积层的输出，卷积层通过滑动滤波器（filter，即卷积层的参数）来得到特征图。通常会有N个滤波器，每个滤波器的深度都是C。将每个滤波器的输出组合到一起构成的N行矩阵，因此<strong>每一行代表特定滤波器的激活向量</strong>。所以最终的代码很简洁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> content_weight * torch.sum((content_current - content_original).pow(<span class="number">2</span>))</div></pre></td></tr></table></figure><h3 id="风格损失（Style-loss）"><a href="#风格损失（Style-loss）" class="headerlink" title="风格损失（Style loss）"></a>风格损失（Style loss）</h3><p>​首先我们需要计算格拉姆矩阵(Gram matrix) G, 代表每个过滤器之间的相关性。利用上面提到的 \(F^\ell \in \mathbb{R}^ {1 \times C_\ell \times M_\ell} \)。那么 \(G^\ell \in \mathbb{R}^ {1 \times C_\ell \times C_\ell} \)：</p><p>$$G_{ij}^\ell  = \sum_k F^{\ell}_{ik} F^{\ell}_{jk}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(features, normalize=True)</span>:</span></div><div class="line">    N, C, H, W = features.shape</div><div class="line">    reshaped_features = features.view(N, C, <span class="number">-1</span>)</div><div class="line">    gram = reshaped_features.matmul(reshaped_features.transpose(<span class="number">1</span>, <span class="number">2</span>))</div><div class="line">    <span class="keyword">if</span>(normalize):</div><div class="line">        gram /= H * W * C</div><div class="line">    <span class="keyword">return</span> gram</div></pre></td></tr></table></figure><p>❗️计算格拉姆矩阵时应该要利用矩阵乘法，即<strong>torch.matmul()</strong></p><p>假设 \(G^\ell\) 是生成图的特征图的格拉姆矩阵，\(A^\ell\) 是原图的特征图的格拉姆矩阵，那么 \(\ell\) 层的损失函数：</p><p>$$L_s^\ell = w_\ell \sum_{i, j} \left(G^\ell_{ij} - A^\ell_{ij}\right)^2$$</p><p>在实际应用中，我们通常会多计算几个层的风格损失，并把它们加起来：</p><p>$$L_s = \sum_{\ell \in \mathcal{L}} L_s^\ell$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(feats, style_layers, style_targets, style_weights)</span>:</span></div><div class="line">    layers = len(style_layers)</div><div class="line">    loss = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(layers):</div><div class="line">        layer = style_layers[i]</div><div class="line">        loss += torch.sum(style_weights[i] * (gram_matrix(feats[layer]) - style_targets[i]).pow(<span class="number">2</span>))</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>❗️计算每层的loss得到的是一个大小等于G的Tensor，要用<strong>torch.sum()</strong>把它们统统加起来，变成一个数值</p><h3 id="总变分正则化（Total-variation-regularization）"><a href="#总变分正则化（Total-variation-regularization）" class="headerlink" title="总变分正则化（Total-variation regularization）"></a>总变分正则化（Total-variation regularization）</h3><p>为了使图像更加平滑，我们需要加入总变分项，可以通过计算相邻像素对（水平和垂直方向），像素值差异的平方和得到。计算公式如下：</p><p>$L_{tv} = w_t \times \sum_{c=1}^3\sum_{i=1}^{H-1} \sum_{j=1}^{W-1} \left( (x_{i,j+1, c} - x_{i,j,c})^2 + (x_{i+1, j,c} - x_{i,j,c})^2  \right)$</p><p>根据公式我们可以写出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span><span class="params">(img, tv_weight)</span>:</span></div><div class="line">    loss = <span class="number">0.0</span></div><div class="line">    _, _, H, W = img.shape</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H<span class="number">-1</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(W<span class="number">-1</span>):</div><div class="line">            loss += tv_weight*torch.sum((img[:, :, i, j+<span class="number">1</span>] - img[:, :, i, j]).pow(<span class="number">2</span>) + (img[:, :, i+<span class="number">1</span>, j] - img[:, :, i, j]).pow(<span class="number">2</span>))</div><div class="line">        </div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>但是作业要求不用循环，然后我又写不出来，参考了网上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hor = torch.dist(img[:, :, :<span class="number">-1</span>, :], img[:, :, <span class="number">1</span>:, :])**<span class="number">2</span></div><div class="line"><span class="comment"># equivalent to (img[:, :, i+1, j] - img[:, :, i, j]).pow(2)</span></div><div class="line">ver = torch.dist(img[:, :, :, :<span class="number">-1</span>], img[:, :, :, <span class="number">1</span>:])**<span class="number">2</span></div><div class="line"><span class="comment"># equivalent to (img[:, :, i, j+1] - img[:, :, i, j]).pow(2)</span></div><div class="line"><span class="keyword">return</span> tv_weight * (hor + ver)</div></pre></td></tr></table></figure><p>这里的torch.dist()使用的是2范数，相当于差异平方和先根号再平方，理解的基础上倾向于下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hor = torch.sum((img[:, :, :<span class="number">-1</span>, :] - img[:, :, <span class="number">1</span>:, :])**<span class="number">2</span>)</div><div class="line">ver = torch.sum((img[:, :, :, :<span class="number">-1</span>] - img[:, :, :, <span class="number">1</span>:])**<span class="number">2</span>)</div></pre></td></tr></table></figure><p>循环法和向量法的效率还是差距很大的，用循环法测试时需要2.2s，而用向量法测试只要53ms，在后面训练的时候效果就很明显了，循环法基本是跑不动的。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用神经网络基于两张画来生成一副新的画，一张画表现内容，一张画代表风格，科学家的脑洞也是大！作业里就实现了这种技术，下面一起来学习一下吧～&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/03/14/cs231n-assignment3-styletransfer/style_content.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>每位计算机科学家应该了解的浮点计算</title>
    <link href="http://yoursite.com/2018/03/11/floating-point/"/>
    <id>http://yoursite.com/2018/03/11/floating-point/</id>
    <published>2018-03-11T08:27:45.000Z</published>
    <updated>2018-03-11T14:09:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>在cs231n的课程上看到了相关内容，一直想找机会看一看，索性用中文记录下来以便日后复习。</p><p>原文链接：<a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" target="_blank" rel="external">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a></p><a id="more"></a><h3 id="舍入误差（Rounding-error）"><a href="#舍入误差（Rounding-error）" class="headerlink" title="舍入误差（Rounding error）"></a>舍入误差（Rounding error）</h3><p>尽管许多整数计算的结果能够存储在32比特中，但实数的计算结果常常无法精确地用固定数量的比特表示出来。因此浮点计算的结果不得不做一些取舍来装入有限的比特数中，这个取舍得到的近似值和精确值之间的差异就是舍入误差。</p><p><strong>浮点格式</strong></p><p>最常见的浮点表示法由一个基数𝛃（通常是个偶数）和一个精度p构成。假设𝛃=10，p=3，那么0.1可以表示为\( 1.00\times 10^ { - 1} \)，用一个通用公式来表示就是 \(d.dd \dots d \times𝛃^e \) ，\(d.dd\dots d\) 即尾数。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在cs231n的课程上看到了相关内容，一直想找机会看一看，索性用中文记录下来以便日后复习。&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-rnn</title>
    <link href="http://yoursite.com/2018/02/08/cs231n-assignment3-rnn/"/>
    <id>http://yoursite.com/2018/02/08/cs231n-assignment3-rnn/</id>
    <published>2018-02-08T10:41:35.000Z</published>
    <updated>2018-02-09T07:51:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>递归神经网络（Recurrent Neural Network, RNN）是为了处理序列数据。之前学的一些机器学习算法的输入数据之间相互是没有联系的，当下一个数据出现的可能依赖于之前出现过的数据，爱捅幺蛾子的科学家们就想出了RNN。</p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>对比之前学习过的CNN，CNN的输入仅为数据，中间网络层的输入即上一层输出的向量，使用不同的参数计算得到该层的输出向量，最后再使用softmax或svm得到输出值。</p><p>而RNN加入了一个玩意儿叫<strong>状态（state）</strong>，RNN的输入包括序列数据的第一步和初始状态，中间的网络层的输入是时间序列中对应的那一步和上一层的状态，使用<strong>相同的参数</strong>计算得到该层的状态，通过这个状态计算输出。</p><p><img src="/2018/02/08/cs231n-assignment3-rnn/rnn_graph.png" alt=""></p><p>最普通的RNN使用tanh作为激活函数，计算公式如下：</p><p><img src="/2018/02/08/cs231n-assignment3-rnn/rnn_formula.png" alt=""></p><a id="more"></a><p>单步rnn的过程比较简单，tanh的求导需要利用到状态值<br>$$<br>f(x) = tanh = \frac{e^{x}-e^{-x}}{e^x+e^{-x}}<br>$$</p><p>$$<br>tan’h = \frac{(e^x-(-e^{-x}))(e^x+e^{-x})-(e^{x}-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}<br>$$</p><p>所以<br>$$<br>tan’h =1-f^2(x)<br>$$<br>在处理整个序列反向传播的时候出现了问题，在<a href="https://www.reddit.com/r/cs231n/comments/48c7wh/question_about_rnn_backward/" target="_blank" rel="external">Question about RNN backward</a>里明白了我出问题的地方。</p><p>首先python函数中，参数传递的是对象(call by object)，当我们引用数组对象并进行操作时，是会改变这个数组的。因此在进行数值验证的时候，使用到的dh实际上改变了，结果自然不正确。解决方法是使用<code>dh_copy = dh.copy()</code></p><p>其次通过<code>nn_step_backward</code>计算出的dprev_h，需要添加到dh里，即<code>dh[:, t, :] += dprev_h</code>，最后一次的dprev_h即为dh0。</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>完成了RNN最主要的计算接下来需要进行词嵌入啦～</p><p>假设输入2个句子(N)，每个句子有4个单词(T)，词汇表共有5个词语(V)，输入<code>X = [[0, 3, 1, 2], [2, 1, 0, 3]]</code>，词嵌入后的第一个句子：<br>$$<br>\begin{bmatrix}<br>1 &amp;0&amp;0&amp;0&amp;0\\<br>0&amp;0&amp;0&amp;1&amp;0\\<br>0&amp;1&amp;0&amp;0&amp;0\\<br>0&amp;0&amp;1&amp;0&amp;0<br>\end{bmatrix}<br>$$<br>正向传播我首先利用了一个循环来完成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</div><div class="line">    seq[n] = np.zeros((T, V))</div><div class="line">    seq[n][list(range(T)), x] = <span class="number">1</span> <span class="comment">#indexing</span></div><div class="line">    out[n] = seq[n].dot(W)</div></pre></td></tr></table></figure><p>反向传播利用seq进行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dW = np.zeros((V, D))</div><div class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</div><div class="line">    dW += seq[n].T.dot(dout[n, :, :])</div></pre></td></tr></table></figure><p>但是当词汇表很大的时候，将seq保存在cache中，会占用了很大的内存，一定有更好的办法😏</p><p>再仔细研究了一下python的indexing，其实可以同时扩展seq的三个维度，这样就省略了for循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rows = np.array(range(N))[:, np.newaxis] </div><div class="line">seq = np.zeros((N, T, V))</div><div class="line">seq[np.tile(rows, T), [list(range(T))]*N, x] = <span class="number">1</span></div><div class="line">out = seq.dot(W)</div></pre></td></tr></table></figure><p>至于反向传播没有使用到np.add.at，说明代码还不够高效。我们已经假设T=4，V=5，那么写出<code>seq[0].T.dot(dout[0, :, :])</code>一个具体公式（dout当然不可能这么大，我随便写的）：<img src="/2018/02/08/cs231n-assignment3-rnn/word_back1.png" alt=""></p><p>可以看到这个矩阵乘法是在进行行变化，这就是矩阵左乘的变换意义。再来仔细分析一下左边这个伪初等矩阵。<img src="/2018/02/08/cs231n-assignment3-rnn/word_back2.png" alt=""></p><p>它的每一行其实代表一个单词，每一列代表一个时刻，dw的结果即为每个单词在它出现时刻的误差的累加。还是用上面的例子来解释，为方便说明假设下标从1开始。w1在t1时刻出现，dw1就应该加上dout1，w2在t3时刻出现，dw2就应该加上dout3，w5没有出现，就没啥好加了。因此俺们的代码终于可以写出来了！（竟然这么简洁）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">np.add.at(dW, x, dout)</div></pre></td></tr></table></figure><p><strong>参考链接</strong></p><ul><li><a href="https://foofish.net/python-function-args.html" target="_blank" rel="external">Python 函数中，参数是传值，还是传引用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;递归神经网络（Recurrent Neural Network, RNN）是为了处理序列数据。之前学的一些机器学习算法的输入数据之间相互是没有联系的，当下一个数据出现的可能依赖于之前出现过的数据，爱捅幺蛾子的科学家们就想出了RNN。&lt;/p&gt;
&lt;h3 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h3&gt;&lt;p&gt;对比之前学习过的CNN，CNN的输入仅为数据，中间网络层的输入即上一层输出的向量，使用不同的参数计算得到该层的输出向量，最后再使用softmax或svm得到输出值。&lt;/p&gt;
&lt;p&gt;而RNN加入了一个玩意儿叫&lt;strong&gt;状态（state）&lt;/strong&gt;，RNN的输入包括序列数据的第一步和初始状态，中间的网络层的输入是时间序列中对应的那一步和上一层的状态，使用&lt;strong&gt;相同的参数&lt;/strong&gt;计算得到该层的状态，通过这个状态计算输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/02/08/cs231n-assignment3-rnn/rnn_graph.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最普通的RNN使用tanh作为激活函数，计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/02/08/cs231n-assignment3-rnn/rnn_formula.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment2-pytorch</title>
    <link href="http://yoursite.com/2018/01/26/cs231n-assignment2-pytorch/"/>
    <id>http://yoursite.com/2018/01/26/cs231n-assignment2-pytorch/</id>
    <published>2018-01-26T09:12:01.000Z</published>
    <updated>2018-01-28T05:56:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>用pytorch来构建自己的网络啦～记录一下训练过程</p><a id="more"></a><p>先搬运一下作业给的Hints：</p><h3 id="Things-you-should-try"><a href="#Things-you-should-try" class="headerlink" title="Things you should try:"></a>Things you should try:</h3><ul><li><strong>Filter size</strong>: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient</li><li><strong>Number of filters</strong>: Above we used 32 filters. Do more or fewer do better?</li><li><strong>Pooling vs Strided Convolution</strong>: Do you use max pooling or just stride convolutions?</li><li><strong>Batch normalization</strong>: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?</li><li><strong>Network architecture</strong>: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:<ul><li>[conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li><li>[conv-relu-conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li><li>[batchnorm-relu-conv]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li></ul></li><li><strong>Global Average Pooling</strong>: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">Google’s Inception Network</a> (See Table 1 for their architecture).</li><li><strong>Regularization</strong>: Add l2 weight regularization, or perhaps use Dropout.</li></ul><h3 id="Tips-for-training"><a href="#Tips-for-training" class="headerlink" title="Tips for training"></a>Tips for training</h3><p>For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:</p><ul><li>If the parameters are working well, you should see improvement within a few hundred iterations</li><li>Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.</li><li>Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.</li><li>You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.</li></ul><p>看完我的内心是崩溃的，每次构建一个网络都要调节学习率，所以我在<strong>train</strong>函数里设置迭代200次就退出循环，先通过loss大致比较一下不同网络结构的效率，最后再仔细学习～</p><h3 id="Try-1"><a href="#Try-1" class="headerlink" title="Try 1"></a>Try 1</h3><p>首先我将第一个卷积层的filter size改成了3，改变卷积层过滤器的数量。然后看了半天不知道strided convolution是啥找到了一个学习笔记视频☞ <a href="http://www.bilibili.com/video/av15968996/" target="_blank" rel="external">什么是strided convolution? 跳出格怎么处理？</a> 原来strided convolution就是stride不为1的卷积层:) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Strided convolution</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"><span class="comment"># no Max Pooling</span></div><div class="line">...</div><div class="line">nn.Linear(<span class="number">7200</span>, <span class="number">1024</span>), <span class="comment"># affine layer (32-3+1)/2</span></div><div class="line"></div><div class="line"><span class="comment"># Pooling</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</div><div class="line">...</div><div class="line">nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</div><div class="line">...</div><div class="line">nn.Linear(<span class="number">7200</span>, <span class="number">1024</span>), <span class="comment"># affine layer</span></div></pre></td></tr></table></figure><p>设置stride为2刚好最后进入全连接时都是7200个神经元～先大致查找了一下学习率的范围，然后缩小范围learning_rate = [10 ** np.random.uniform(-5, -2) for i in range(20)]分别尝试两个模型的准确率。</p><p>Strided convolution同学的准确率在50%以上的不多，然鹅Pooling同学准确率达到50%的就比较多了，说明使用Pooling的效果比较好。</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/strided_conv.png" alt="cross-validation"></p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/pooling.png" alt="cross-validation"></p><p>猜想：因为Strided convolution多跳了几个格子，可能会遗漏某些图像特征，而Max-pooling过滤出最大的像素值，反而强调了图像特征。</p><h3 id="Try-2"><a href="#Try-2" class="headerlink" title="Try 2"></a>Try 2</h3><p>比较一下过滤器数量为50和20两个模型的效率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">10</span> ** np.random.uniform(<span class="number">-5</span>, <span class="number">-3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</div><div class="line"><span class="comment"># Convolution of 50 filters</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">50</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>) </div><div class="line">...</div><div class="line">nn.Linear(<span class="number">11250</span>, <span class="number">1024</span>) <span class="comment"># (32-3+1)/2 **2 x 50</span></div><div class="line"><span class="comment"># Convolution of 20 filters</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">20</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>) </div><div class="line">...</div><div class="line">nn.Linear(<span class="number">4500</span>, <span class="number">1029</span>) <span class="comment"># (32-3+1)/2 **2 x 20</span></div></pre></td></tr></table></figure><p><img src="/2018/01/26/cs231n-assignment2-pytorch/conv_num_filters.jpg" alt="cross-validation"></p><p>随机了10个学习率，可以看到相对来说还是过滤器比较多的结果比较好，但同时也比较耗时</p><h3 id="Try-3"><a href="#Try-3" class="headerlink" title="Try 3"></a>Try 3</h3><p>使用上面50个过滤器的模型，在第一个卷积层之后、全连接层之后加Batch normalization。</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/batch_norm.png" alt="cross-validation"></p><p>虽然准确率最高的没有上面不使用Batch normalization的高，但是整体水平提升了。但是Hints里貌似是说训练速度变快了？但是俺忘记打印出时间了。。。</p><h3 id="Try-4"><a href="#Try-4" class="headerlink" title="Try 4"></a>Try 4</h3><p>之前总结神经网络的时候就总结过，softmax代表交叉熵损失（cross-entropy loss），SVM是折页损失（hinge loss），这个在pytorch里都有对应的loss函数。接下来就是要改变网络结构啦～</p><p>模型结构：[conv-relu-pool]x2 -&gt; [affine]x2 -&gt; [softmax]（在每层中间都插入了batchnorm）</p><table><thead><tr><th>type</th><th>patch size/stride</th><th>input size</th></tr></thead><tbody><tr><td>conv</td><td>3 x 3 / 1</td><td>32x32x3</td></tr><tr><td>pool</td><td>2 x 2 / 2</td><td>30x30x50</td></tr><tr><td>conv</td><td>3 x 3 / 1</td><td>15x15x50</td></tr><tr><td>pool</td><td>2 x 2 / 2</td><td>13x13x30</td></tr><tr><td>linear</td><td>logits</td><td>1x1x1080</td></tr><tr><td>linear</td><td>logits</td><td>1x1x500</td></tr><tr><td>softmax</td><td>classifier</td><td>1x1x10</td></tr></tbody></table><p>搜索一下合适的学习率～</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/network1.png" alt="cross-validation"></p><p>可以看到最好的准确率接近0.6啦，撒花～现在用学习率训练1个epoch试试</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/network1_epoch1.png" alt="cross-validation"></p><p>训练5个epoch～</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/network1_epoch5.png" alt="cross-validation"></p><p>准确率达到70%啦，撒花~</p><h3 id="Try-5"><a href="#Try-5" class="headerlink" title="Try 5"></a>Try 5</h3><p>现在我们再改变一下网络结构，尝试一下<strong>整体平均汇聚</strong>，也就是不在中间使用多个全链接层，而是将图片训练得足够小大约（7, 7），在最后使用一个汇聚层使得图像变成 (1, 1 , Filter#)。</p><p>模型结构：[conv-relul]x4 -&gt; [pool]x1 -&gt; [softmax]</p><table><thead><tr><th>type</th><th>patch size/stride</th><th>input size</th></tr></thead><tbody><tr><td>conv</td><td>3 x 3 / 1</td><td>32x32x3</td></tr><tr><td>conv</td><td>3 x 3 / 2</td><td>30x30x32</td></tr><tr><td>conv</td><td>3 x 3 / 1</td><td>14x14x64</td></tr><tr><td>conv</td><td>3 x 3 / 2</td><td>12x12x80</td></tr><tr><td>pool</td><td>5 x 5</td><td>5x5x100</td></tr><tr><td>linear</td><td>logits</td><td>1x1x100</td></tr><tr><td>softmax</td><td>classifier</td><td>1x1x10</td></tr></tbody></table><p>尝试了几个迭代，在一开始测试随机学习率的时候都不如上面一个模型，决定再换一个结构～</p><h3 id="Try-5-1"><a href="#Try-5-1" class="headerlink" title="Try 5"></a>Try 5</h3><p>[batchnorm-relu-conv]x7 -&gt; [pool]x1 -&gt; [softmax]</p><p>太久了。。放弃。。我决定回到Try4的模型多来几层。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">    </div><div class="line">model1 = nn.Sequential( </div><div class="line">                    nn.Conv2d(<span class="number">3</span>, <span class="number">50</span>, kernel_size=<span class="number">3</span>, stride = <span class="number">1</span>), <span class="comment"># input: 32x32x3</span></div><div class="line">                    nn.BatchNorm2d(<span class="number">50</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.BatchNorm2d(<span class="number">50</span>),</div><div class="line">                    nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># input: 28x28x32</span></div><div class="line">                    </div><div class="line">                    nn.Conv2d(<span class="number">50</span>, <span class="number">80</span>, kernel_size=<span class="number">3</span>, stride = <span class="number">1</span>), <span class="comment"># input: 14x14x32</span></div><div class="line">                    nn.BatchNorm2d(<span class="number">80</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.BatchNorm2d(<span class="number">80</span>),</div><div class="line">                    nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 8x8x150</span></div><div class="line">    </div><div class="line">                    Flatten(), </div><div class="line">                    nn.Linear(<span class="number">2880</span>, <span class="number">1000</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.Linear(<span class="number">1000</span>,<span class="number">10</span>)</div><div class="line">            )</div></pre></td></tr></table></figure><p><img src="/2018/01/26/cs231n-assignment2-pytorch/val_accr.png" alt="cross-validation"></p><p>经过10个epoch，loss已经挺小的了，而且达到了74%的准确率，现在在测试集上尝试</p><p><img src="/2018/01/26/cs231n-assignment2-pytorch/test_accr.png" alt="cross-validation"></p><p>完成啦～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用pytorch来构建自己的网络啦～记录一下训练过程&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>卷积神经网络学习笔记</title>
    <link href="http://yoursite.com/2018/01/23/cnn/"/>
    <id>http://yoursite.com/2018/01/23/cnn/</id>
    <published>2018-01-23T06:53:12.000Z</published>
    <updated>2018-01-24T13:03:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络和普通的神经网络很相似，但是明确表示了输入是图像，并且允许我们编码一些特征，因此就使得提升了网络前向传播的效率，并且大大减少了网络中的参数。下面就让我们一起瞅瞅CNN加入了哪些神奇的东西吧 :)</p><a id="more"></a><blockquote><p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p></blockquote><p>上面这句话透露了很多有意思的东西，包括<strong>3D volume</strong>、<strong>Layers</strong>、<strong>Parameters</strong>，让我们一个个来说说～</p><h3 id="3D-volume"><a href="#3D-volume" class="headerlink" title="3D volume"></a>3D volume</h3><p>普通的神经网络将输入图像排列成一个向量，通过和对应数量的参数点乘获得预测结果：</p><p><img src="/2018/01/23/cnn/fc_layer.png" alt=""></p><p>但CNN保留了输入图像的空间结构，将其看作是3维（<strong>width, height, depth</strong>）结构的，并利用较少的参数过滤映射出下一层：</p><p><img src="/2018/01/23/cnn/cn_layer.png" alt=""></p><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>我们主要使用三种类型的层来构建卷积神经网络，分别是：<strong>卷积层（Convolutional Layer）</strong>、<strong>汇聚层（ Pooling Layer）</strong>、和<strong>全连接层（Fully-Connected Layer）</strong>。</p><h4 id="卷积层（Convolutional-Layer）"><a href="#卷积层（Convolutional-Layer）" class="headerlink" title="卷积层（Convolutional Layer）"></a>卷积层（Convolutional Layer）</h4><p>卷积层是构建卷积神经网络的核心层。卷积层的参数是由一些可学习的滤波器集合（参数）构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。</p><p><strong>局部连接</strong>：我们已经知道全连接需要与对应数量的参数点乘，但这对于大尺寸的图像来说运算量是非常大的。因此我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的<strong>感受野（receptive field）</strong>。它的尺寸其实就是滤波器的空间尺寸。</p><p>❗️我们让在滤波器在宽度和高度上滑动，但总是让滤波器的深度=输入数据深度。</p><p><strong>空间排列</strong>：有了输入和参数，我们应该就可以得到输出了。但这里还要提一下3个控制输出数据尺寸的超参数：<strong>深度（depth），步长（stride）</strong>和<strong>零填充（zero-padding）</strong>。</p><ol><li><p>此深度非彼深度。上面我们提到了输入数据的深度，现在这个输出数据的深度其实是使用的<strong>滤波器数量</strong>。其中的每一个输出数据称作<strong>深度切片（depth slice）</strong>。</p><p><img src="/2018/01/23/cnn/maps.png" alt=""></p></li><li><p>在滑动滤波器的时候我们要指定每次移动多少个像素，也就是步长。</p><p>假设使用3x3滤波器在[7x7]的输入数据上滑动 =&gt; [5x5]的输出数据，而使用2个步长滑动，会让输出数据在空间上变小 =&gt; [3x3]的输出数据</p></li><li><p>有时候会用0在输入数据体的边缘处进行填充，可以让输出数据与输入数据空间保持一致。</p><p><img src="/2018/01/23/cnn/zero_padding.png" alt=""></p></li></ol><p>我们就可以通过输入数据尺寸 \( W_1 \times H_1 \times D_1 \)、滤波器尺寸𝑭、步长𝑺 和零填充的数量𝑷，计算出输出数据的空间尺寸：\( (W - F + 2P) / S + 1 \)</p><h4 id="汇聚层（-Pooling-Layer）"><a href="#汇聚层（-Pooling-Layer）" class="headerlink" title="汇聚层（ Pooling Layer）"></a>汇聚层（ Pooling Layer）</h4><p>汇聚层的作用是降低数据的空间尺寸，减少网络中参数的数量。汇聚层其实就是对输入数据的每一个深度切片进行降采样，改变它的空间尺寸。最常用的是MAX操作。</p><p><img src="/2018/01/23/cnn/POOLING.png" alt=""></p><h4 id="全连接层（Fully-Connected-Layer）"><a href="#全连接层（Fully-Connected-Layer）" class="headerlink" title="全连接层（Fully Connected Layer）"></a>全连接层（Fully Connected Layer）</h4><p>这个其实就没什么好说的啦，和普通的神经网络是一样的</p><p><strong>参考链接</strong></p><ol><li><a href="https://www.quora.com/What-is-a-CNN%E2%80%99s-receptive-field" target="_blank" rel="external">https://www.quora.com/What-is-a-CNN%E2%80%99s-receptive-field</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积神经网络和普通的神经网络很相似，但是明确表示了输入是图像，并且允许我们编码一些特征，因此就使得提升了网络前向传播的效率，并且大大减少了网络中的参数。下面就让我们一起瞅瞅CNN加入了哪些神奇的东西吧 :)&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络方法小结</title>
    <link href="http://yoursite.com/2018/01/21/neural-nets-note/"/>
    <id>http://yoursite.com/2018/01/21/neural-nets-note/</id>
    <published>2018-01-21T03:02:06.000Z</published>
    <updated>2018-01-23T09:12:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。</p><a id="more"></a><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>机器学习本质上是数据工程，数据的分布对于算法学习过程是有影响的。所以在整个学习开始前，需要对<strong>原始数据</strong>进行预处理。通常的方法有：</p><ul><li><p><strong>均值减法（Mean subtraction）</strong>：使数据零均值化<code> X -= np.mean(X)</code></p></li><li><p><strong>归一化（Normalization）</strong>：使数据范围近似相等<code>X /= np.std(X, axis=0)</code></p><p><img src="/2018/01/21/neural-nets-note/data_preprocessing1.jpeg" alt=""></p></li><li><p><strong>主成分分析（Principal Component Analysis，PCA）</strong>：降低数据维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></div><div class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></div><div class="line">U,S,V = np.linalg.svd(cov) <span class="comment"># 奇异值分解</span></div><div class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></div><div class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># 降维</span></div></pre></td></tr></table></figure></li><li><p><strong>白化（Whitening）</strong>：降低数据的冗余度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 除以特征值 </span></div><div class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/data_preprocessing2.jpeg" alt=""></p></li></ul><p>❗️在进行预处理的过程中，对所有数据统一处理后再划分训练集/验证集/测试集的做法是<strong>错误的</strong>。正确做法是：先将数据划分为训练集/验证集/测试集，对<strong>训练集</strong>进行操作后，将其运用于验证集和测试集。</p><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>除了数据X，神经网络还有一些很重要的数值就是<strong>权重W</strong>和<strong>偏置b</strong>。</p><p><strong>初始化权重</strong>有以下几种方法：</p><ul><li><p><strong>小随机数初始化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</div></pre></td></tr></table></figure></li><li><p><strong>使用1/sqrt(n)校准方差</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = np.random.randn(n) / sqrt(n)</div><div class="line">W = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n) <span class="comment"># ReLU神经元的特殊初始化</span></div></pre></td></tr></table></figure></li><li><p><strong>稀疏初始化（Sparse initialization）</strong></p></li></ul><p><strong>初始化偏置</strong>：通常初始为全0。</p><h3 id="损失函数（Loss-Functions）"><a href="#损失函数（Loss-Functions）" class="headerlink" title="损失函数（Loss Functions）"></a>损失函数（Loss Functions）</h3><p><strong>数据损失</strong>是一个有监督学习问题，用于衡量分类算法的预测结果和真实标签结果的一致性。我们要计算所有样本的平均数据损失，向减小这个数值的方向不断努力。可以说这就是深度学习的根本目的了。因此也就产生了一系列计算数据损失的损失函数：</p><ul><li><p><strong>Multiclass Support Vector Machine loss（SVM）</strong>：SVM分类器的损失函数想要SVM在正确分类的得分比不正确分类的得分来得高，且至少高出一个边界值𝚫。<br>$$<br>L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)<br>$$<br>这里要提一下<strong>折页损失（hinge loss）</strong>，即\(max(0, —)\)函数，它常用于“maximum-margin”的算法。因为SVM分类器就属于这类算法，所以我们用名称SVM来代表这类损失函数。</p></li><li><p><strong>Softmax</strong>：Softmax分类器将评分值视为每个分类的未归一化的对数概率（\(e^{f_{y_{i}}}\)，把折页损失替换成了<strong>交叉熵损失（cross-entropy loss）</strong>，它所做的就是最小化在估计分类概率和“真实”分布之间的交叉熵：<br>$$<br>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)<br>$$</p></li></ul><p>❗️在实际编程实现Softmax的过程中，中间项 \(e^{f_{y_i}}\) 和 \(\sum_j e^{f_j}\) 因为存在指数函数，数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化的小技巧非常重要。通常将𝐶设为\(\log C = -\max_j f_j\)<br>$$<br>\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}<br>= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}<br>= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}<br>$$<br><img src="/2018/01/21/neural-nets-note/svmvssoftmax.png" alt="svmvssoftmax"></p><h3 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h3><p>除了数据损失部分，正则项也是在损失函数很重要的一部分。如果模型太复杂即出现过拟合（overfitting），我们就需要加入一些正则项来解决这个问题。</p><ul><li><p><strong>L2正则化</strong>：\(R(W) = \sum_k\sum_lW_{k,l}^2\) </p></li><li><p><strong>L1正则化</strong>：\(R(W) = \sum_k\sum_l|W_{k,l}|\) </p></li><li><p><strong>最大范式约束（Max norm constraints）</strong>：给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。</p></li><li><p><strong>随机失活（Dropout）</strong>：让神经元以超参数𝑝的概率被激活或者被设置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line">U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 随机失活遮罩</span></div><div class="line">H1 *= U1 <span class="comment"># drop!</span></div></pre></td></tr></table></figure></li><li><p><strong>批量归一化（Batch Normalization）</strong>：为了使神经网络的每一层输入数据都满足正态分布，除了一开始的数据预处理，科学家们引入了批量归一化。就是在使用每层的激活函数前，对采样的批数据进行归一化处理。</p></li></ul><p>❗️在开始最优化前我们最好做一些<strong>合理性检查</strong>：</p><ul><li>寻找特定情况的正确损失值</li><li>提高正则化强度时导致损失值变大</li><li>对小数据子集过拟合</li></ul><h3 id="最优化（Optimization）"><a href="#最优化（Optimization）" class="headerlink" title="最优化（Optimization）"></a>最优化（Optimization）</h3><p>机器学习的过程不断修改权重W，使Loss能够减小。最常见的方法就是进行<strong>反向传播（Backpropagation）</strong>。通过微分公式和链式法则计算出<strong>解析梯度</strong>，沿着梯度的负方向更新权重。但解析梯度非常容易算错，所以我们可以计算<strong>数值梯度</strong>来进行<strong>梯度检查</strong>，有两种比较方式：</p><ul><li><strong>使用中心化公式</strong></li></ul><p>$$<br>\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in}<br>$$</p><ul><li><strong>使用相对误差</strong><br>$$<br>\frac{\mid f’_a - f’_n \mid}{\max(\mid f’_a \mid, \mid f’_n \mid)}<br>$$</li></ul><p>不断迭代计算梯度并更新权重的最优化方法就是<strong>梯度下降法（Gradient descent）</strong>。迭代的思路主要有两种：</p><ul><li><strong>批量梯度下降（Batch Gradient Descent，BGD）</strong>：每次使用所有数据计算梯度</li><li><strong>随机梯度下降（Stochastic Gradient Descent，SGD）</strong>：每次只使用一个数据计算梯度</li><li><strong>小批量数据梯度下降（Mini-batch gradient descent）</strong>：每次使用一个小批量数据计算</li></ul><p>❗️即使SGD在技术上是指每次使用1个数据来计算梯度，但人们会使用SGD来指代小批量数据梯度下降</p><p>因为深度学习的参数通常处于高维空间，不同的更新方法会导致不同的优化速度。</p><ul><li><p><strong>普通更新</strong>：沿着负梯度方向改变参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure></li><li><p><strong>动量更新</strong>：从物理角度出发，负梯度与质点的加速度是成比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></div><div class="line">x += v <span class="comment"># 与位置融合</span></div></pre></td></tr></table></figure></li><li><p><strong>Nesterov动量</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">v_prev = v <span class="comment"># 存储备份</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/gradientdescent.png" alt="loss"></p><p>​</p></li></ul><p>第二类常用的最优化方法是基于牛顿法的。在这些方法中最流行的是<strong>L-BFGS</strong>。但在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p><p>上面这些更新方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。因此就有人提出了适应性学习率算法：</p><ul><li><p><strong>Adagrad</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></li><li><p><strong>RMSprop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></li><li><p><strong>Adam</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</div><div class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</div><div class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/opt2.gif" alt="loss"></p></li></ul><h3 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h3><p>整个训练过程我们会遇到很多超参数，掌握一些小技巧对于超参数初的始化和调整是必不可少的。</p><ul><li><strong>超参数范围</strong>：在对数尺度上进行超参数搜索。一个典型的学习率应该看起来是这样：learning_rate = 10 ** uniform(-6, 1)。对于正则化强度，可以采用同样的策略。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。</li><li><strong>随机搜索优于网格搜索</strong>：通常，有些超参数比其余的更重要。通过随机搜索，可以更精确地发现那些比较重要的超参数的好数值。</li><li><strong>小心边界上的最优值</strong>：假设我们使用learning_rate = 10 ** uniform(-6,1)来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</li><li><strong>从粗到细地分阶段搜索</strong>：在实践中，先进行初略范围搜索，让模型训练一个周期就可以了；第二个阶段就是根据好结果出现的地方，缩小范围进行搜索，这时可以让模型运行5个周期；最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</li><li><strong>贝叶斯超参数最优化</strong>：主要是研究在超参数空间中更高效的导航算法。</li></ul><h3 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h3><p>可以通过可视化的方式来检查我们的损失值、精确值等是否符合预期。</p><ul><li><p><strong>损失值</strong>：损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时，震荡就会最小。</p><p><img src="/2018/01/21/neural-nets-note/loss.png" alt="loss"></p><p>​</p></li><li><p><strong>训练集和测试集的准确率</strong>：在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度</p><p><img src="/2018/01/21/neural-nets-note/accuracies.jpeg" alt="accuracies"> </p></li><li><p><strong>权重更新比例</strong>：权重中更新值的数量和全部值的数量之间的比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></div><div class="line">param_scale = np.linalg.norm(W.ravel())</div><div class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></div><div class="line">update_scale = np.linalg.norm(update.ravel())</div><div class="line">W += update <span class="comment"># 实际更新</span></div><div class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></div></pre></td></tr></table></figure></li><li><p><strong>每层的激活数据及梯度分布</strong>：可以输出网络中所有层的激活数据和梯度分布的柱状图。如果看到任何奇怪的分布情况，那都不是好兆头。</p></li><li><p><strong>权重可视化</strong>：如果数据是图像像素数据，可以对第一层权重进行可视化，观察特征分布。</p><p><img src="/2018/01/21/neural-nets-note/firstlayer.png" alt="accuracies"></p></li></ul><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>有时候单个模型并不能满足需求，可以进行模型集成来获得额外的性能提高。进行集成有以下几种方法：</p><ul><li><strong>同一个模型，不同的初始化</strong>：使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。</li><li><strong>在交叉验证中发现最好的模型</strong>：使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。</li><li><strong>一个模型设置多个记录点</strong>：如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。</li><li><strong>在训练的时候跑参数的平均值</strong>：在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。</li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-2/</a></p></li><li><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a></p></li><li><p><a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">Loss function</a></p><p>​</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>训练全连接网络</title>
    <link href="http://yoursite.com/2018/01/19/FullyConnectedNets-train/"/>
    <id>http://yoursite.com/2018/01/19/FullyConnectedNets-train/</id>
    <published>2018-01-19T03:08:25.000Z</published>
    <updated>2018-01-21T03:06:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习的训练过程不是盲目的，应该掌握一定的技巧和方法。参考<a href="https://www.reddit.com/r/cs231n/comments/443y2g/hints_for_a2/" target="_blank" rel="external">Hints for A2</a>和<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="external">Lecture</a>记录一下FullyConnectedNets的训练过程。在开始训练之前要先<strong>预处理数据</strong>和<strong>选择网络结构</strong>，然后我们就可以开始训(tiao)练(can)啦～</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>首先我们要先检测一下_loss_是不是合理的，例如十个分类，loss就应该是-np.log(0.1) = 2.3左右。</p><p>然后我们需要确保在最初的几次迭代中_loss_是下降的，在这里_regularization_(L2/drop out)可以扔掉，_learning rate decay_也可以不设置。我们尝试不同的_weigtht scale_和_learning_rate_，将两者定位在一个大致范围。那么先用较小的数据集和1个epoch来调调<strong>weight scale</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">weight_scale = [<span class="number">1e-5</span>, <span class="number">1e-3</span>, <span class="number">1e-1</span>, <span class="number">1e2</span>, <span class="number">1e6</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> ws, lr, hl, ur <span class="keyword">in</span> product(weight_scale, learning_rate, hidden_layers, update_rule):</div><div class="line">    num_layer = len(hl)</div><div class="line">    model = FullyConnectedNet(hl, weight_scale=ws, use_batchnorm=<span class="keyword">False</span>)</div><div class="line">    solver = Solver(model, small_data,</div><div class="line">                    print_every=print_every, num_epochs=num_epochs, batch_size=batch_size,</div><div class="line">                    update_rule=ur,</div><div class="line">                    verbose=<span class="keyword">True</span>,</div><div class="line">                    optim_config=&#123;</div><div class="line">                      <span class="string">'learning_rate'</span>: lr</div><div class="line">                    &#125;</div><div class="line">                    </div><div class="line">             )</div><div class="line">    solver.train()</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale3.png" alt=""></p><p>可以看出 <code>weight_scale=1e-1</code> 的时候表现较好，1e+2就开始爆炸了。Hints里说ReLU nets的lecture里给了”correct” value，不知道是不是lecture里的1e-2，但设置成这个值表现确实比较好，所以就它了！</p><p>接下来调整<strong>learning_rate</strong>来查看loss的变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">1e-7</span>, <span class="number">1e-5</span>, <span class="number">1e-3</span>, <span class="number">1e-1</span>, <span class="number">1e2</span>]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/lr1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr3.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr4.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr5.png" alt=""></p><p>可以看到_learning_rate_太小(1e-7)时，loss的变化不大；当_learning_rate_太大(1e+2)时，loss就爆炸了。这里有一个小trick：当看到loss &gt; 3倍的初始值时，就可以停止了。</p><h3 id="Learning-rate"><a href="#Learning-rate" class="headerlink" title="Learning rate"></a>Learning rate</h3><p>我们大致知道了_learning_rate_同学的表现情况，现在开始缩小_learning_rate_的范围，并使用原始数据集，来看看训练结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">10</span> ** uniform(<span class="number">-5</span>, <span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</div></pre></td></tr></table></figure><p>其中比较好的几个结果：</p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate3.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate4.png" alt=""></p><p>可以看到1.74e-4同学表现较好，那我们就把learning_rate设置为这个，epoch增加为2，再来看看loss的变化：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_loss.png" alt=""></p><p>看到loss趋于平坦，那我们把<strong>learning rate decay</strong>设置为0.95看看效果：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_decay_95.png" alt=""></p><p>表现似乎还不如不设置呢？那把<strong>learning rate decay</strong>设置为0.9：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_decay_90.png" alt=""></p><p>好像差别也不大，那设置回1.0。然后开始考虑<strong>增强模型训练能力</strong></p><h3 id="Increasing-model-capacity"><a href="#Increasing-model-capacity" class="headerlink" title="Increasing model capacity"></a>Increasing model capacity</h3><p>一开始我设置的网络结构是3层，每层100个神经元，增强模型的训练能力可以通过<strong>增加神经元</strong>或者<strong>增加层数</strong>。然后就需要重新训练学习率了（望天</p><p>我先将层数增加为5层，改用小的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden_layers = [[<span class="number">100</span>]*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">6</span>)]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_1.png" alt=""></p><p>可以看到这个结果并不是很好，需要通过上面的方法，查找合适的学习率。</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_3.png" alt=""></p><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>决定使用一下<strong>batchnorm</strong>来看看效果～</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_batch.png" alt=""></p><p>可以看到准确率一下子提高了～ 但是还没有达到我们想要的50%以上。这时候设置<strong>learning rate decay</strong>为0.95，并训练2个epoch，可以看到准确度又提高了一点。但是还不如上面三层的结果？</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_decay.png" alt=""></p><p>俺现在要把神经元加到500个，回去用三层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden_layers = [[<span class="number">500</span>]*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>,<span class="number">4</span>)]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/layer3_500.png" alt=""></p><p>✌ 在2个epoch准确率达到50%了，开心～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习的训练过程不是盲目的，应该掌握一定的技巧和方法。参考&lt;a href=&quot;https://www.reddit.com/r/cs231n/comments/443y2g/hints_for_a2/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hint
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n assignment1 学习笔记</title>
    <link href="http://yoursite.com/2017/11/10/cs231n-assignment1/"/>
    <id>http://yoursite.com/2017/11/10/cs231n-assignment1/</id>
    <published>2017-11-10T13:43:02.000Z</published>
    <updated>2017-12-10T09:53:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 <a href="https://matplotlib.org/faq/osx_framework.html#osxframework-faq" target="_blank" rel="external">Working with Matplotlib on OSX</a>，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。</p><h3 id="k-Nearest-Neighbor-kNN-exercise"><a href="#k-Nearest-Neighbor-kNN-exercise" class="headerlink" title="k-Nearest Neighbor (kNN) exercise"></a>k-Nearest Neighbor (kNN) exercise</h3><p>点击「run」执行每个框框，<code>dists = classifier.compute_distances_two_loops(X_test)</code> 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。</p><a id="more"></a><p>进行预测的时候发现准确度无论如何都是0.14，查了一下发现 <code>predict_labels</code> 函数没有修改😂 </p><p>二次循环的函数比较简单，一次循环边洗澡边思考，然后头脑风暴矩阵变换，开心地想出来了☺️ 核心代码<code>dists[i, :] = np.sqrt(np.sum((X[i, :] - self.X_train) ** 2, axis = 1)).T</code></p><p>无循环函数估摸着应该是要运用数学公式来解决。假设测试集(2x3)：<br>$$<br>testX = \begin{pmatrix} x_{11}&amp;x_{12}&amp;x_{13} \\x_{21}&amp;x_{22}&amp;x_{23} \end{pmatrix}<br>$$<br>训练集(4x3)：<br>$$<br>trainX = \begin{pmatrix}y_{11}&amp;y_{12}&amp;y_{13} \\y_{21}&amp;y_{22}&amp;y_{23}\\y_{31}&amp;y_{32}&amp;y_{33}\\y_{41}&amp;y_{42}&amp;y_{43}\ \end{pmatrix}<br>$$<br>最后生成 dist (2x4) 的距离矩阵，自然联想到矩阵乘法 (2x4) := (2x3) x (3x4)。刚好代码中也有提示矩阵乘法，那么 \(trainX\) 必然是转置一下的。<br>$$<br>trainX^T = \begin{pmatrix}y_{11}&amp;y_{21}&amp;y_{31}&amp;y_{41} \\y_{12}&amp;y_{22}&amp;y_{32}&amp;y_{42}\\y_{13}&amp;y_{23}&amp;y_{33}&amp;y_{43}\\ \end{pmatrix}<br>$$<br>我们取出一个测试集和一个训练集，计算一下它们的欧氏距离：<br>$$<br>dist[1, 1] = \sqrt{(x_{11}-y_{11})^2 + (x_{12}-y_{12})^2 + (x_{13}-y_{13})^2 }<br>$$<br>我们试一下把每个平方拆开：<br>$$<br>dist[1, 1] = \sqrt{x_{11}^2 - 2x_{11}y_{11}+y_{11}^2 + x_{12}^2 - 2x_{12}y_{12}+y_{12}^2+x_{13}^2 - 2x_{13}y_{13}+y_{13}^2}<br>$$<br>整理出根号里头的家伙：<br>$$<br>x_{11}^2+ x_{12}^2+x_{13}^2 +y_{11}^2+y_{12}^2 +y_{13}^2 - 2(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13})<br>$$<br>嗯。。。再一次感叹数学之美。。。于是「根号里头的家伙」就被分成了三个部分</p><ul><li>\(x_{11}^2+ x_{12}^2 +x_{13}^2\) 相当于 \(testX\) 元素平方，再按列相加，即把矩阵压缩成一列。</li><li>\(y_{11}^2+y_{12}^2 +y_{13}^2\) 相当于 \(trainX^T\) 元素平方，再按行相加 ，即把矩阵压缩成一行。</li><li>\(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13}\) 相当于\(testX * trainX\)</li></ul><p>于是就可以<strong>broadcasting</strong>啦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">transXtrain = self.X_train.T <span class="comment"># 转置训练集</span></div><div class="line">sumX = np.sum(X ** <span class="number">2</span>, axis = <span class="number">1</span>)[:, np.newaxis] <span class="comment">#（500x1）</span></div><div class="line">sumtransXtrain = np.sum(transXtrain ** <span class="number">2</span>, axis = <span class="number">0</span>)[np.newaxis] <span class="comment">#（1x5000）</span></div><div class="line">dists = np.sqrt( sumX + sumtransXtrain - <span class="number">2</span> * np.dot(X, transXtrain)) <span class="comment"># broadcasting &amp; matrix multiplication</span></div></pre></td></tr></table></figure><p>最后一个部分是关于k-折交叉验证，根据作业提示，循环每个可能的k值，运行k-nn算法 num_folds 次，每次选择一个子集作为训练集，最后一个子集为验证集。但参考了网上关于10折交叉验证的说法，是轮流将其中9份作为训练数据，1份作为测试数据的。所以我就循环了num+folds-1次。（后又参考网上其他实现代码，发现应该选择一个自己作为验证集，其他所有自己合并作为训练集。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">size = X_train_folds[<span class="number">0</span>].shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</div><div class="line">    k_to_accuracies[k] = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds<span class="number">-1</span>): <span class="comment"># each case </span></div><div class="line">        classifier.train(X_train_folds[i], y_train_folds[i]) <span class="comment"># use one as training data</span></div><div class="line">        predict_labels = classifier.predict(X_train_folds[<span class="number">-1</span>], k, num_loops = <span class="number">0</span>)</div><div class="line">        accuracy = sum(predict_labels == y_train_folds[<span class="number">-1</span>]) / size <span class="comment"># last fold as validation set</span></div><div class="line">        k_to_accuracies[k].append(accuracy)</div></pre></td></tr></table></figure><p><img src="/2017/11/10/cs231n-assignment1/knn.png" alt="cross-validation"></p><p>最后根据交叉验证的结果选择k=8最佳。</p><h3 id="Multiclass-Support-Vector-Machine-exercise"><a href="#Multiclass-Support-Vector-Machine-exercise" class="headerlink" title="Multiclass Support Vector Machine exercise"></a>Multiclass Support Vector Machine exercise</h3><p>在第一部分补充dW就卡了超级久。。。首先要先看官方的课程笔记 <a href="http://cs231n.github.io/optimization-1/#opt3" target="_blank" rel="external">optimization-1</a> ，我们要使用微积分来计算dW。当 $j = y_j $时公式如下：<br>$$<br>\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} 𝟙(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i<br>$$<br>代表梯度减少 margin &gt; 0 的个数乘以样本。</p><p>当 $ j  \neq y_j $时，公式如下：<br>$$<br>\nabla_{w_j} L_i = \ 𝟙(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i<br>$$<br>首先是实用循环方式的函数。一开始我想着那就用当 margin &gt; 0 时设置一个cnt来进行计数，然后还弄了一个非常麻烦的 tile</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">correct_j = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">  cnt = <span class="number">0</span></div><div class="line">  scores = X[i].dot(W) <span class="comment"># (1, C)</span></div><div class="line">  correct_class_score = scores[y[i]]</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">    <span class="keyword">if</span> j == y[i]:</div><div class="line">      correct_j = j</div><div class="line">      <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        loss += margin</div><div class="line">        cnt += <span class="number">1</span></div><div class="line">        </div><div class="line">  dW_test = np.tile(X[i].T[:,np.newaxis],(<span class="number">1</span>, num_classes)) <span class="comment"># 错的错的</span></div><div class="line">  dW_test[:, correct_j] =  cnt * X[i].T</div></pre></td></tr></table></figure><p>在实在想不出来的情况下，偷偷找了一下其他同学的代码，发现其实只要非常简单的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">loss += margin</div><div class="line">    dW[:, j] += X[i].T</div><div class="line">    dW[:, y[i]] -= X[i].T</div></pre></td></tr></table></figure><p>这里意识到了自己代码的两个问题：</p><ul><li><p>偏导都是累积的过程也就是说中间的符号是 += 或者 -= </p></li><li><p>当 $ j \neq y_j $ 时，dW只有相应的 j 列发生变化。</p></li><li><p>$$<br>\hat{y}(x) := \underbrace {w_0 + \sum_{i=1}^{n} w_i x_i }_{\text{线性回归}} + \underbrace {\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j}_{\text{交叉项（组合特征）}} \qquad \text{(n.ml.1.9.1)}<br>$$</p></li></ul><p>接下来是使用向量方式的函数。根据 <a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" rel="external">linear classification note</a> 写出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[y] + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>但在这里我测试了一下 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; print(scores[y].shape)</div><div class="line"></div><div class="line">(500,10)</div></pre></td></tr></table></figure><p>预料中结果应该是一个向量，由label索引所在值组成。但事实证明<code>scores[y]</code>只是把scoers重新排列了一次。参考<a href="https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy" target="_blank" rel="external">[1]</a>，修改代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[np.arange(scores.shape[<span class="number">0</span>]), y][:, np.newaxis] + <span class="number">1</span>)</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(scores[0]- scores[0][y[0]]) #1</div><div class="line">print((scores - scores[np.arange(scores.shape[0]), y][:, np.newaxis])[0]) #2</div><div class="line">print((scores - scores[y])[0]) #3</div><div class="line">print(scores[0]-scores[y[0]]) #4</div></pre></td></tr></table></figure><p>验证一下：12结果一致，34结果一致，12和34不一致。</p><p>同时也要记得在margins上作label索引处理，完整核心代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">num_train = X.shape[<span class="number">0</span>]</div><div class="line">scores = X.dot(W) <span class="comment"># scores.shape =  (N, C)</span></div><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[np.arange(num_train), y][:, np.newaxis] + <span class="number">1</span>)</div><div class="line">margins[np.arange(num_train), y] = <span class="number">0</span> </div><div class="line">loss = np.sum(margins) / num_train + reg * np.sum(W * W)</div></pre></td></tr></table></figure><p>使用向量来完成梯度也卡了挺久。提示说重用计算过的值，那肯定就是margins啦，通过它们shape来判断了一下，估摸着要用X.T(D, N) * margins(N, C)来完成，但是空想不出来，于是写一个具体的例子来理解一下：</p><p><img src="/2017/11/10/cs231n-assignment1/svm_dw.jpg" alt="IMG_3133(20171115-204831)"></p><p>因为我们只需判断margins是否大于0，首先将其转化为只含0，1的矩阵。接着可以计算出每一行1的数量（即margin&gt;0的个数），将分类所在位置改为该数量，最后与X的转置相乘即可。完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mat = np.zeros(margins.shape) <span class="comment"># mat.shape = N x C</span></div><div class="line">idx = np.where(margins &gt; <span class="number">0</span>) <span class="comment"># return index of margins &gt; 0</span></div><div class="line">mat[idx] = <span class="number">1</span> </div><div class="line">count = np.sum(mat, axis=<span class="number">1</span>) <span class="comment"># count the number of margins &gt; 0</span></div><div class="line">mat[np.arange(num_train), y] = -count </div><div class="line"></div><div class="line">dW = np.dot(X.T, mat) / num_train</div></pre></td></tr></table></figure><p>但素plot loss的结果长这样。。。总觉得哪里怪怪的</p><p><img src="/2017/11/10/cs231n-assignment1/plot_loss.png" alt="IMG_3133(20171115-204831)"></p><p>然后plot weights的结果长这样。。。更觉得哪里怪怪的了</p><p><img src="/2017/11/10/cs231n-assignment1/plot_weights1.png" alt="IMG_3133(20171115-204831)"></p><p>感谢踩过坑的同学 <a href="https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/" target="_blank" rel="external">https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/</a> 原来是dW忘记做正则化了。加上<code>dW += reg * W </code>我们就可以看见光滑的曲线和多样的色彩了，噢耶！</p><p><img src="/2017/11/10/cs231n-assignment1/plot_loss2.png" alt="IMG_3133(20171115-204831)"></p><p><img src="/2017/11/10/cs231n-assignment1/plot_weights2.png" alt="IMG_3133(20171115-204831)"></p><h3 id="Softmax-exercise"><a href="#Softmax-exercise" class="headerlink" title="Softmax exercise"></a>Softmax exercise</h3><p>这次作业和svm的类似，分别通过循环和矩阵运算实现softmax，计算loss和gradient</p><p>首先是通过循环实现，理解一下softmax的公式</p><p><img src="/2017/11/10/cs231n-assignment1/softmax_loss.png" alt=""></p><p>根据 <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf" target="_blank" rel="external">lecture3-slides</a>，softmax的scores是非标准化的log概率（括号里看起来相当复杂，其实就是概率）。计算过程看下图比较明确：</p><p><img src="/2017/11/10/cs231n-assignment1/probability.png" alt=""></p><p>一开始还按照公式循环计算，然后发现标准化后，概率相加为1，因此只要取出所属类别的概率log就可以了，so easy～</p><p>关于梯度的计算结合了wiki以及参考 <a href="https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function" target="_blank" rel="external">derivative-of-softmax-loss-function</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  “”“省略部分代码”“”</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</div><div class="line">    scores = X[i].dot(W)  <span class="comment"># (1,C)</span></div><div class="line">    probabilities = np.exp(scores) <span class="comment"># exp</span></div><div class="line">    probabilities /= np.sum(probabilities) <span class="comment">#normalize</span></div><div class="line">    loss -= np.log(probabilities[y[i]]) <span class="comment">#log</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes): <span class="comment"># pi-yi</span></div><div class="line">      <span class="keyword">if</span>(j == y[i]):</div><div class="line">        dW[:, j] += X[i] * (probabilities[y[i]] - <span class="number">1</span>)</div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        dW[:, j] += X[i] * probabilities[j]</div></pre></td></tr></table></figure><p>然后让我们迎来矩阵的计算～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""省略部分代码"""</span></div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  scores = X.dot(W)</div><div class="line">  probabilities = np.exp(scores) <span class="comment"># exp</span></div><div class="line">  probabilities /= np.sum(probabilities, axis=<span class="number">1</span>)[:, np.newaxis] <span class="comment"># normalize</span></div><div class="line"></div><div class="line">  mat = probabilities[np.arange(num_train), y] <span class="comment"># 获取对应分类的概率</span></div><div class="line">  loss = -np.sum(np.log(mat)) / num_train + reg * np.sum(W * W)</div><div class="line"></div><div class="line">  mat = np.zeros_like(probabilities) </div><div class="line">  mat[np.arange(num_train), y] = <span class="number">1</span> <span class="comment"># 对应分类置为1</span></div><div class="line">  dW = X.T.dot(probabilities - mat) / num_train</div></pre></td></tr></table></figure><h3 id="Implementing-a-Neural-Network"><a href="#Implementing-a-Neural-Network" class="headerlink" title="Implementing a Neural Network"></a>Implementing a Neural Network</h3><p>loss的计算参考上面的softmax，但是要注意正则化时W1和W2都需要加上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算loss</span></div><div class="line">probabilities = np.exp(scores)  <span class="comment"># exp</span></div><div class="line">probabilities /= np.sum(probabilities, axis=<span class="number">1</span>)[:, np.newaxis]  <span class="comment"># normalize</span></div><div class="line">mat = probabilities[np.arange(N), y]  <span class="comment"># 获取对应分类的概率</span></div><div class="line">loss = -np.sum(np.log(mat)) / N + reg * (np.sum(W1*W1) + np.sum(W2*W2))</div></pre></td></tr></table></figure><p>反向传播的时候有一项很重要的<code>dh1[h1 &lt;=0] = 0</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算backward pass</span></div><div class="line"><span class="comment"># softmax gradient derivation</span></div><div class="line">dscores = probabilities</div><div class="line">dscores[range(N), y] -= <span class="number">1</span></div><div class="line">dscores /= N  <span class="comment"># (N, C)</span></div><div class="line"><span class="comment"># dW2 and db2</span></div><div class="line">grads[<span class="string">'W2'</span>] = h1.T.dot(dscores) + <span class="number">2</span> * reg * W2 <span class="comment"># (H, C)</span></div><div class="line">grads[<span class="string">'b2'</span>] = np.sum(dscores, axis=<span class="number">0</span>)</div><div class="line"><span class="comment"># next backprop into hidden layer</span></div><div class="line">dh1 = dscores.dot(W2.T) <span class="comment"># dh1.size = (N, H)</span></div><div class="line">dh1[h1 &lt;=<span class="number">0</span>] = <span class="number">0</span></div><div class="line"><span class="comment"># dW1 and db1</span></div><div class="line">grads[<span class="string">'W1'</span>] = X.T.dot(dh1) + <span class="number">2</span> * reg * W1 <span class="comment"># W1.size = (D, H)</span></div><div class="line">grads[<span class="string">'b1'</span>] = np.sum(dh1, axis = <span class="number">0</span>)</div></pre></td></tr></table></figure><p>作业要求说有参数的导数值与数值计算出的差值应该小于1e-8，之前正则化的时候都是使用<code>reg * W2</code>，但是现在要乘以2以后误差才会小于1e-11（包括后面准确率也会高一些）</p><p>predict的时候注意使用ReLu，否则就卡在0.38以下（如下图），准确率上不去0.4。</p><p><img src="/2017/11/10/cs231n-assignment1/hs300.png" alt=""></p><p>下面我们要开始调参啦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">best_net = <span class="keyword">None</span> <span class="comment"># store the best model into this </span></div><div class="line">best_val = <span class="number">-1</span></div><div class="line">results = &#123;&#125;</div><div class="line"></div><div class="line">hidden_size = [<span class="number">200</span>, <span class="number">300</span>, <span class="number">500</span>, <span class="number">800</span>, <span class="number">1000</span>]</div><div class="line">learning_rate = [<span class="number">9e-4</span>]</div><div class="line">iters = [<span class="number">100</span>]</div><div class="line">regularization_strength = [<span class="number">0.01</span>]</div><div class="line"></div><div class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> product(hidden_size, learning_rate, iters, regularization_strength):</div><div class="line">    </div><div class="line">    hs = item[<span class="number">0</span>]</div><div class="line">    lr = item[<span class="number">1</span>]</div><div class="line">    it = item[<span class="number">2</span>]</div><div class="line">    rg = item[<span class="number">3</span>]</div><div class="line">    net = TwoLayerNet(input_size, hs, num_classes)</div><div class="line"></div><div class="line">    <span class="comment"># Train the network</span></div><div class="line">    stats = net.train(X_train, y_train, X_val, y_val,</div><div class="line">                num_iters=it, batch_size=<span class="number">200</span>,</div><div class="line">                learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</div><div class="line">                reg=rg, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Predict on the validation set</span></div><div class="line">    val_acc = (net.predict(X_val) == y_val).mean()</div><div class="line">    train_acc = (net.predict(X_train) == y_train).mean()</div><div class="line">    results[(hs, lr, it, rg)] = (train_acc, val_acc)</div><div class="line">    </div><div class="line">    <span class="comment"># Check best validation accuracy</span></div><div class="line">    <span class="keyword">if</span>(val_acc &gt; best_val):</div><div class="line">        best_val = val_acc</div><div class="line">        best_net = net</div><div class="line">        </div><div class="line"><span class="comment"># Print out results.\</span></div><div class="line"><span class="keyword">for</span> hs, lr, it, reg <span class="keyword">in</span> sorted(results):</div><div class="line">    </div><div class="line">    train_acc, val_acc = results[(hs, lr, it, reg)]</div><div class="line">    print(<span class="string">'hs:%d it:%d lr:%e rg:%e train_acc:%f val_acc：%f'</span> %(hs, it, lr, reg, train_acc, val_acc))</div><div class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</div></pre></td></tr></table></figure><p>首先我们固定学习率、迭代次数（小一点）、正则化系数，修改隐藏层大小 = [200, 300, 500, 800, 1000]</p><p><img src="/2017/11/10/cs231n-assignment1/change_hs.png" alt=""></p><p>很明显隐藏层越大精确度更高，接下来固定其他参数，来调整一下学习率</p><p>发现学习率在 &gt; -e2 数量级都会报错？？？应该是学习率太大会导致W超出-1到1的范围吧。</p><p><img src="/2017/11/10/cs231n-assignment1/learning_rate_error.png" alt=""></p><p>测试学习率 = [3e-3, 4e-4, 5e-5, 6e-6]</p><p><img src="/2017/11/10/cs231n-assignment1/change_lr1.png" alt=""></p><p>似乎学习率大一些的结果比较好，那在这个数量级周围再测试一下</p><p><img src="/2017/11/10/cs231n-assignment1/change_lr2.png" alt=""></p><p>8e-2的学习率还是崩啦，这样看起来学习率设置成2e=e会比较好。接下来我们调大迭代次数和隐藏层大小，来调整正则化参数</p><p><img src="/2017/11/10/cs231n-assignment1/change_hs_rg.png" alt=""></p><p>嘻嘻最高到0.534了，可以加分了～</p><p><strong>参考链接</strong></p><ul><li><a href="https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy" target="_blank" rel="external">https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 &lt;a href=&quot;https://matplotlib.org/faq/osx_framework.html#osxframework-faq&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Working with Matplotlib on OSX&lt;/a&gt;，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。&lt;/p&gt;
&lt;h3 id=&quot;k-Nearest-Neighbor-kNN-exercise&quot;&gt;&lt;a href=&quot;#k-Nearest-Neighbor-kNN-exercise&quot; class=&quot;headerlink&quot; title=&quot;k-Nearest Neighbor (kNN) exercise&quot;&gt;&lt;/a&gt;k-Nearest Neighbor (kNN) exercise&lt;/h3&gt;&lt;p&gt;点击「run」执行每个框框，&lt;code&gt;dists = classifier.compute_distances_two_loops(X_test)&lt;/code&gt; 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记五：自适应增强算法</title>
    <link href="http://yoursite.com/2017/11/05/machine-learning-in-action-note5/"/>
    <id>http://yoursite.com/2017/11/05/machine-learning-in-action-note5/</id>
    <published>2017-11-05T01:14:40.000Z</published>
    <updated>2017-11-23T03:18:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：</p><ul><li><p>Bagging（bootstrap aggregating）</p><p>Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。</p></li><li><p>Boosting</p><p>Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。</p><p>根据<a href="https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8" target="_blank" rel="external">LightGBM介绍视频</a>对Bossting做一些补充：本质上来说，Boosting的方法都是在训练好一个子模型后，统计一下现有复合模型的拟合情况，从而调节接下来学习任务的setting，使得接下来加入复合模型的子模型符合降低整体loss的目标。</p></li></ul><p>Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。</p><a id="more"></a><p>AdaBoost的每个样本都有一个权重，构成向量D。首先初始化每个样本的权重相等，在训练集上训练出一个弱分类器，然后计算出错误率 𝝐，通过错误率计算该分类器的 alpha 值，通过这个 alpha 值计算该分类器的权重。<br>$$<br>𝜶 = \frac{1}{2}\ln\frac{1-𝝐}{𝝐}<br>$$<br>接着对权重做出调整，降低分对的样本权重，<br>$$<br>D_{i+1} = \frac{D_i^{i}e^{-𝜶}}{Sum(D)}<br>$$</p><p>提高分错的样本权重。</p><p>$$<br>D_{i+1} = \frac{D_i^{i}e^{𝜶}}{Sum(D)}<br>$$<br>不断迭代达到一定数量或错误率为0，最后输出集成的弱分类器，通过加权来进行预测。算法过程如下图所示：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/adaboost.png" alt="adaboost"></p><p>###训练：决策树桩</p><p>算法的关键就在于如何训练弱分类器，并把它们集成一个强分类器。本章我们使用决策树桩（decision stump）来实现AdaBoost。决策树桩的树桩意味着我们只用单个特征来进行决策。首先我们设定一个阈值 _threshVal_ 与比较规则 _threshIneq_ ，在规则下判断特征值与阈值的大小。当 _threshIneq == ‘lt’_，将所有比 _threshVal_ 小的归为 -1；当 _threshIneq == ‘gt’_ 时，将所有比 _threshVal_ 大的归为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(data, dimen, threshVal, threshIneq)</span>:</span></div><div class="line">    <span class="string">"""测试是否某个值大于或小于阈值"""</span></div><div class="line">    retArr = ones((shape(data)[<span class="number">0</span>], <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</div><div class="line">        retArr[data[:, dimen] &lt;= threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        retArr[data[:, dimen] &gt; threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> retArr</div></pre></td></tr></table></figure><p>这里利用数组过滤来对比阈值大小 <code>retArr[data[:, dimen] &lt;= threshVal]</code> 可以很简单滴获取到返回数组，这个返回数组就是一个弱分类器预测的结果。建立决策树桩的过程就是找出错误率最低的弱分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(data, labels, D)</span>:</span></div><div class="line">    <span class="string">"""遍历样本的每一列，返回错误率最小的弱分类器"""</span></div><div class="line">    dataMat = mat(data); labelMat = mat(labels).T</div><div class="line">    m, n = shape(dataMat)</div><div class="line">    numSteps = <span class="number">10</span>; bestStump = &#123;&#125;; bestClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    minError = inf</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n): <span class="comment"># 每个特征</span></div><div class="line">        rangeMin = dataMat[:, i].min(); rangeMax = dataMat[:, i].max()</div><div class="line">        stepSize = (rangeMax - rangeMin) / numSteps</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">-1</span>, int(numSteps + <span class="number">1</span>)): <span class="comment"># 每个步长</span></div><div class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment"># 每个不等号</span></div><div class="line">                threshVal = (rangeMin + float(j) * stepSize)</div><div class="line">                predictedVals = stumpClassify(dataMat, i, threshVal, inequal) <span class="comment"># 建立一棵决策树桩</span></div><div class="line">                errArr = mat(ones((m, <span class="number">1</span>)))</div><div class="line">                errArr[predictedVals ==  labelMat] = <span class="number">0</span></div><div class="line"></div><div class="line">                <span class="comment"># 计算错误率</span></div><div class="line">                <span class="comment"># AdaBoost和分类器交互的地方</span></div><div class="line">                weightedError = D.T * errArr</div><div class="line">                <span class="comment">#print("split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f" %(i, threshVal, inequal, weightedError))</span></div><div class="line">                <span class="keyword">if</span> weightedError &lt; minError: <span class="comment"># 保存错误率较小的弱分类器</span></div><div class="line">                    minError = weightedError</div><div class="line">                    bestClassEst = predictedVals.copy() <span class="comment"># 预测结果</span></div><div class="line">                    bestStump[<span class="string">'dim'</span>] = i</div><div class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</div><div class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</div><div class="line">    <span class="keyword">return</span> bestStump, minError, bestClassEst</div></pre></td></tr></table></figure><p>我们知道了如何建立一个弱分类器，那么接下来就到了集成弱分类器的部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(data, labels, numIt = <span class="number">40</span>)</span>:</span></div><div class="line">    weakClassArr = []</div><div class="line">    m = shape(data)[<span class="number">0</span>]</div><div class="line">    <span class="comment"># D保存每个样本的权重</span></div><div class="line">    <span class="comment"># AdaBoost算法会降低分对的样本权重，提高分错的样本权重</span></div><div class="line">    D = mat(ones((m, <span class="number">1</span>))/m)</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</div><div class="line">        bestStump, error, classEst = buildStump(data, labels, D)</div><div class="line">        <span class="comment"># 计算该分类器的权重</span></div><div class="line">        alpha = float(<span class="number">.5</span> * log((<span class="number">1</span> - error) / max(error, <span class="number">1e-16</span>))) <span class="comment"># 确保没有错误时不发生除零溢出</span></div><div class="line">        bestStump[<span class="string">'alpha'</span>] = alpha</div><div class="line">        weakClassArr.append(bestStump)</div><div class="line"></div><div class="line">        <span class="comment"># 1 / 计算下一次迭代的权重向量D</span></div><div class="line">        expon = multiply(<span class="number">-1</span> * alpha * mat(labels).T, classEst)</div><div class="line">        D = multiply(D, exp(expon))</div><div class="line">        D = D / D.sum() <span class="comment"># D是一个概率分布, sum = 1</span></div><div class="line"></div><div class="line">        <span class="comment"># 2 / 计算总错误率</span></div><div class="line">        aggClassEst += alpha * classEst</div><div class="line">        aggErrors = multiply(sign(aggClassEst) != mat(labels).T, ones((m, <span class="number">1</span>)))</div><div class="line">        errorRate = aggErrors.sum() / m</div><div class="line">        <span class="keyword">if</span> errorRate == <span class="number">0</span>: <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> weakClassArr</div></pre></td></tr></table></figure><p>###分类：分类器加权</p><p>将每个弱分类器的结果进行加权，输出最终预测的分类结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass, classifierArr)</span>:</span></div><div class="line">    dataMatrix = mat(datToClass)</div><div class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)): <span class="comment"># 每个弱分类器</span></div><div class="line">        classEst = stumpClassify(dataMatrix, classifierArray[i][<span class="string">'dim'</span>],classifierArray[i][<span class="string">'thresh'</span>], classifierArray[i][<span class="string">'ineq'</span>])</div><div class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>] * classEst <span class="comment"># 加权</span></div><div class="line">    <span class="comment"># 返回预测结果</span></div><div class="line">    <span class="keyword">return</span> sign(aggClassEst)</div></pre></td></tr></table></figure><h3 id="非均衡分类问题：ROC曲线"><a href="#非均衡分类问题：ROC曲线" class="headerlink" title="非均衡分类问题：ROC曲线"></a>非均衡分类问题：ROC曲线</h3><p>之前我们都只用错误率来判断分类器的好坏，假设分类问题的结果是不均衡的，比如预测一个人一个人得癌症／没有得癌症，这个分类的代价是不同的，只看错误率是没什么意义的。因此我们引入一些别的性能指标来判断分类器</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">+1</th><th style="text-align:center">-1</th></tr></thead><tbody><tr><td style="text-align:center">+1</td><td style="text-align:center">真正例（TP）</td><td style="text-align:center">伪反例（FN）</td></tr><tr><td style="text-align:center">-1</td><td style="text-align:center">伪正例（FP）</td><td style="text-align:center">真反例（TN）</td></tr></tbody></table><ul><li>正确率（Precision） = TP /（TP + FP）</li><li>召回率（Recall） = TP /（TP + FN）</li></ul><p>ROC代表接受者特征（reciver operating characteristic）曲线。横轴是伪正例的比例 （假阳率 = FP /（FP+TN）），纵轴是真正例的比例（真阳率 = TP / （TP + FN））。</p><blockquote><p>为了创建ROC曲线，首先将分类样例按照其测试强度排序。先从排名最低的样例开始，所有排名更低的样例都被视为反例，而所有排名更高的样例都被判为正例。该情况对应点为(1, 1)。然后将其移到排名次低的样例中去，如果该样例属于正例，那么对真阳率进行修改；如果该样例属于反例，那么对假阴率进行修改。</p></blockquote><p>虽然没看懂上面这段话，但看懂了原代码，感觉循环 <code>for index in sortedIndicies.tolist()[0]</code> 超迷的，tolist() 是什么鬼，[0] 又是什么鬼。对原函数进行分析，首先是对传入参数 _aggClassEst_ 进行 _argsort_ 排序。_aggClassEst_ 是一个矩阵， shape ==  (298,1)，如果直接排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort1.png" alt=""></p><p>如果将其转置再进行排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort2.png" alt=""></p><p>根据 <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argsort.html" target="_blank" rel="external">numpy.argsort</a> 关于一维数组的排序，我们先将其转换为一维数组，并将其<strong>shape</strong>转换 (298 ,)。转换有两种方法：</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aggClassEst.getA().reshape((-1, ))</div></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">squeeze(aggClassEst.getA())</div></pre></td></tr></table></figure></li></ul><p>在这里简单说明一下 (R，1)和 (R，) 的区别。NumPy数组的<strong>shape</strong>为 (R， ) 意味着这个数组只有一个索引，以一个有12个元素的数组为例：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/narray1.png" alt=""></p><p>当我们将其 <code>reshape((3, 4))</code> 后，它有了两个索引：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape34.png" alt=""></p><p>当我们 <code>reshape((12, 1))</code>，它其实也是有两个索引，只是其中一个恒为0：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape1.png" alt=""></p><p>于是修改代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></div><div class="line">    cur = (<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 保留绘制光标的位置</span></div><div class="line">    ySum = <span class="number">0</span> <span class="comment"># 计算AUC</span></div><div class="line">    numPosClas = sum(array(classLabels) == <span class="number">1</span>)</div><div class="line">    yStep = <span class="number">1</span> / float(numPosClas)</div><div class="line">    xStep = <span class="number">1</span> / float(len(classLabels) - numPosClas)</div><div class="line">    <span class="comment"># 获取排序后的索引</span></div><div class="line">    <span class="comment"># 从 (1, 1) 绘制到 (0, 0)</span></div><div class="line">    sortedIndicies = predStrengths.argsort()</div><div class="line">    <span class="comment"># fig = plt.figure();  fig.clf();ax = plt.subplot(111)</span></div><div class="line">    fig, ax = plt.subplots()</div><div class="line">    <span class="comment"># for index in sortedIndicies.tolist()[0]:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sortedIndicies:</div><div class="line">        print(predStrengths[i])</div><div class="line">        <span class="keyword">if</span> classLabels[i] == <span class="number">1</span>:</div><div class="line">            delX = <span class="number">0</span>; delY = yStep</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            delX = xStep; delY = <span class="number">0</span></div><div class="line">            ySum += cur[<span class="number">1</span>]</div><div class="line">        ax.plot([cur[<span class="number">0</span>], cur[<span class="number">0</span>]-delX], [cur[<span class="number">1</span>], cur[<span class="number">1</span>]-delY], c= <span class="string">'b'</span>)</div><div class="line">        cur = (cur[<span class="number">0</span>] - delX, cur[<span class="number">1</span>] - delY)</div><div class="line">    ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'b--'</span>)</div><div class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</div><div class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</div><div class="line">    ax.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    plt.show()</div><div class="line">    print(<span class="string">"the Area Under the Curve is: "</span>, ySum * xStep)</div><div class="line"></div><div class="line">predStrengths = aggClassEst.getA().reshape((<span class="number">-1</span>, ))</div><div class="line">plotROC(predStrengths, labels)</div></pre></td></tr></table></figure><p><img src="/2017/11/05/machine-learning-in-action-note5/roc.png" alt=""></p><p><strong>参考链接</strong></p><p>1) <a href="https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r" target="_blank" rel="external">Difference between numpy.array shape (R, 1) and (R,)</a></p><p>2）<a href="https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8" target="_blank" rel="external">LIghtGBM官方介绍视频</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bagging（bootstrap aggregating）&lt;/p&gt;
&lt;p&gt;Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Boosting&lt;/p&gt;
&lt;p&gt;Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&quot;https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;LightGBM介绍视频&lt;/a&gt;对Bossting做一些补充：本质上来说，Boosting的方法都是在训练好一个子模型后，统计一下现有复合模型的拟合情况，从而调节接下来学习任务的setting，使得接下来加入复合模型的子模型符合降低整体loss的目标。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记四：支持向量机</title>
    <link href="http://yoursite.com/2017/10/22/machine-learning-in-action-note4/"/>
    <id>http://yoursite.com/2017/10/22/machine-learning-in-action-note4/</id>
    <published>2017-10-22T00:50:53.000Z</published>
    <updated>2017-11-07T11:54:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。</p><p>上一章学习的「对数几率函数」中，我们提到了</p><blockquote><p>利用线性回归模型的预测结果去逼近真实标记的对数几率</p></blockquote><p>标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。</p><p>上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svmpic.jpg" alt="svmpic"></p><a id="more"></a><p>👆🏻有一条线把苹果和香蕉分开了（在二维空间中就是一条线），这样分开不同训练样本的线就称为「划分超平面」。距离这条线最近的几个点就是「支持向量」，它们到这条线的距离就为「间隔」。那么我们再来回顾一下刚说的一句话：</p><blockquote><p>找出具有“最大间隔”（maximum margin）的「划分超平面」</p></blockquote><p>能够划分训练样本的超平面可能有很多，我们应该要找的，是位于“正中间”的那个划分超平面，也就是距离不同类别都尽可能远的那个超平面。这样进行预测时误差才会尽可能小。</p><p>说到这里，我们可以明确一下SVM算法的设计问题了。因为越接近超平面的点越“难”分割，找到这些点就万事大吉了。因此SVM学习分类器最重要的是找到哪些样本作为「支持向量」。其本质是一个最优化问题。</p><p>一个最优化问题通常有两个最基本的因素：1）目标函数：希望什么东西的指标达到最好；2）优化对象：你希望通过改变哪些因素使目标函数达到最优。在SVM中，目标函数是「最大间隔」，优化对象就是「划分超平面」。下面我们就需要对这两个基本因素进行数学描述。</p><p>###SVM的数学建模</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svm_model.png" alt="svmpic"></p><p>划分超平面的线性方程：<br>$$<br>w^Tx + b = 0<br>$$<br>其中，<strong>w</strong> = （w~1~； w~2~；… ； w~d~）为法向量。样本空间中任意点 <strong>x</strong> 到超平面的距离（几何间隔）可写为：<br>$$<br>d = \frac{|w^Tx + b|}{||w||}<br>$$<br>其中，$||w||$ 为向量的模。前面我们说到，SVM的分类结果是+1／-1，令：<br>$$<br>\begin{cases}w^Tx + b≥+1, y_i = +1 \\ w^Tx + b≤-1, y_i = -1<br>\end{cases}<br>$$<br>支持向量使得等号成立。两个异类支持向量到超平面的距离之和为<br>$$<br>𝜸 = \frac{2}{||w||}<br>$$<br>显然，为了最大化间隔，仅需最大化$||w||^{-1}$，等价于最小化$||w||^2$。</p><p>到这里，我们可以给出SVM的数学描述：<br>$$<br>\min_{w,b}\frac{1}{2}||w||^2 \\  s.t. \quad y_i(w^Tx+b) ≥1,i = 1,2,…,m<br>$$</p><p>缩写s. t. 表示“Subject to”，是“服从某某条件”的意思。根据参考链接[3]解释一下这个条件的含义。我们定义「函数间隔」为<br>$$<br>y(w^Tx+b)=yf(x)≥𝜸<br>$$<br>前面乘上类别 y 之后保证间隔的非负性（因为 f(x)&lt;0 对应于 y=−1 的那些点）。</p><p>###对偶问题</p><p>上面SVM的数学描述其实是一个<strong>二次优化问题</strong>——目标函数是二次的，约束条件是线性的。引入「拉格朗日乘子法」求解，对每条约束添加拉格朗日乘子𝜶~i~ ≥ 0，则该问题的拉格朗日函数可写为<br>$$<br>L（w,b,𝜶) =\frac{1}{2}||w||^2 + 𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))<br>$$<br>其中，\(𝜶 = （𝛼_1;𝛼_2;…;𝛼_m)\)。我们令<br>$$<br>𝜃(w) =\max _{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$</p><p>则上式的最优值为 $𝜃(w) = \frac{1}{2}||w||^2$, 即我们需要优化的SVM数学模型。具体公式为</p><p>$$<br>\min_{w,b} 𝜃(w)  =\min_{w,b}\max_{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$<br>将min和max交换位置得到原始问题的对偶问题</p><p>$$<br>\max_{𝜶_i≥ 0}\min_{w,b}\quad L（w,b,𝜶)<br>$$<br><strong>为什么可以转化呢？</strong>因为瘦死的骆驼比马大，「最大值中的最小值」也比「最小值中的最大值」来得大。</p><p><strong>那么先求最大值和先求最小值有什么区别呢？</strong>因为这样更容易求解。我们通过偏导先求 <strong>L</strong> 关于 𝒘 和 𝑏 极小，再求 <strong>L</strong> 的极大。分别令 ∂/∂w 和 ∂/∂b 为零可得<br>$$<br>\frac{∂L}{∂w} = 0 ⇒ w = \sum_{i=1}^m𝜶_iy_ix_i<br>$$</p><p>$$<br>\frac{∂L}{∂b} = 0 ⇒ \sum_{i=1}^m𝜶_iy_i = 0<br>$$</p><p>代回 <strong>L</strong> 得到SVM数学描述的对偶问题<br>$$<br>max_𝜶\quad \sum_{i=1}^m𝜶_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m𝜶_i𝜶_jy_iy_jx_i^Tx_j^T<br>$$</p><p>$$<br>s.t\quad\sum_{i=1}^m𝜶_iy_i = 0,\quad𝜶_i≥ 0,i =1,2,…,m<br>$$</p><p>解出𝜶后，求出w与b即可得到模型<br>$$<br>\begin {align_} f(x) &amp; = w^T+b \\ &amp; = \sum_{i=1}^m𝜶_iy_ix_i^Tx + b\end {align_}<br>$$<br>所以只要求出了w和b，将测试数据带入上面这个模型，即可得出预测值。</p><p>实际上，所有非支持向量的𝜶值都为零，<strong>最终模型只与支持向量有关</strong>。回忆一下我们的拉格朗日函数<br>$$<br>\max _{𝜶_i≥ 0}\quad L（w,b,𝜶) =\max_{𝜶_i≥ 0}\quad \frac{1}{2}||w||^2 + \color{red}{𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))}<br>$$<br>如果 x~i~ 是支持向量，那么红色标出部分为0（因为支持向量函数间隔为1）。对于非支持向量，函数间隔大于1，那么红色标出部分将小于0，为满足整个式子最大，只能𝜶~i~ 为0。因此我们预测的过程只需要计算少量向量的内积，速度是很快的。</p><h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>好的，接下来问题就转换为<strong>如何求解对偶问题</strong>。1996年（竟然在我出生这年搞事情🙂）John Platt发布了SMO（Sequntial Minimal Optimization）算法，将大优化问题分解为多个小优化问题求解。SMO的基本思路是先固定𝜶~i~之外的所有参数，然后求𝜶~i~上的极值。由于存在约束 $sum_{i=1}^m𝜶_iy_i = 0$ ，若固定 𝜶~i~ 之外的其他变量，则 𝜶~i~ 可由其他变量导出。于是SMO每次选择两个变量𝜶~i~ 和 𝜶~j~ ，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li>选取一对需要更新的变量 𝜶~i~ 和 𝜶~j~ ；</li><li>固定 𝜶~i~ 和 𝜶~j~ 以外参数，求解获得更新后的 𝜶~i~ 和 𝜶~j~ </li></ul><p>好的，理解SMO实在是无能为力了，我们进入下一个话题「核函数」。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>上面我们解决了线性分类问题，但SVM还可以解决非线性的分类问题，这就需要引入「核函数」，来将数据从一个特征空间转换到另一个特征空间，在新空间下再利用模型对数据进行处理。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/kernel.gif" alt="kernel"></p><p>径向基函数（radial bias function）是SVM常用的一个核函数，具体公式为<br>$$<br>k(x,y) = exp(\frac{-||x-y||^2}{2𝜎^2})<br>$$<br>其中 𝜎 是用户定义的函数值跌落到0的速度参数。</p><h3 id="SVM的一般流程"><a href="#SVM的一般流程" class="headerlink" title="SVM的一般流程"></a>SVM的一般流程</h3><ol><li>收集数据：可以使用任意方法</li><li>准备数据：需要数值型数据</li><li>分析数据：可视化分割超平面是很有帮助的</li><li>训练算法：SVM算法最耗时的地方。该过程主要实现两个参数调优</li><li>测试算法：计算十分简单</li><li>使用算法：几乎所有分类问题都可以用SVM来解决，值得一提的是，SVM本身是一个二类分类器，你需要修改一些代码来适应多分类问题</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在作者给的源码中，有可视化数据的部分，修改了一下用于测试不同k值rbf选择的支持向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fr = open(<span class="string">'testSetRBF.txt'</span>) <span class="comment"># generate data</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</div><div class="line">    lineSplit = line.strip().split(<span class="string">'\t'</span>)</div><div class="line">    xPt = float(lineSplit[<span class="number">0</span>])</div><div class="line">    yPt = float(lineSplit[<span class="number">1</span>])</div><div class="line">    label = float(lineSplit[<span class="number">2</span>])</div><div class="line">    <span class="keyword">if</span> (label &lt; <span class="number">0</span>):</div><div class="line">        xcord0.append(xPt)</div><div class="line">        ycord0.append(yPt)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        xcord1.append(xPt)</div><div class="line">        ycord1.append(yPt)</div><div class="line">k1 = <span class="number">1.3</span> <span class="comment"># 修改测试</span></div><div class="line">sVs = svm.testRbf(k1)</div><div class="line">m = shape(sVs)[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">    x = sVs.A[i][<span class="number">0</span>]</div><div class="line">    y = sVs.A[i][<span class="number">1</span>]</div><div class="line">    circle = Circle((x,y), <span class="number">0.05</span>, facecolor=<span class="string">'none'</span>, edgecolor=(<span class="number">0</span>, <span class="number">0.8</span>, <span class="number">0.8</span>), linewidth=<span class="number">3</span>, alpha=<span class="number">0.5</span>)</div><div class="line">    ax.add_patch(circle)</div><div class="line">ax.scatter(xcord0, ycord0, marker=<span class="string">'s'</span>, s=<span class="number">30</span>)</div><div class="line">ax.scatter(xcord1, ycord1, marker=<span class="string">'o'</span>, s=<span class="number">30</span>, c=<span class="string">'red'</span>)</div><div class="line">plt.title(<span class="string">'RBF k1 = %f, %d Support Vectors'</span> %(k1, m))</div><div class="line">plt.show()</div><div class="line">fr.close()</div></pre></td></tr></table></figure><p><img src="/2017/10/22/machine-learning-in-action-note4/k_1.3.png" alt="k_1.3"></p><p><img src="/2017/10/22/machine-learning-in-action-note4/k_0.1.png" alt="k_0.1"></p><p>可以看出参数值越小支持向量的范围越模糊，选择合适的参数还是很重要的。</p><p><strong>参考链接</strong></p><ul><li><a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">https://www.zhihu.com/question/21094489</a></li><li><a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="external">零基础学SVM—Support Vector Machine(一)</a> </li><li><a href="http://blog.pluskid.org/?p=632" target="_blank" rel="external">http://blog.pluskid.org/?p=632</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。&lt;/p&gt;
&lt;p&gt;上一章学习的「对数几率函数」中，我们提到了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;利用线性回归模型的预测结果去逼近真实标记的对数几率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。&lt;/p&gt;
&lt;p&gt;上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2017/10/22/machine-learning-in-action-note4/svmpic.jpg&quot; alt=&quot;svmpic&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记三：对数几率回归</title>
    <link href="http://yoursite.com/2017/10/20/machine-learning-in-action-note3/"/>
    <id>http://yoursite.com/2017/10/20/machine-learning-in-action-note3/</id>
    <published>2017-10-20T08:24:26.000Z</published>
    <updated>2017-11-07T11:53:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。</p><p>「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：<br>$$<br>f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i<br>$$<br>有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>$$<br>\ln y = w^Tx + b<br>$$<br>这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）<br>$$<br>y = g^{-1}(w^Tx + b)<br>$$<br>其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。</p><a id="more"></a><p>其次，我们想用回归分析来做<strong>分类任务</strong>怎么办？那么我们可以找一个单调可微函数，将分类任务的真实标记y与线性回归模型的预测值 <strong>z=w^T^x+b</strong> 联系起来。考虑最简单的二分类任务，我们想到了“单位阶跃函数”（unit-step function）<br>$$<br>y = \begin{cases}0, z<0\\0.5, z="0\\1,z">0<br>\end{cases}<br>$$<br>但单位阶跃函数不连续，因此不能作为g(･)。于是聪明的人们就找到了Sigmoid函数，也称对数几率函数（logistic function）：<br>$$<br>y = \frac{1}{1+e^{-z}}<br>$$<br>类比上面的线性回归我们可以得到：<br>$$<br>\ln\frac{y}{1-y} = w^Tx + b<br>$$<br>由此就引发了关于 Logistic Regression 中文名的思考。大多数看到的翻译都是「逻辑回归」，其中逻辑一词代表什么一直不懂。但在西瓜书中作者称之为「对数几率回归」，其言语是让我信服的。</0\\0.5,></p><p>将 y 视为样本x作为正例的可能性，则 1-y 时其反例的可能性，两者比值称为“对率”（odds）：<br>$$<br>\frac{y}{1-y}<br>$$<br>对率反映了x作为正例的相对可能性。对几率取对数则得到“对数几率”（log odds，亦称logit）：<br>$$<br>\ln\frac{y}{1-y}<br>$$<br>由此可看出，实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。</p><p>不管怎么说，在理解一个回归模型的时候至少先把它的名字念对吧233</p><h3 id="分析：可视化分析数据"><a href="#分析：可视化分析数据" class="headerlink" title="分析：可视化分析数据"></a>分析：可视化分析数据</h3><p>在调用plotBestFit时，有一个<code>weights.getA())</code>这个getA()是啥玩意儿？根据参考链接[1]-11楼的小伙伴给出了答案，矩阵通过这个getA()这个方法可以将自身返回成一个n维数组对象</p><p><img src="/2017/10/20/machine-learning-in-action-note3/geta_test.png" alt=""></p><p>从上图输出类型可以看出，使用getA函数将矩阵转化为了数组</p><p><img src="/2017/10/20/machine-learning-in-action-note3/matnarr.png" alt=""></p><p>当矩阵只有一行的时候输出 mat[1] 是会报错的</p><p><img src="/2017/10/20/machine-learning-in-action-note3/mat_trans.png" alt=""></p><p>将 mat 转制后 mat[1] 输出的是一个1x1的矩阵而不是一个数值</p><p>###训练：梯度下降（Gradient Descent）</p><p>在对数几率函数的训练过程中，最重要的就是如何训练权值w。为了使其公式化，我们定义了一个“代价函数”（cost function），来衡量预测值和实例标记的差距：<br>$$<br>J(w) = \frac{1}{2}\sum_{i=1}^m(h_w(x^{(i)})-y^{i})^2<br>$$<br>我们希望这个值很小，于是就有了梯度下降算法的迭代公式：<br>$$<br>w_j := w_j - 𝛼\frac{\partial }{\partial w_j}J(w)<br>$$<br>让我们针对一个训练样本 (x, y) 算算这个偏导是啥：<br>$$<br>\begin{align}\frac{\partial }{\partial w_j}J(w)   &amp; =   \frac{\partial }{\partial w_j}\frac{1}{2}(h_w(x)-y)^2\\<br>&amp; =  2･\frac{1}{2}(h_w(x)-y)･\frac{𝜕}{𝜕w_j}(h_w(x)-y)\\<br>&amp; = (h_w(x)-y)･\frac{𝜕}{𝜕w_j}(\sum_{i=0}^nw_ix_i-y)\\<br>&amp; = (h_w(x)-y)x_j\end{align}<br>$$<br>因此对于一个训练样本有：<br>$$<br>w_j := w_j - 𝛼(y^{i}-h_w(x^{i}))x_j^{(i)}<br>$$<br>运用在多于一个样本的训练集上时，我们有两种选择：</p><ol><li><p><strong>批量梯度下降（batch gradient descent）</strong></p><p>将所有权值初始化为1</p><p>循环R次: </p><p>​    计算整个训练集的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataMatrix = mat(dataMatIn) <span class="comment"># m x n</span></div><div class="line">    labelMat = mat(classLabels).transpose() <span class="comment">#  1x100 to 100x1</span></div><div class="line">    m, n = shape(dataMatrix)</div><div class="line">    alpha = <span class="number">0.001</span></div><div class="line">    maxCycles = <span class="number">500</span></div><div class="line">    weights = ones((n,<span class="number">1</span>)) <span class="comment"># n x 1</span></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</div><div class="line">        h = sigmoid(dataMatrix * weights) <span class="comment"># m x 1, 整个训练集</span></div><div class="line">        error = labelMat - h</div><div class="line">        weights = weights + alpha * dataMatrix.transpose()*error</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure></li><li><p><strong>随机梯度下降（stochastic gradient descent／incremental gradient descent）</strong></p><p>将所有权值初始化为1</p><p>对每一个训练样本：</p><p>​    计算训练样本的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">        h = sigmoid(sum(dataArr[i] * weights)) <span class="comment"># single value，一个训练样本</span></div><div class="line">        error = classLabels[i] - h <span class="comment"># single value</span></div><div class="line">        weights = weights + alpha * error * dataArr[i]</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><p>两种方法的区别在于计算 <strong>weights</strong> 时，「批量梯度下降」每次都用整个训练集来更新权值。假设我们有m个实例和n个特征，每次就要做 <strong>m*n</strong> 次乘法，当 <strong>m</strong> 非常大时，计算代价是很高的。而「随机梯度下降」每次只用一个训练样本，它同时也是一种在线学习算法。虽然它有时候很难安全等于最优值，但也差不多了。因此，我们一般使用「随机梯度下降」来训练权值。</p><p>接下来我们对随机梯度下降函数做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatIn, classLabels, numIter=<span class="number">150</span>)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter): <span class="comment"># 迭代次数</span></div><div class="line">        dataIndex = list(range(m))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">            <span class="comment"># 1/ Alpha changes with each iteration</span></div><div class="line">            alpha = <span class="number">4</span>/(<span class="number">1</span>+iter+i) + <span class="number">0.01</span></div><div class="line">            <span class="comment"># 2/ Update vectors are randomly selected</span></div><div class="line">            randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</div><div class="line">            h = sigmoid(sum(dataArr[randIndex] * weights)) <span class="comment"># single value</span></div><div class="line">            error = classLabels[randIndex] - h <span class="comment"># single value</span></div><div class="line">            weights = weights + alpha * error * dataArr[randIndex]</div><div class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><h3 id="预测：从疝气病预测病马的死亡率"><a href="#预测：从疝气病预测病马的死亡率" class="headerlink" title="预测：从疝气病预测病马的死亡率"></a>预测：从疝气病预测病马的死亡率</h3><p>这里插播一下如何处理数据的缺失值。面对缺失的一些特征我们有如下选择：</p><ol><li>使用可用特征的均值来填补缺失值</li><li>使用特殊值来填补缺失值，如-1</li><li>忽略有缺失值的样本</li><li>使用相似样本的均值填补缺失值</li><li>使用另外的机器学习算法预测缺失值</li></ol><p>在「对数几率回归」中，特征的缺失值可以用0来填补。因为 <code>weights = weights + alpha _ error _ dataArr[randIndex]</code>，当对应特征值为0时，该特征的系数值将不变，则 <code>weights = weights</code>。由于 <code>sigmoid（0）=0.5</code>，对于结果的预测不具有任何倾向性。但是标记缺失的样本我们就不得不丢弃了啊（都不知道你有病没病要你何用</p><p>好的，说了这么多，作者已经帮我们处理完数据了。</p><p>终于我们要开始预测了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></div><div class="line">    prob = sigmoid(sum(inX*weights))</div><div class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div></pre></td></tr></table></figure><p><img src="/2017/10/20/machine-learning-in-action-note3/test_result.png" alt=""></p><p>我的妈呀训练快一小时呢。。。</p><p><strong>参考链接：</strong></p><p>[1] <a href="http://tieba.baidu.com/p/2905471495" target="_blank" rel="external">http://tieba.baidu.com/p/2905471495</a></p><p>[2] 周志华《机器学习》</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。&lt;/p&gt;
&lt;p&gt;「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：&lt;br&gt;$$&lt;br&gt;f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i&lt;br&gt;$$&lt;br&gt;有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即&lt;br&gt;$$&lt;br&gt;\ln y = w^Tx + b&lt;br&gt;$$&lt;br&gt;这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）&lt;br&gt;$$&lt;br&gt;y = g^{-1}(w^Tx + b)&lt;br&gt;$$&lt;br&gt;其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>image-classification-note</title>
    <link href="http://yoursite.com/2017/10/19/image-classification-note/"/>
    <id>http://yoursite.com/2017/10/19/image-classification-note/</id>
    <published>2017-10-19T07:41:20.000Z</published>
    <updated>2017-10-21T09:39:25.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems:"></a>Problems:</h3><ol><li>Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.</li><li>Viewpoint variation: All pixels change when the camera moves</li><li>Illumination: There can be lighting conditions going on in the scene</li><li>Deformation: Cats can assume a lot of different, varied poses and positions.</li><li>Occlusion: You might only see a part of a cat.</li><li>Background Clutter: The foreground of the cat look similar in appearance</li><li>Intraclass variation: Cats can come in different shapes and sizes and colors and ages</li></ol><a id="more"></a><h3 id="An-image-classifier"><a href="#An-image-classifier" class="headerlink" title="An image classifier"></a>An image classifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify_image</span><span class="params">(image)</span>:</span></div><div class="line"><span class="comment"># Some magic here?</span></div><div class="line">    <span class="keyword">return</span> class_label</div></pre></td></tr></table></figure><p> <strong>no obvious way</strong> to hard-code the algorithm for recognizing a cat, or other classes.</p><h3 id="Data-Driven-Approach"><a href="#Data-Driven-Approach" class="headerlink" title="Data-Driven Approach"></a>Data-Driven Approach</h3><ol><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(images, labels)</span>:</span></div><div class="line"><span class="comment"># Machine learning</span></div><div class="line"><span class="keyword">return</span> model</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, test_images)</span>:</span></div><div class="line"><span class="comment"># Use model to predict labels</span></div><div class="line">    <span class="keyword">return</span> test_labels</div></pre></td></tr></table></figure><p>Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One called <strong>train</strong>, that’s going to input images and labels and then output a model, another function called <strong>predict</strong>, which will input the model and make predictions for images.</p><p>#Nearest Neighbor classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span>:</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line"><span class="keyword">pass</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></div><div class="line"><span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></div><div class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></div><div class="line">    self.Xtr = X</div><div class="line">    self.ytr = y</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">     <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></div><div class="line">    num_test = X.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></div><div class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</div><div class="line">    </div><div class="line">    <span class="comment"># loop over all test rows</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</div><div class="line"><span class="comment"># find the nearest training image to the i'th test image</span></div><div class="line">     <span class="comment"># using the L1 distance (sum of absolute alue differences)</span></div><div class="line">        distances = np.sum(np.abs(self.Xtr - X[i, :]), axis = <span class="number">1</span>)</div><div class="line">        min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></div><div class="line">Ypred[i] = self.ytr[min_index] <span class="comment">#predict the label of the nearest example</span></div><div class="line">     <span class="keyword">return</span> Ypred</div></pre></td></tr></table></figure><p>Q: With N examples. how fast are training and prediction?</p><p>A: Train O(1), predict O(N)</p><p>This is bad: we want classifiers that are <strong>fast</strong> at prediciton; <strong>slow</strong> for training is ok.</p><h3 id="k-Nearest-Neighbors"><a href="#k-Nearest-Neighbors" class="headerlink" title="k-Nearest Neighbors"></a>k-Nearest Neighbors</h3><p>Instead of copying label from nearest neighbor, thake <strong>majority vote</strong> form K closest points.</p><p>###Hyperparameters</p><ul><li>What is the best value of <strong>k</strong> to use?</li><li>What is the best <strong>distance</strong> to use?</li></ul><p>These are <strong>hyperparameters</strong>: choices about the algorithm that we set rather than learn</p><p>_Very problem-dependent._</p><p>_Must try them all out and see what works best._</p><h3 id="Setting-Hyperparameters"><a href="#Setting-Hyperparameters" class="headerlink" title="Setting Hyperparameters"></a>Setting Hyperparameters</h3><ul><li>Split data into <strong>train</strong>, <strong>val</strong>, and <strong>test</strong>; choose hyperparameters on val and evaluate on test</li><li><strong>Cross-Validation</strong>: Split data into **folds, try each fold as validation and average the results. Useful for small datasets but not used too frequently in deep learning.</li></ul><h3 id="k-Nearest-Neighbor-on-images-never-used"><a href="#k-Nearest-Neighbor-on-images-never-used" class="headerlink" title="k-Nearest Neighbor on images never used"></a>k-Nearest Neighbor on images never used</h3><ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li><li>Curse of dimensionality</li></ul><h3 id="k-Nearest-Neighbors-Summary"><a href="#k-Nearest-Neighbors-Summary" class="headerlink" title="k-Nearest Neighbors: Summary"></a>k-Nearest Neighbors: Summary</h3><ul><li>In <strong>Image classification</strong> we start with a <strong>training set</strong> of images and labels, and must predict labels on the <strong>test set</strong></li><li>The *K-Nearest Neighbors classifier predicts labels based on nearest training examples</li><li>Distance metric and K are <strong>hyperparameters</strong></li><li>Choose hyperparameters using the <strong>validation set</strong>; only run on the test set once at the very end!</li></ul><p>#Linear Classification</p><p>These deep neural networks are kind of like Legos and this linear classifier is kind of like the most basic building blocks of these giant networks.</p><p>f(x, W) = Wx + b</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Problems&quot;&gt;&lt;a href=&quot;#Problems&quot; class=&quot;headerlink&quot; title=&quot;Problems:&quot;&gt;&lt;/a&gt;Problems:&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.&lt;/li&gt;
&lt;li&gt;Viewpoint variation: All pixels change when the camera moves&lt;/li&gt;
&lt;li&gt;Illumination: There can be lighting conditions going on in the scene&lt;/li&gt;
&lt;li&gt;Deformation: Cats can assume a lot of different, varied poses and positions.&lt;/li&gt;
&lt;li&gt;Occlusion: You might only see a part of a cat.&lt;/li&gt;
&lt;li&gt;Background Clutter: The foreground of the cat look similar in appearance&lt;/li&gt;
&lt;li&gt;Intraclass variation: Cats can come in different shapes and sizes and colors and ages&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记二：朴素贝叶斯</title>
    <link href="http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"/>
    <id>http://yoursite.com/2017/10/18/machine-learning-in-action-note2/</id>
    <published>2017-10-18T00:52:21.000Z</published>
    <updated>2018-01-21T08:35:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p><p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p><p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p><p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母 <strong>P(X=x)</strong> 对所有c~i~都没差，那我们大可不必计算出这个值。</p><h3 id="朴素贝叶斯学习与分类的算法过程："><a href="#朴素贝叶斯学习与分类的算法过程：" class="headerlink" title="朴素贝叶斯学习与分类的算法过程："></a>朴素贝叶斯学习与分类的算法过程：</h3><p>输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x</p><p>输出：实例x的分类</p><p>1) 计算先验概率和条件概率</p><p>$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$</p><p>$$<br>P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}<br>$$</p><p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p><p>2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算<br>$$<br>P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><p>3) 确定实例x的类<br>$$<br>y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><a id="more"></a><h3 id="一个栗子：邮件分类问题"><a href="#一个栗子：邮件分类问题" class="headerlink" title="一个栗子：邮件分类问题"></a>一个栗子：邮件分类问题</h3><p>公式都有了，到底要如何实现呢？我们通过邮件分类问题来引入解决办法。</p><p>问题：假设有很多个邮件，标记为<strong>1-垃圾邮件</strong>，<strong>0-正常邮件</strong>，给定一个实例判断它属于哪一类。</p><p>思路：首先在啥都没有的情况下，我们要处理邮件，将其转化为python可以理解的list，并为出现的所有单词构建一个词汇表，每封邮件对应为词汇表上的一个向量，所有的邮件构成一个矩阵，利用矩阵计算出先验概率和条件概率，将给定实例转化为向量，通过比较大小确定实例的类。</p><ol><li><p>处理邮件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</div><div class="line">        wordList = textParse(open(<span class="string">'email/spam/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>,<span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList) <span class="comment"># 文档合集</span></div><div class="line">        fullText.extend(wordList) <span class="comment"># 单词合集</span></div><div class="line">        classList.append(<span class="number">1</span>)</div><div class="line">        wordList = textParse(open(<span class="string">'email/ham/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>, <span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">0</span>)</div></pre></td></tr></table></figure></li><li><p>构建词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></div><div class="line">    vocabSet = set([])</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</div><div class="line">        vocabSet = vocabSet | set(document) <span class="comment"># 不重复的单词</span></div><div class="line">    <span class="keyword">return</span> list(vocabSet)</div><div class="line"> </div><div class="line">vocabList = vocabSet(docList)</div></pre></td></tr></table></figure></li><li><p>将所有邮件分为训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">trainIndex = list(range(<span class="number">50</span>)); testIndex = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    randIndex = int(random.uniform(<span class="number">0</span>, len(trainIndex)))</div><div class="line">    testIndex.append(trainIndex[randIndex])</div><div class="line">    <span class="keyword">del</span>(trainIndex[randIndex])</div></pre></td></tr></table></figure></li><li><p>将所有邮件转换为矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></div><div class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabList:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span> <span class="comment"># 转换为词汇表对应向量</span></div><div class="line">    <span class="keyword">return</span>  returnVec</div><div class="line"></div><div class="line">trainMat = []; trainClasses = []</div><div class="line"><span class="keyword">for</span> docIndex <span class="keyword">in</span> trainIndex:</div><div class="line">trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) </div><div class="line">    trainClasses.append(classList[docIndex])</div></pre></td></tr></table></figure></li><li><p>计算先验概率和条件概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></div><div class="line">    numTrainDocs = len(trainMatrix)</div><div class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</div><div class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs) <span class="comment"># 先验概率 p(class=1)</span></div><div class="line"></div><div class="line">    <span class="comment"># 1/ Initialize probabilities</span></div><div class="line">    <span class="comment"># 拉普拉斯平滑处理</span></div><div class="line">    p0Num = ones(numWords) <span class="comment"># p(xi|c0)</span></div><div class="line">    p1Num = ones(numWords) <span class="comment"># p(xi|c1)</span></div><div class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</div><div class="line"></div><div class="line">        <span class="comment"># 2 / Vector addition</span></div><div class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</div><div class="line">            p1Num += trainMatrix[i]</div><div class="line">            p1Denom += sum(trainMatrix[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            p0Num += trainMatrix[i]</div><div class="line">            p0Denom += sum(trainMatrix[i])</div><div class="line"></div><div class="line">    <span class="comment"># 3/ Element-wise division</span></div><div class="line">    <span class="comment"># 条件概率</span></div><div class="line">    p1Vect = log(p1Num / p1Denom)</div><div class="line">    p0Vect = log(p0Num / p0Denom)</div><div class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</div><div class="line"></div><div class="line">p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</div></pre></td></tr></table></figure><p>让我们把条件概率用普通话稍微翻译一下：<br>$$<br>P(X=x_i|c_k) = \frac{单词x_i出现次数}{c_k类文档总单词数}<br>$$<br>把所有的 \(P(X=x_i|c_k)\) 捋到一起就是一个向量 piVcet。</p><p>在上面的代码中我们其实处理了两种极端情况：</p><ol><li>早些时候这个代码是<code>p0Num = zeros(numWords); p0Denom =0</code>（p1同理）。如果某个单词没有出现，当计算乘积 ΠP(X=x~i~|c~k~)时，结果就会为0，这显然就没有办法进行预测了嘛。拉普拉斯就提出用加1的方法估计没有出现过的现象的概率。</li><li>还有一个是下溢出问题。在计算乘积  \(\prod P(X=x_i|c_k)\)时，由于概率都是很小的数值，程序有可能下溢出而得不到正确答案。解决办法就是对乘积取自然对数。</li></ol></li><li><p>交叉验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></div><div class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</div><div class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1</span>-pClass1)</div><div class="line">    <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    </div><div class="line">    errorCount = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testIndex:</div><div class="line">        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])</div><div class="line">classifyResult = classifyNB(array(wordVector), p0V, p1V, pSpam)</div><div class="line">    <span class="keyword">if</span>  classifyResult != classList[docIndex]:</div><div class="line">   errorCount += <span class="number">1</span></div><div class="line">print(<span class="string">"the error rate is: "</span>, float(errorCount)/len(testIndex))</div></pre></td></tr></table></figure></li></ol><p>这里我们终于要计算\(\prod P(X=x_i|c_k)\)了。<code>vec2Classify _ piVec</code> 表示先找出实例有的单词，由于\(\ln(a_b) = ln(a)+ln(b)\)，条件概率的相乘便转换为矩阵元素相加。</p><p>🙂 适用于量少的数据集／可以处理多类别问题</p><p>🙁 对输入数据的准备方式较敏感</p><p>🛠 标称型数据</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。&lt;/p&gt;
&lt;p&gt;假设有i个分类，我们需要比较的其实是后验概率 &lt;strong&gt;P(Y=c~k~|X=x)&lt;/strong&gt; 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。&lt;/p&gt;
&lt;p&gt;那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：&lt;br&gt;$$&lt;br&gt;P(A|B) = P(A)\frac{P(B|A)}{P(B)}&lt;br&gt;$$&lt;br&gt;让我们来代入一下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}&lt;br&gt;$$&lt;br&gt;给一个训练集，&lt;strong&gt;P(Y=c~i~)&lt;/strong&gt;是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：&lt;br&gt;$$&lt;br&gt;P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)&lt;br&gt;$$&lt;br&gt;由于分母 &lt;strong&gt;P(X=x)&lt;/strong&gt; 对所有c~i~都没差，那我们大可不必计算出这个值。&lt;/p&gt;
&lt;h3 id=&quot;朴素贝叶斯学习与分类的算法过程：&quot;&gt;&lt;a href=&quot;#朴素贝叶斯学习与分类的算法过程：&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯学习与分类的算法过程：&quot;&gt;&lt;/a&gt;朴素贝叶斯学习与分类的算法过程：&lt;/h3&gt;&lt;p&gt;输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x&lt;/p&gt;
&lt;p&gt;输出：实例x的分类&lt;/p&gt;
&lt;p&gt;1) 计算先验概率和条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算&lt;br&gt;$$&lt;br&gt;P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;3) 确定实例x的类&lt;br&gt;$$&lt;br&gt;y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记一：kNN和决策树</title>
    <link href="http://yoursite.com/2017/10/15/machine-learning-in-action-note1/"/>
    <id>http://yoursite.com/2017/10/15/machine-learning-in-action-note1/</id>
    <published>2017-10-15T13:58:12.000Z</published>
    <updated>2017-11-06T14:02:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。</p><p>算法的一般流程为：收集数据 -&gt; 准备数据 -&gt; 分析数据 -&gt; 训练算法 -&gt; 测试算法 -&gt; 使用算法</p><p>第一个笔记本包括kNN分类算法和决策树算法。</p><h1 id="kNN分类算法"><a href="#kNN分类算法" class="headerlink" title="kNN分类算法"></a>kNN分类算法</h1><p>kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。</p><h3 id="kNN的一般流程"><a href="#kNN的一般流程" class="headerlink" title="kNN的一般流程"></a>kNN的一般流程</h3><ol><li>收集数据</li><li>准备数据：最好使用结构化数据格式，因为计算距离需要数值。</li><li>分析数据</li><li>训练算法：此步骤不适用于kNN算法</li><li>测试算法：计算错误率</li><li>使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。</li></ol><a id="more"></a><p>我们一般使用欧氏距离公式来计算距离（以两点为例）：</p><p>$$<br>d = \sqrt{(x_0-y_0)^2+(x_1-y_1)^2+…+(x_n-y_n)^2)}<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div></pre></td></tr></table></figure><p>对距离进行排序，并找出最小的k个距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line">classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k): </div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div></pre></td></tr></table></figure><p>最后对出现次数进行排序，预测结果即为出现次数最多的类别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line"><span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>原书中sorted的第一个参数为<code>classCount.iteritems</code>，根据python3作出修改。</p><p>完整的分类函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></div><div class="line">    <span class="string">"""k-Nearest Neighbors algorithm</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    :param inX: A row vector under test</span></div><div class="line"><span class="string">    :param dataSet: The training data</span></div><div class="line"><span class="string">    :param labels: Labels of training data</span></div><div class="line"><span class="string">    :return: Prediction for the class of inX</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Distance calculation</span></div><div class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div><div class="line">    sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line"></div><div class="line">    <span class="comment"># 2 Voting with lowest k distances</span></div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div><div class="line"></div><div class="line">    <span class="comment"># 3 Sort dictionary</span></div><div class="line">    sortedClassCount = sorted(classCount.items(),</div><div class="line">                              key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>🙂：准确度高／对异常值不敏感／不假设数据</p><p>🙁：计算复杂度高／占用大量内存</p><p>🛠：数值型／标称型</p><h1 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h1><p>决策树算法（Decision trees）也是一种常见的分类算法。我们每次用一个特征来对数据集进行分类，迭代直到分出的数据集都属于一个类别时，训练完成，训练模型即为一个树结构。</p><h3 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h3><ol><li>数据收集</li><li>准备数据：这个构造树的过程只适用于标称型数据，因此需要离散化连续的数值</li><li>分析数据</li><li>训练算法：构造一个树的数据结构</li><li>测试算法：使用经验树计算错误率</li><li>使用算法：这可以应用于任何监督学习任务。通常，决策树树可以更好的理解数据的内在含义。</li></ol><p>判断使用哪一个特征来进行分类即为训练算法的关键。在这里，我们使用特征的「信息增益」来判断。好了，我们得来复习一下「信息增益」。</p><p>说到信息增益就有一个不得不提的家伙叫「熵」，它是随机变量的平均量，代表了随机变量的不确定性。啥玩意儿？？？一个事件发生的概率越小，所含的信息量就越大，如果所有事件发生的概率都很小，平均信息量就比较大。讲人话。「熵」越大，数据集越混乱。</p><p>用公式来表示：<br>$$<br>H = -\sum_{i=1}^np(x_i)log_2p(x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></div><div class="line">    numEntries = len(dataSet)</div><div class="line"></div><div class="line">    labelCounts = &#123;&#125; <span class="comment"># 用一个字典记录一个类别的出现次数</span></div><div class="line">    <span class="keyword">for</span> featVect <span class="keyword">in</span> dataSet:</div><div class="line">        currentLabel = featVect[<span class="number">-1</span>]</div><div class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</div><div class="line">            labelCounts[currentLabel] = <span class="number">0</span></div><div class="line">        labelCounts[currentLabel] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># 计算熵</span></div><div class="line">    shannonEnt = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">        prob = float(labelCounts[key])/numEntries <span class="comment"># 概率</span></div><div class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> shannonEnt</div></pre></td></tr></table></figure><p>还有一个家伙叫「条件熵」，是已知条件下，随机变量的不确定性。即当我们固定了一个特征时，整个系统的信息量。然而这个特征的取值也不止一个，因此我们就需要求出它的平均值。</p><p>用公式来表示：<br>$$<br>H(C|X) = p_1H(C|x_1)+p_2(C|x_2)+…p_n(C|x_n) = \sum_{i=1}^np_iH(C|x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line">featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">newEntropy = <span class="number">0</span></div><div class="line">    </div><div class="line">    <span class="comment"># 计算条件熵</span></div><div class="line"><span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">      subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">   prob = len(subDataSet)/float(len(dataSet))</div><div class="line">   newEntropy += prob * calcShannonEnt(subDataSet)</div></pre></td></tr></table></figure><p>接着又来了一个家伙，它就是「信息增益」。<strong>信息增益 = 熵 - 条件熵</strong>。这个信息增益代表着，系统固定一个特征后，<strong>不确定性的减少程度</strong>。我们当然希望这个减少程度尽可能大，使得系统更加有组织纪律，那么信息增益很大就表明这个特征很关键！！</p><p>我们将上面的内容整理进一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></div><div class="line">    </div><div class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment"># 获取特征的数量</span></div><div class="line">    baseEntropy = calcShannonEnt(dataSet)</div><div class="line">    bestInfoGain = <span class="number">0</span>; bestFeature = <span class="number">-1</span></div><div class="line">    </div><div class="line">    <span class="comment"># 分别计算每一个特征的信息增益</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line"></div><div class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">        uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">        newEntropy = <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># 计算条件熵</span></div><div class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">            subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">            prob = len(subDataSet)/float(len(dataSet))</div><div class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line">        <span class="comment"># 计算信息增益</span></div><div class="line">        infoGain = baseEntropy - newEntropy</div><div class="line">        </div><div class="line">        <span class="comment"># 找到最大的信息增益</span></div><div class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line">    <span class="keyword">return</span>  bestFeature <span class="comment"># 返回最佳分类特征</span></div></pre></td></tr></table></figure><p>整理到这里，其实有一个疑惑，我觉得信息增益并没有什么卵用，每次找出条件熵最小的不就行了？？？经实验是可以的。</p><p>在treePlotter.py中发现一个神奇的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></div><div class="line">    <span class="comment"># 2 Draws annotations with arrows</span></div><div class="line">    createPlot.ax.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           xytext = centerPt, textcoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodeType, arrowprops=arrow_args)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></div><div class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</div><div class="line">    fig.clf()</div><div class="line">    createPlot.ax = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>)</div><div class="line">    plotNode(<span class="string">'a decision node'</span>, (<span class="number">.5</span>, <span class="number">.1</span>), (<span class="number">.1</span>, <span class="number">.5</span>), decisionNode)</div><div class="line">    plotNode(<span class="string">'a leaf node'</span>, (<span class="number">.8</span>, <span class="number">.1</span>), (<span class="number">.3</span>, <span class="number">.8</span>), leafNode)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Python中一切皆为对象，在createPlot函数中，为这个函数对象绑定了一个属性ax，变成了一个全局的变量，可以在plotNode函数中调用。</p><p>书中作者利用treePlotter中写好的树结构来进行预测，但tree.py里头不有一个createDataSet函数和createTree函数吗？尝试一下利用这两个函数来生成并预测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels)</div></pre></td></tr></table></figure><p>Buuuuuuuut……….</p><p><img src="/2017/10/15/machine-learning-in-action-note1/no_surfacing_error.png" alt=""></p><p>检查了一下调用createTree函数前后labels的值，发现调用前后labels的值发生了变化，因此对原函数createTree稍作修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></div><div class="line"></div><div class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Stop when all classes are equal</span></div><div class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</div><div class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 2 When just one feature, return majority</span></div><div class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> majorityCnt(classList)</div><div class="line"></div><div class="line">    subLabels = labels[:] <span class="comment"># 将labels全部复制到subLabels，进行接下来的处理</span></div><div class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">    bestFeatLabel = subLabels[bestFeat] <span class="comment"># 利用subLabels来获取最佳分类特征</span></div><div class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</div><div class="line">    <span class="keyword">del</span>(subLabels[bestFeat])</div><div class="line"></div><div class="line">    <span class="comment"># 3 Get list of unique values</span></div><div class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">    uniqueVals = set(featValues)</div><div class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">        <span class="comment">#subLabels = labels[:] 作者在这里才进行参数复制，导致labels的值发生改变</span></div><div class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</div><div class="line">    <span class="keyword">return</span> myTree</div></pre></td></tr></table></figure><p>接下来就可以愉快利用该函数进行分类了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> treePlotter</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">省略一堆作者源代码</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels) <span class="comment"># 训练数据</span></div><div class="line">treePlotter.createPlot(myTree) <span class="comment"># 显示模型</span></div><div class="line">print(classify(myTree, labels, [<span class="number">1</span>, <span class="number">1</span>])) <span class="comment"># 预测数据</span></div></pre></td></tr></table></figure><p>看一看生成的决策树模型：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/tree1.png" alt=""></p><p>我们可以引入pickle模块来将训练出的模型序列化，保存在磁盘中，以便后续的调用。因为书本作者是使用Python2的，我打算用Python3来完成，在下面的代码中就遇到了问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></div><div class="line">    <span class="keyword">import</span> pickle</div><div class="line">    <span class="comment">#Python2用法：fw = open(filename,'w')</span></div><div class="line">    <span class="comment">#改为python3:</span></div><div class="line">    <span class="keyword">with</span> open(filename,<span class="string">'wb'</span>) <span class="keyword">as</span> fw:</div><div class="line">        pickle.dump(inputTree, fw)</div><div class="line">    fw.close()</div></pre></td></tr></table></figure><p>接下来就要引入稍微大一点的数据集来进行训练了，然鹅….</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fr = open(<span class="string">'lenses.txt'</span>)</div><div class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readline()]</div><div class="line">lensesLables = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</div><div class="line">print(lenses)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/readlines_error.png" alt=""></p><p>这输出的啥玩意儿？仔细一看，<strong>fr.readlines()</strong>函数写错了，少了一个<strong>sssssssss</strong>。修改好以后我们就来看看训练完的决策树吧：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree1.png" alt=""></p><p>？？？这又啥玩意儿？？这看得下去？？？？那就只能修改一下plotMidText函数了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></div><div class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>])/<span class="number">2</span> + cntrPt[<span class="number">0</span>]</div><div class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>])/<span class="number">2</span> + cntrPt[<span class="number">1</span>]</div><div class="line">    createPlot.ax.text(xMid, yMid, txtString, fontsize=<span class="number">8</span>, horizontalalignment=<span class="string">'center'</span>,verticalalignment=<span class="string">'center'</span>, rotation=<span class="number">30</span>)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree2.png" alt=""></p><p>完美！</p><p>🙂：计算复杂度不高／便于人们理解学习结果／对中间的缺失值不敏感／可以处理无关的特征值</p><p>🙁：可能会过拟合</p><p>🛠：数值型／标称型</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。&lt;/p&gt;
&lt;p&gt;算法的一般流程为：收集数据 -&amp;gt; 准备数据 -&amp;gt; 分析数据 -&amp;gt; 训练算法 -&amp;gt; 测试算法 -&amp;gt; 使用算法&lt;/p&gt;
&lt;p&gt;第一个笔记本包括kNN分类算法和决策树算法。&lt;/p&gt;
&lt;h1 id=&quot;kNN分类算法&quot;&gt;&lt;a href=&quot;#kNN分类算法&quot; class=&quot;headerlink&quot; title=&quot;kNN分类算法&quot;&gt;&lt;/a&gt;kNN分类算法&lt;/h1&gt;&lt;p&gt;kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。&lt;/p&gt;
&lt;h3 id=&quot;kNN的一般流程&quot;&gt;&lt;a href=&quot;#kNN的一般流程&quot; class=&quot;headerlink&quot; title=&quot;kNN的一般流程&quot;&gt;&lt;/a&gt;kNN的一般流程&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;收集数据&lt;/li&gt;
&lt;li&gt;准备数据：最好使用结构化数据格式，因为计算距离需要数值。&lt;/li&gt;
&lt;li&gt;分析数据&lt;/li&gt;
&lt;li&gt;训练算法：此步骤不适用于kNN算法&lt;/li&gt;
&lt;li&gt;测试算法：计算错误率&lt;/li&gt;
&lt;li&gt;使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex8</title>
    <link href="http://yoursite.com/2017/06/16/machine-learning-ex8/"/>
    <id>http://yoursite.com/2017/06/16/machine-learning-ex8/</id>
    <published>2017-06-16T11:46:41.000Z</published>
    <updated>2017-10-21T09:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。</p><p>这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。</p><a id="more"></a><h3 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h3><p>首先计算出 <strong>μ</strong> 和 <strong>σ^2^ </strong>：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mu = mean(X)';</div><div class="line">sigma2 = var(X)'*(m<span class="number">-1</span>)/m;</div></pre></td></tr></table></figure><p>在这里要注意函数 var() 除以的是m-1所以我们要修改一下函数。然后我们要利用交叉验证样本，计算 F~1~ Score 并挑选临界值 <strong>ε</strong>：</p> <figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 获取验证集异常值坐标</span></div><div class="line">cvPredictions = (pval &lt; epsilon);</div><div class="line">tp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line">fp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">0</span>) );</div><div class="line">fn = sum( (cvPredictions == <span class="number">0</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line"><span class="comment">% 计算精确率</span></div><div class="line">prec = tp / (tp + fp);</div><div class="line"><span class="comment">% 计算召回率</span></div><div class="line">rec = tp / (tp + fn);</div><div class="line">F1 = <span class="number">2</span> * prec * rec / (prec + rec);</div></pre></td></tr></table></figure><p>然后可以看到红红的圈出的异常值：</p><img src="/2017/06/16/machine-learning-ex8/2017/06/16/machine-learning-ex8/detected.png" alt="detected.png" title=""><h3 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h3><p>说到推荐系统，以电影为例，一方面要预测用户对于某电影的评分，另一方面要寻找相似的电影，我们经常使用的算法是协同过滤算法。进一步了解这个算法，查了一些中文资料。原本的线性回归，我们只需要根据特征值计算出参数 <strong>θ</strong>，但是现在变态了，我们不光要预测用户的喜好，还要查找相似的特征向量，俩参数（都用矩阵表示）一起学习。</p><p>首先我们完成未正规化的梯度和代价函数的计算：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_grad = (X * Theta' - Y) .* R * Theta;</div><div class="line">Theta_grad = (X * Theta' - Y)' .* R' * X;</div><div class="line"></div><div class="line">J = sum(( (X * Theta' - Y).^<span class="number">2</span> .* R )(:)) / <span class="number">2</span>;</div></pre></td></tr></table></figure><p>期间完全忘记梯度是什么鬼。。回顾一下，是代价函数对变量求偏导~ 计算公式完全按照矩阵的大小来判断。接下来我们正规化代价函数，按照公式加上<code>J += lambda / 2 _ sum(Theta.^2(:)) + lambda / 2 _ sum(X.^2(:));</code>，但发现J变成了1x3的向量，发现问题在于sum中应该在平方的时候添加括号，修改代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J += lambda / <span class="number">2</span> * sum((Theta.^<span class="number">2</span>)(:)) + lambda / <span class="number">2</span> * sum((X.^<span class="number">2</span>)(:));</div></pre></td></tr></table></figure><p>然后继续完成梯度的正规化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。&lt;/p&gt;
&lt;p&gt;这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Mmachine Learning ex7</title>
    <link href="http://yoursite.com/2017/06/13/machine-learning-ex7/"/>
    <id>http://yoursite.com/2017/06/13/machine-learning-ex7/</id>
    <published>2017-06-13T01:46:37.000Z</published>
    <updated>2017-06-15T04:38:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章主要学习了 <strong>K-均值算法</strong> 和 <strong>PCA 算法</strong>，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。</p><a id="more"></a><h3 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h3><p>我们先使用2维的数据集来感受一下K均值算法。接着我们要将写好的函数运用到图像压缩上。K-均值算法最核心的步骤如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Initialize centroids</span></div><div class="line">centroids = kMeansInitCentroids(X, K);</div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:iterations</div><div class="line"><span class="comment">% Cluster assignment step: Assign each data point to the</span></div><div class="line"><span class="comment">% closest centroid. idx(i) corresponds to cˆ(i), the index</span></div><div class="line"><span class="comment">% of the centroid assigned to example i</span></div><div class="line">idx = findClosestCentroids(X, centroids);</div><div class="line"><span class="comment">% Move centroid step: Compute means based on centroid</span></div><div class="line"><span class="comment">% assignments</span></div><div class="line">centroids = computeMeans(X, idx, K);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>第一步完成 <strong>findClosesCentroids</strong> 函数，计算每个样本到中心点的距离，用数值表示其所属类，返回聚类后的向量：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(X,<span class="number">1</span>)</div><div class="line">  min = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="number">1</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">  min_idx = <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">2</span> : K</div><div class="line">    cur = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="built_in">j</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">    <span class="keyword">if</span>(cur &lt; min)</div><div class="line">      min = cur;</div><div class="line">      min_idx = <span class="built_in">j</span>;</div><div class="line">    <span class="keyword">end</span></div><div class="line">  <span class="keyword">end</span></div><div class="line">  idx(<span class="built_in">i</span>) = min_idx;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>题解里说用一个for循环完成，但我先使用了两个，后面再进行优化好了。这其中出现了一个小问题，计算norm的时候，<code>norm(X(i,:) - centroids(j,:), 2).^2;</code> 主要包含所有列，否则只包含了单个数字。</p><p>接下来完成 <strong>computeMeans</strong> 函数，通过同个类里的样本计算新的中心：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : K</div><div class="line">  <span class="comment">% 查找聚类样本</span></div><div class="line">  examples_idx = <span class="built_in">find</span>(idx == <span class="built_in">i</span>);</div><div class="line">  <span class="comment">% 计算中心值</span></div><div class="line">  X(examples_idx, :);</div><div class="line">  centroids(<span class="built_in">i</span>,:) = sum( X(examples_idx, :) ) / <span class="built_in">size</span>(examples_idx, <span class="number">1</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>这里出现的问题在<code>centroids(i,:) = sum( X(examples_idx, :) ) / size(examples_idx, 1);</code> 。注意包含centroids的所有列，和在计算szie()的时候，选择所有行的大小。</p><p>然后我们就可以运行我们的K-均值算法来聚类了。可以看到初始化的时候三个中心点分别为（3,3）、（6,2）、（8,5）。</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter1.png" alt="iter1.png" title=""><p>经过6次迭代可以看到中心点逐渐往好的方向移动：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter6.png" alt="iter6.png" title=""><p>经过10次迭代基本到达有模有样的聚类中心了：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter10.png" alt="iter10.png" title=""><p>接下来我们要使用K均值算法来压缩图像了。每个像素用 <strong>24-bit</strong> 来表示颜色，即3个 <strong>8-bit</strong> 的无符号整形来表示RGB的值，目标压缩成用<strong>16-bit</strong> 来表示。</p><p>我们使用 <strong>imread(A)</strong> 来读入图像，因为我们原始图像的大小为128x128，因此 A 为一个三维矩阵，<strong>size(A) = 128x128x3</strong> ，然后reshape图像变为 <strong>mX3</strong>（m = 16384 = 128x128） 的矩阵，用于后面执行K-均值算法。</p><p>我们将K设置为16，对每个像素点进行聚类。获得最后的图像。原始图像需要 <strong>128 x 128 x 24 = 393,216</strong> bits。而压缩后我们使用 24bits存储16种颜色，但每个像素点只需要4bits 来进行定位。因此压缩后的图像为 <strong>16 x 24+128x128x4 = 65,920</strong> bits。（从pdf里理解是这样，然而真实的情况似乎并没有进行压缩，看了助教在论坛里的回答，完整的压缩过程还要创建一个新的图像，只使用4bit来表示，因为本课程只是关于聚类，而不是关于压缩图像的细节。）因此压缩后的图片如下：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress.png" alt="compress.png" title=""><p>然后我们换一张自己的图片来看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress2.png" alt="compress2.png" title=""><p>然后修改 <strong>K=5</strong> 再看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress3.png" alt="compress3.png" title=""><h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h3><p>我们使用PCA来减少数据集的维度。先实验2D的数据集来了解一下PCA的运作，再将其运用更高的维度上。</p><p>PCA的步骤为：1、正规化数据集。2、计算协方差矩阵。3、利用SVD函数计算主成分（U、S）。</p><p>2、3步骤实现</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sigma = X' * X / m;</div><div class="line">[U, S, V] = svd(Sigma);</div></pre></td></tr></table></figure><p>可以看到特征向量如下图：</p><p><strong>特征向量1</strong></p><p><strong>特征向量1 &amp; 2</strong></p><p>因为我们要降低到一维，即 <strong>K=1</strong> 因此在这次只会使用到U(:, 1)</p><p>通过SVD函数计算出了主成分后，我们可以就利用特征向量U来将每个样本映射为更低的维度，x^(i)^ -&gt; z^(i)^ 。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">U_reduce = U(:, <span class="number">1</span>:K);</div><div class="line">Z = X * U_reduce;</div></pre></td></tr></table></figure><p>重构出通过PCA映射后的点（红色）：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/reconstruction.png" alt="reconstruction.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章主要学习了 &lt;strong&gt;K-均值算法&lt;/strong&gt; 和 &lt;strong&gt;PCA 算法&lt;/strong&gt;，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
