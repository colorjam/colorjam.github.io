<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-12T05:26:00.080Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>每周论文 Vol.01</title>
    <link href="http://yoursite.com/2019/05/11/weekly-paper-01/"/>
    <id>http://yoursite.com/2019/05/11/weekly-paper-01/</id>
    <published>2019-05-10T23:57:11.000Z</published>
    <updated>2019-05-12T05:26:00.080Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019"><a href="#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019" class="headerlink" title="1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)"></a>1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)</h3><p>基于度量的一种剪枝方法。度量的是权重之间的连接敏感度。优点在于不需要layer-by-layer的训练过程。</p><p>砍掉某个连接$j$对loss的影响：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D})=L(\mathbf{1} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{1}-\mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)<br>$$<br>但是上式不可导，作者用$g_j$来代表评估指标，将离散的$e_j$松弛为连续值$\delta e_j$：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D}) \approx g_{j}(\mathbf{w} ; \mathcal{D})=\left.\frac{\partial L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})}{\partial c_{j}}\right|_{\mathbf{c}=1}=\lim _{\delta \rightarrow 0}\left.\frac{L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{c}-\delta \mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)}{\delta}\right|_{\mathbf{c}=1}<br>$$<br>定义了连接敏感度：<br>$$<br>s_{j}=\frac{\left|g_{j}(\mathbf{w} ; \mathcal{D})\right|}{\sum_{k=1}^{m}\left|g_{k}(\mathbf{w} ; \mathcal{D})\right|}<br>$$<br>算法过程：</p><p><img src="https://i.loli.net/2019/05/09/5cd3a33306172.png" alt=""></p><blockquote><p>连接敏感度的计算方式简单，就像是在权重上加了一些噪音，看一下它对loss的影响。但是它的算法过程是对随机初始化的网络权重进行剪枝，再训练q。这样的剪枝是否有意义？</p></blockquote><h3 id="2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks"><a href="#2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks" class="headerlink" title="2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks"></a>2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks</h3><p>📍<a href="https://github.com/anibali/dsntnn" target="_blank" rel="external">https://github.com/anibali/dsntnn</a></p><p>本文提出了一个空间可微的数值转换操作(DSNT)来找到输入图像的显著点坐标。</p><p>传统方法有<strong>Heatmap matching</strong>和<strong>Fully connected</strong>，代表性的运用是<em>Human pose estimation</em>和<em>STN</em>。前者不完全可微，后者缺乏空间泛化能力。</p><p><img src="https://i.loli.net/2019/05/07/5cd10b71062f2.png" alt=""></p><p>DSNT的输入是一个单通道归一化的大小为$m \times n$ 的 heatmap $\hat{Z}$，代表了概率分布。输出是显著点的坐标，即概率分布最大点的那个坐标值。用两个相同大小的矩阵$X$和$Y$分别代表$x-$和$y-$坐标，让坐标分布变为左上角是(-1, -1)，右下角是(1,1)。</p><p>我们可以通过坐标$c$计算出这个概率函数：<br>$$<br>\operatorname{Pr}\left(\mathbf{c}=\left[ \begin{array}{ll}{X_{i, j}} &amp; {Y_{i, j}}\end{array}\right]\right)=\hat{Z}_{i, j}<br>$$<br>传统方法利用的是$c$的模，DSNT利用的是$c$的期望$\boldsymbol{\mu}=\mathbb{E}[\mathbf{c}]$，可以用如下公式表示：<br>$$<br>\operatorname{DSNT}(\hat{Z})=\boldsymbol{\mu}=\left[\langle\hat{Z}, \boldsymbol{X}\rangle_{F}<br>\quad\langle\hat{\boldsymbol{Z}}, \boldsymbol{Y}\rangle_{F}\right]<br>$$</p><blockquote><p>The “mode” is the value that occurs most often. The “mean” is the “average” you’re used to, where you add up all the numbers and then divide by the number of numbers.</p></blockquote><p>一个直观的例子：</p><p><img src="https://i.loli.net/2019/05/07/5cd1422432340.png" alt=""></p><p>DSNT的损失函数由两个部分组成：<br>$$<br>\mathcal{L}(\hat{Z}, \boldsymbol{p})=\mathcal{L}_{e u c}(\operatorname{DSNT}(\hat{Z}), \boldsymbol{p})+\lambda \mathcal{L}_{r e g}(\hat{Z})<br>$$<br>第一部分直接计算预测坐标和gt的欧式距离：<br>$$<br>\mathcal{L}_{e u c}(\boldsymbol{\mu}, \boldsymbol{p})=|\boldsymbol{p}-\boldsymbol{\mu}|_{2}<br>$$<br>第二部分是正则项，文章对比了两种正则方法：</p><ol><li><p>方差正则：<br>$$<br>\begin{aligned} \operatorname{Var}\left[\mathrm{c}_{x}\right] &amp;=\mathbb{E}\left[\left(\mathrm{c}_{x}-\mathbb{E}\left[\mathrm{c}_{x}\right]\right)^{2}\right] \\ &amp;=\left\langle\hat{Z},\left(\boldsymbol{X}-\mu_{x}\right) \odot\left(\boldsymbol{X}-\mu_{x}\right)\right\rangle_{F} \end{aligned}<br>$$</p><p>$$<br>\mathcal{L}_{v a r}(\hat{Z})=\left(\operatorname{Var}\left[c_{x}\right]-\sigma_{t}^{2}\right)^{2}+\left(\operatorname{Var}\left[\mathrm{c}_{y}\right]-\sigma_{t}^{2}\right)^{2}<br>$$</p></li><li><p>分布正则（KL散度/JS散度）：<br>$$<br>\mathcal{L}_{D}(\hat{Z}, \boldsymbol{p})=D\left(p(\mathbf{c}) | \mathcal{N}\left(\boldsymbol{p}, \sigma_{t}^{2} \boldsymbol{I}_{2}\right)\right)<br>$$</p></li></ol><h3 id="3️⃣-Searching-for-MobileNetV3"><a href="#3️⃣-Searching-for-MobileNetV3" class="headerlink" title="3️⃣ Searching for MobileNetV3"></a>3️⃣ Searching for MobileNetV3</h3><ul><li><p>用搜索的方式搜索网络结构</p><ul><li>Block-wise：用MnasNet-A1作为初始大网络，修改了反馈$A C C(m) \times[L A T(m) / T A R]^{w}$</li><li>Layer-wise：NetAdapt的剪枝方式，修改了评估指标$\frac{\Delta \mathrm{Acc}}{|\Delta \mathrm{latency}|}$</li></ul></li><li><p>加入了SEblock，重新设计了网络的输出层</p><p><img src="https://i.loli.net/2019/05/09/5cd39f284e1fb.png" alt=""></p><p><img src="https://i.loli.net/2019/05/09/5cd39f84c5807.png" alt=""></p></li><li><p>基于swish提出了h-swish替代ReLU<br>$$<br>\mathrm{h}-\operatorname{swish}[x]=x \frac{\operatorname{Re} \mathrm{LU} 6(x+3)}{6}<br>$$</p></li><li><p>在语义分割任务上基于R-ASPP，提出了LR-ASSP。</p><p><img src="https://i.loli.net/2019/05/09/5cd3a1b27ccf1.png" alt=""></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019&quot;&gt;&lt;a href=&quot;#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-
      
    
    </summary>
    
    
      <category term="paper, 每周论文" scheme="http://yoursite.com/tags/paper-%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>namesilo域名购买 &amp; github pages域名设置</title>
    <link href="http://yoursite.com/2019/05/10/set-blog-domain/"/>
    <id>http://yoursite.com/2019/05/10/set-blog-domain/</id>
    <published>2019-05-10T15:42:05.000Z</published>
    <updated>2019-05-12T04:50:42.535Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-域名购买和设置"><a href="#1-域名购买和设置" class="headerlink" title="1. 域名购买和设置"></a>1. 域名购买和设置</h3><p>首先在<a href="http://xn--eqrt2g.xn--vuq861b/#" target="_blank" rel="external">工信部公示</a>查看可备案的网站后缀，在<a href="https://www.namesilo.com/" target="_blank" rel="external">namesilo</a>上购买喜欢的域名，可以使用支付宝付款。</p><ul><li>进入「域名管理」，点击刚申请到的域名</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd613b55c18e.png" alt=""></p><ul><li>更新「DNS记录」</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd612451ffa6.png" alt=""></p><p>在其中添加以下三条记录：</p><p><img src="https://i.loli.net/2019/05/11/5cd612ba05f60.png" alt=""></p><blockquote><p>A记录类型在<a href="https://help.github.com/en/articles/troubleshooting-custom-domains#dns-configuration-errors" target="_blank" rel="external">这里</a>查看Github的地址，最后一行CNAME是自己的<code>usernmae.github.io</code>，实际上A记录和CNAME指向的是相同的IP地址，设置一项即可。</p></blockquote><h3 id="2-修改域名DNS服务器"><a href="#2-修改域名DNS服务器" class="headerlink" title="2. 修改域名DNS服务器"></a>2. 修改域名DNS服务器</h3><ul><li>在namesilo上将域名服务商修改为<a href="https://www.dnspod.cn/console/dns" target="_blank" rel="external">DNSpod</a></li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6152a1859a.png" alt=""></p><blockquote><p>DNSpod提供了两个免费的DNS地址：f1g1ns1.dnspod.net / f1g1ns2.dnspod.net</p></blockquote><ul><li>登陆DNSpod，添加域名和记录</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6160b109d2.png" alt=""></p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190511082546709.png" alt="image-20190511082546709"></p><h3 id="3-更新Github上的自定义域名"><a href="#3-更新Github上的自定义域名" class="headerlink" title="3. 更新Github上的自定义域名"></a>3. 更新Github上的自定义域名</h3><p>进入本地存储博客的文件夹</p><p><img src="https://i.loli.net/2019/05/11/5cd6186e83770.png" alt=""></p><p>创建一个没有后缀的<code>CNAME</code>文件，输入购买的域名。</p><p><img src="https://i.loli.net/2019/05/11/5cd6190c11dab.png" alt=""></p><p>部署到github上，就可以利用自定义的域名访问博客啦～</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</div></pre></td></tr></table></figure><p><strong>参考链接：</strong></p><ul><li><a href="http://cps.ninja/2016/10/09/customize-your-blog-domain/" target="_blank" rel="external">http://cps.ninja/2016/10/09/customize-your-blog-domain/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-域名购买和设置&quot;&gt;&lt;a href=&quot;#1-域名购买和设置&quot; class=&quot;headerlink&quot; title=&quot;1. 域名购买和设置&quot;&gt;&lt;/a&gt;1. 域名购买和设置&lt;/h3&gt;&lt;p&gt;首先在&lt;a href=&quot;http://xn--eqrt2g.xn--vuq861
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>沿着某个维度进行操作</title>
    <link href="http://yoursite.com/2019/05/10/along-axis/"/>
    <id>http://yoursite.com/2019/05/10/along-axis/</id>
    <published>2019-05-10T04:40:24.000Z</published>
    <updated>2019-05-10T04:57:27.083Z</updated>
    
    <content type="html"><![CDATA[<p>一直对沿着axis进行操作不太理解，今天决定还是要搞明白。</p><p>比如<code>np.sum(a, aixs=1)</code>，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动<code>a[0][0],a[0][1], a[0][2]​</code>。<br><img src="https://pic3.zhimg.com/80/v2-7a0716230a6f3d4840a6098001b1d2a2_hd.jpg" alt="img"></p><p>于是将红色框的元素相加得到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],</div><div class="line">[<span class="number">9</span>],</div><div class="line">[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>如果是<code>np.sum(a, axis=0)</code>，即<code>a[0][0],a[1][0], a[2][0]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],[<span class="number">9</span>],[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://zhuanlan.zhihu.com/p/31275071" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/31275071</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直对沿着axis进行操作不太理解，今天决定还是要搞明白。&lt;/p&gt;
&lt;p&gt;比如&lt;code&gt;np.sum(a, aixs=1)&lt;/code&gt;，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动&lt;code&gt;a[0][0],a[0][1], a[0][2]​&lt;/code
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令 - 显示文件指定行</title>
    <link href="http://yoursite.com/2019/05/05/linux-cat/"/>
    <id>http://yoursite.com/2019/05/05/linux-cat/</id>
    <published>2019-05-05T01:42:07.000Z</published>
    <updated>2019-05-12T05:29:04.159Z</updated>
    
    <content type="html"><![CDATA[<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">head -n [行数a] # 显示开始a行</div><div class="line">tail -n [行数a] # 显示最后a行</div><div class="line">tail -n +[行数a] # 从a行以后开始显示</div></pre></td></tr></table></figure><p>单独使用 <code>tail</code> 和 <code>head</code> 比较简单，组合使用时它们的顺序是有讲究的。</p><p>从200行开始显示10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | tail -n +200 | head -n 10</div></pre></td></tr></table></figure><p>等同于显示200行到210行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | head -n 210 | tail -n +200</div></pre></td></tr></table></figure><h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><p>还有一种使用<code>sed</code>命令的方式，从200行开始现实10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sed -n <span class="string">'200, 210p'</span> [filename]</div></pre></td></tr></table></figure><p>参考链接：</p><ul><li><a href="https://blog.csdn.net/vitaminc4/article/details/78136696" target="_blank" rel="external">https://blog.csdn.net/vitaminc4/article/details/78136696</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;cat&quot;&gt;&lt;a href=&quot;#cat&quot; class=&quot;headerlink&quot; title=&quot;cat&quot;&gt;&lt;/a&gt;cat&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;d
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/08/06/adversiarial-compression/"/>
    <id>http://yoursite.com/2018/08/06/adversiarial-compression/</id>
    <published>2018-08-06T02:13:32.000Z</published>
    <updated>2018-08-17T07:36:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对抗网络压缩"><a href="#对抗网络压缩" class="headerlink" title="对抗网络压缩"></a>对抗网络压缩</h2><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrfwrb0vj314e0fwq4u.jpg" alt="network_architecture"></p><p>对抗网络压缩过程是在老师和学生之间进行博弈，判别器的任务是判别输入样本是来自老师or学生。老师网络事先用标签训练完成，在对抗压缩的过程中不更新。</p><h3 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h3><ol><li>D接受两个网络最后一层的特征图$f^k_{t}(x)$和$f^l_s(x)$作为输入，利用交叉熵进行real/fake判别（蓝色线）</li><li>利用学生和老师的logits计算L2损失，引导学生模仿老师，产生输出（绿色线）</li><li>D接受学生网络dropout后的特征作为输入（红色线），欺骗D将其判别为real。</li></ol><h3 id="目标函数："><a href="#目标函数：" class="headerlink" title="目标函数："></a>目标函数：</h3><p>借鉴cGAN的思想，定义对抗压缩的损失函数为：</p><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrynfkouj316e04cdgr.jpg" alt=""></p><p>由于利用的是特征图作为D的输入，学生网络中还有许多参数需要更新，因此需要加入一个损失函数，减少老师网络和学生网络输出数据的差异：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftzs7ncxe2j315o03c0t5.jpg" alt=""></p><p>最后再加上正则项，得出最终的优化目标：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftzs779uxaj315i03m0t9.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;对抗网络压缩&quot;&gt;&lt;a href=&quot;#对抗网络压缩&quot; class=&quot;headerlink&quot; title=&quot;对抗网络压缩&quot;&gt;&lt;/a&gt;对抗网络压缩&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/0069RVTdly1ftzr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/07/29/compression_models/"/>
    <id>http://yoursite.com/2018/07/29/compression_models/</id>
    <published>2018-07-29T06:39:32.000Z</published>
    <updated>2018-11-12T08:00:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟</p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><p>论文：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">Aggregated Residual Transformations for Deep Neural Networks</a></p><p>ResNeXt提出了聚集变换(aggregated transofrmations)：<br>$$<br>\mathcal { F } ( \mathbf { x } ) = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } )<br>$$<br>$C$为基数(cardinality)，表明进行聚集变换的集合大小。$\mathcal { T } _ { i } ( \mathbf { x } )$代表一系列变换。</p><p><strong>ResNext与ResNet</strong></p><p>ResNet block：$\mathbf { y } = \mathcal { F } \left( \mathbf { x } , \left\{ W _ { i } \right\} \right) + \mathbf { x }$</p><p>ResNeXt block：$\mathbf { y } = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } ) + \mathbf { x }$</p><p>在计算代价相同的情况下，画出来就是下图的东东（左边的代表ResNet block，右边代表ResNeXt block）：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftrwnc8rz2j30t60cuq4s.jpg" alt="resnet_resnext"></p><p>ResNeXt将ResNet的一个大变换，分解为一系列小变换，然后将结果聚集起来（相加）。两个结构都做了跳跃连接。</p><p><strong>ResNeXt、Inception、Group Convolution</strong></p><p>文章还比较了与Inception组件和组卷积的关系，如下图所示：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftrwicz2c8j31f80fs0wu.jpg" alt=""></p><p>文章在后续的实验中表明，在计算代价近似的情况下，基数$C$越多，准确率越高。</p><h2 id="clcNet"><a href="#clcNet" class="headerlink" title="clcNet"></a>clcNet</h2><p>论文：<a href="https://arxiv.org/abs/1712.06145" target="_blank" rel="external">clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions</a></p><p>本文将深度卷积和组卷积归为一种通用卷积操作——通道局部卷积(channel local convolution, CLC)，并提出了通道依赖图(channel dependency graph, CDG)的表示方法。下图表示了常规卷积(a)、组卷积(b)、深度卷积(c)的CDG。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftry0ix2cwj30vm0b8q4u.jpg" alt=""></p><blockquote><p>CDG这个概念之前看的一篇文章中也提到了：<a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="external">Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast</a>，很直观地对比了ResNet、ResNeXt、MobileNet以及ShuffleNet的组件。</p></blockquote><p>通过CDG可以很明显的看出通道之间的依赖关系（箭头由输出指向输入）。这个依赖关系可以用通道感知域(channel receptive field, CRF)来表示。CRF代表每个输出通道所依赖的输入通道，CRF大小为每个输出通道依赖的输入通道数量。</p><p>分析上图三种卷积操作的CRF大小（size）：(a) $size = 6$，(b) $size =6/3 = 2$，(c) $size=1$</p><p>在这些概念的基础上，作者提出了<strong>全通道感知域(full channel receptive field, FCRF)</strong>，FCRF意味着_CRF大小 = 输入通道数_，即每个输出通道都依赖于所有的输入通道。作者表明FCRF能够更有效地表示特征，并且在实验中能获得更高的准确率。</p><p>作者还提出了交叉组卷积(interlaced group convolution, IGC)，如下图(b)所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt3uwgee8j30vq0ikac6.jpg" alt="igc"></p><p>与组卷积(a)进行比较，可以看出filed数量即为每组的通道数。IGC实际上就是将每组的输出通道，分到不同的field里去。因此交叉组卷积的CRF大小和组卷积是相同的。</p><p><strong>CLC block</strong>是本文提出的组件，由3x3的IGC和1x1的GC构成，组件结构如下图所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt48c72dkj30x80hqq54.jpg" alt="clcblock"></p><p>在CLC block的基础上，作者探索了FCRF的规则，假设IGC的分组数为$g_1$，$M、L、N$分别为输入、中间阶段、输出的通道数，GC的分组数为$g_2$，要达到FCRF，应满足以下条件：<br>$$<br>L/g_2 \geq  g_1  \quad or  \quad  g _ { 1 } g _ { 2 } \leq L<br>$$<br>基于FCRF规则，使得计算代价最小：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftt4m3cte8j30x808y3zy.jpg" alt="computational_cost"></p><h2 id="ShuffleNet-v2"><a href="#ShuffleNet-v2" class="headerlink" title="ShuffleNet v2"></a>ShuffleNet v2</h2><p>本文指出模型的运算速度不单单取决于每秒浮点运算次数（FLOPS），还有其他的考虑因素，比如内存访问代价（MAC）、运算平台等。通过一系列的控制实验，对于模型的设计提出了4个指导方针：</p><ol><li>相同的输入输出通道宽度，能减少内存访问代价。</li><li>分组过大，会增加内存访问代价。</li><li>网络分割会降低并行度。</li><li>不能忽略逐元素操作。</li></ol><p>基于指导方针，设计了新的ShuffleNet单元：</p><p>针对输入输出相同的单元，在一开始加入了通道分割（Channel Split），取消了组卷积和相加操作，以符合提出的指导方针2和4；针对降采样的单元，利用两路相似的操作进行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟&lt;/p&gt;
&lt;h2 id=&quot;SqueezeNet&quot;&gt;&lt;a href=&quot;#SqueezeNet&quot; class=&quot;headerlink&quot; title=&quot;Sq
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Network compression, speedup</title>
    <link href="http://yoursite.com/2018/07/27/Network-compression-speedup/"/>
    <id>http://yoursite.com/2018/07/27/Network-compression-speedup/</id>
    <published>2018-07-27T03:19:32.000Z</published>
    <updated>2018-07-27T03:30:13.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h2><h3 id="Singular-value-Decomposition-SVD"><a href="#Singular-value-Decomposition-SVD" class="headerlink" title="Singular value Decomposition(SVD)"></a>Singular value Decomposition(SVD)</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto95l6e4aj30w60cajt9.jpg" alt=""><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto96k0qcsj30uy04e755.jpg" alt=""></p><h3 id="Flattened-Convolutions"><a href="#Flattened-Convolutions" class="headerlink" title="Flattened Convolutions"></a>Flattened Convolutions</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto96skp45j30vo0aoac6.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto98lg9m4j30w00asac3.jpg" alt=""></p><h2 id="Weight-Pruning"><a href="#Weight-Pruning" class="headerlink" title="Weight Pruning"></a>Weight Pruning</h2><h3 id="Magnitude-based-method"><a href="#Magnitude-based-method" class="headerlink" title="Magnitude-based method"></a>Magnitude-based method</h3><ul><li>Iterative Pruning + Retraining<br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99b7u7hj30x8084dh3.jpg" alt=""></li><li>Dynamic Network Surgery<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto99ja0soj30vm06ogml.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto99q0hilj30wi0cyq5s.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99wbecrj30wm082dh2.jpg" alt=""></li></ul><h3 id="Hessian-based-method"><a href="#Hessian-based-method" class="headerlink" title="Hessian-based method"></a>Hessian-based method</h3><ul><li>Diagonal Hessian-based method: Optimal Brain Damage<br>💫  . Motivation<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9a3vy26j30yy04s3zc.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9a9srdtj310s0dqac9.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ae9ircj30zo0d040y.jpg" alt=""></li><li>Full Hessian-based method: Optimal Brain Surgeon<br>💫  . Motivation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9ajoj63j310s0d4q5t.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9aof4apj310s0dutbh.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9atm2jzj311c0aiwgd.jpg" alt=""></li></ul><h2 id="Quantization-method"><a href="#Quantization-method" class="headerlink" title="Quantization method"></a>Quantization method</h2><h3 id="Full-Quantization"><a href="#Full-Quantization" class="headerlink" title="Full Quantization"></a>Full Quantization</h3><ul><li>Fixed-point format<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ay9ggsj310o0b4tbh.jpg" alt=""><br>⛓ .  Rounding Modes<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9b5dkn2j310u0c6taq.jpg" alt=""><br>🍄 . Multiply and accumulate (MACC) operation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9b9p5z4j30zs0ceq5c.jpg" alt=""></li><li>Code book<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9begz5mj30zc0feq7k.jpg" alt=""></li></ul><h3 id="Quantization-with-full-precision-copy"><a href="#Quantization-with-full-precision-copy" class="headerlink" title="Quantization with full-precision copy"></a>Quantization with full-precision copy</h3><ul><li>Binnaryconnect<br>💫  . Motivation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9c757zjj31000ag0ue.jpg" alt=""><br>⛓ .  Binarization<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9ccdumnj31080eetbw.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9cijm0jj30zo07o3zt.jpg" alt=""><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9cmyf5sj311i082gnq.jpg" alt=""></li><li>Binarized Neural Network (BNN)<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9css9thj3114066gn9.jpg" alt=""><br>⛓ .  Method<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9cxe89pj311k0ecgoo.jpg" alt=""></li></ul><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Matrix-Factorization&quot;&gt;&lt;a href=&quot;#Matrix-Factorization&quot; class=&quot;headerlink&quot; title=&quot;Matrix Factorization&quot;&gt;&lt;/a&gt;Matrix Factorization&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>与Inception有关的那些事儿</title>
    <link href="http://yoursite.com/2018/07/24/inception/"/>
    <id>http://yoursite.com/2018/07/24/inception/</id>
    <published>2018-07-24T05:42:32.000Z</published>
    <updated>2018-07-26T07:26:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="external">《Going Deeper With Convolutions》</a>是最早提出Inception结构的文章。文章指出，增加神经网络的大小通常能获得更好的表现，但随之而来的缺点是模型很容易过拟合，并且计算代价高昂。关于这两个问题，最直观的解决办法是用稀疏的层来代替全连接层。</p><p>然而目前的计算机对于非均匀的稀疏数据运算不太友好，所以大多数系统是基于卷积实现的。卷积是我们稀疏空间域的好帮手，它像一个收集器，将前一层中的图像块稠密连接起来。但是，目前的卷积网络大多是均匀的结构。因此作者就提出了这么一个问题：如何利用好卷积的稀疏性，同时使用对计算机友好的稠密矩阵乘法进行计算。</p><h3 id="Inceptoin-module"><a href="#Inceptoin-module" class="headerlink" title="Inceptoin module"></a>Inceptoin module</h3><p>以往的许多文献表明，稀疏矩阵乘法运算可以通过将稀疏矩阵聚类成相对稠密的子矩阵来实现。Inception结构就是本文提出的稠密组件，用来近似卷积视觉网络的最佳局部稀疏结构。作者首先给出了naive版本的Inception组件(a)，如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftlxlf8uiwj30ss0f0q46.jpg" alt="inception_module_naive"></p><p>作者假设来自前一层的每个单元与输入的某些区域对应。在网络的浅层（靠近输入）部分，单元集中在局部区域，因此可以通过1x1卷积将其传递到下一层网络。同时，在较大的图像块上进行卷积，可以获得更少数量但扩展空间更大的特征图。因此作者将1x1卷积，3x3卷积，5x5卷积，再加上由经验表明效果很好的池化层，对它们的输出进行拼接，一起构成了Inception组件。</p><p>由于要将Inception组件层层堆叠，会导致网络高层部分的特征图通道数增加，这也意味着较大卷积的计算负担加重，特别是还要拼接上池化层的输出。因此作者提出了第二种减少维度的Inception组件(b)，即在较大卷积之前，池化层之后，加入1x1卷积来减少特征图的通道数：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftlxlr4cz3j30sg0fidhf.jpg" alt="inception_module_dimensionality_reduction"></p><h3 id="GoogleNet-architecture"><a href="#GoogleNet-architecture" class="headerlink" title="GoogleNet architecture"></a>GoogleNet architecture</h3><p>由以上两个组件构成的GoogleNet网络结构如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm3sth6elj31ga0w2aia.jpg" alt="GoogleNet"></p><p>在计算参数数量的时候发现了两个问题🤔</p><ol><li>7x7的卷积数量应该为$7^2 \times 3 \times 64 = 9408$，但表格中是2.7K，google了一下说有可能7x7卷积由7x1和1x7卷积构成，因此参数数量为$(7 \times 1+ 1\times7 ) \times 3 \times 64=2688$。</li><li>计算参数时都没有将depth计算在内，并且最后的结果需要除以1024。比如第二个3x3卷积，$(64 \times 64 + 9 \times 64 \times 192) =114,688$，除以1024后得到112K。</li></ol><h3 id="Auxiliary-classifier"><a href="#Auxiliary-classifier" class="headerlink" title="Auxiliary classifier"></a>Auxiliary classifier</h3><p>作者还在Inception (4a) 和 (4d) 之后添加了辅助分类器，将辅助分类器获得的损失以0.3的比例，添加到最后的损失函数中。然而辅助分类器的影响在之后的实验中证明发现是很小的。</p><p>完整的GoogleNet网络图太长啦，仅贴出含辅助分类器的Inception (4a)部分：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm6copd73j31020hudif.jpg" alt="googlenet_auxiliary_classifier"></p><h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception-v2"></a>Inception-v2</h2><p>在<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">《Rethinking the Inception Architecture for Computer Vision》</a>中，作者基于GoogleNet的Inception组件，提出了一些新的思考。</p><h3 id="Design-Principles"><a href="#Design-Principles" class="headerlink" title="Design Principles"></a>Design Principles</h3><p>作者首先提出了四个设计原则：</p><p>➊ 避免极端压缩的瓶颈。在信息传递的过程中，应缓慢减小特征图的大小。</p><p>➋ 更高维度的特征图更容易在网络中进行本地处理。在卷积网络中增加每个区块的激活能够解开更多特征。</p><p>❸ 空间聚合可以通过嵌入较低维度的特征图来完成，并且其表示能力没有太多损失。</p><p>❹ 平衡网络的宽度和深度。</p><h3 id="Factorizing-Convolutions"><a href="#Factorizing-Convolutions" class="headerlink" title="Factorizing Convolutions"></a>Factorizing Convolutions</h3><p>➊ 更小的卷积核</p><p>较大的卷积核可以用堆叠的较小的卷积核替代。比如一个5x5卷积就可以用两个连续的3x3替代：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm6xrwibpj30vo0g40y3.jpg" alt="smaller_convolutions"></p><p>这样就减小了计算代价：$(3+3) \times 2 / (5+5)=18/25$</p><p>➋ 非对称卷积</p><p>一个3x3卷积又可以在空间上分解为3x1卷积和1x3卷积：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftm716z34xj30vs0jkq69.jpg" alt="asymmetric_convolution"></p><p>再一次减少了计算代价：$(1 \times 3) + (3\times1) / (3 \times 3) = 2/3$</p><h3 id="Inception-module"><a href="#Inception-module" class="headerlink" title="Inception module"></a>Inception module</h3><p>因此基于设计原则➂和卷积核的分解方法➀，本文更新了传统的Inception组件结构：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftm7neonf9j314u0u20vh.jpg" alt="inceptionv2_module1"></p><p>在实践中，作者发现将$n\times n$卷积替换成$1 \times n$和$n\times1$的组合卷积，对于早期的网络层效果不是很好，但是在中期的网络层有很好的表现。基于卷积核的分解方法➁的Inception组件结构：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftm7u6ebzvj31dk128tcb.jpg" alt="inceptionv2_module2"></p><p>基于设计原则➁，作者还提出了第三种Inception组件结构：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftm7zo7emqj31540yk0xc.jpg" alt="Inception_module3"></p><p>作者解释道，仅在最粗糙的网格上使用第三种结构，因为与空间聚集相比，通过1x1卷积进行本地处理的比率增强时，产生高维稀疏表示的位置是最关键的，</p><h3 id="Grid-Size-Reduction"><a href="#Grid-Size-Reduction" class="headerlink" title="Grid Size Reduction"></a>Grid Size Reduction</h3><p>假设输入特征图大小为$d \times d \times k$，我们希望获得$\frac{d}{2} \times \frac{d}{2} \times 2k$大小的特征图，通常会使用池化层来减小其网格大小，常见的方式有以下两种：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftn0p7kdgkj30y40eagmw.jpg" alt="two_ways_reduce_grid_size"></p><p>❶ 先进行池化，再进行卷积（左）</p><p>这种方法的计算代价是$2(\frac{d}{2})^2 k^2$，但是特征图一下子就减小到$(\frac{d}{2})^2 k$，产生了瓶颈，违反了设计原则➀</p><p>❷ 先进行卷积，再进行池化（右）</p><p>这种方法会造成较高的计算代价：$2d^2k^2$</p><p>于是本文就提出了一种既不产生瓶颈，又能减少计算代价的方法，如下图右边所示。该方法使用两个stride为2的模块：池化层$P$和卷积层$C$，并将它们的输出拼接起来。计算代价减小为$2(\frac{d}{2})^2k^2$，然而不是很懂这个方法为什么不产生瓶颈。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftn1hqzbx1j30vo0eutad.jpg" alt="reduce_gride_size"></p><h3 id="Inception-v2-architecture"><a href="#Inception-v2-architecture" class="headerlink" title="Inception-v2 architecture"></a>Inception-v2 architecture</h3><p>将上述方法结合在一起，构成了Incpetion-v2网络：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftkwd3szd8j30um0l6tdx.jpg" alt="Inception-v2"></p><h3 id="Label-Smoothing-Regularization"><a href="#Label-Smoothing-Regularization" class="headerlink" title="Label Smoothing Regularization"></a>Label Smoothing Regularization</h3><p>假设一个训练样本产生$k$个分类的可能性分别为为$p(k)$，实值的分布为$q(k)$，那么交叉熵定义的损失函数为：<br>$$<br>L = - \sum_{k=1}^{K}\log ({p(k)})q(k)<br>$$<br>对输出值$z_k$求偏导：$\frac { \partial \ell } { \partial z _ { k } } = p ( k ) - q ( k )$</p><p>顺便复习了一下Softmax函数及其求导过程👉🏻<a href="https://blog.csdn.net/behamcheung/article/details/71911133#softmax%E5%87%BD%E6%95%B0" target="_blank" rel="external">Softmax函数与交叉熵</a></p><p>假设第$y$个为正确的标签，当$k=y$时，$q(k)=1$，当$k \ne q$时，$q(k)=0$。在这种情况下，最小化交叉熵的过程相当于最大化正确标签的对数似然函数。假设分类正确，那么$z_y \gg  z_k$。文章指出，这样会带来两个问题：</p><p>❶ 可能造成过拟合，如果模型学会为每个训练样本分配到正确标签的完整概率，则不能保证泛化。</p><p>❷ 使得最大的输出值和其他输出值之间的差异变大，降低模型的适应性。</p><p>也就是说，模型对它的预测太过自信。为了让模型不要太骄傲，作者提出了将实值分布$q ( k | x ) = \delta _ { k , y }$替换为标签平滑正则化(label-smoothing regularizatoin, LSR)：<br>$$<br>q ^ { \prime } ( k | x ) = ( 1 - \epsilon ) \delta _ { k , y } + \epsilon u ( k )<br>$$<br>其中$\epsilon$表示权重，$ u(k)$表示一个基于标签的先验分布，本文将其设置为均匀分布，则上式可以写为：<br>$$<br>q ^ { \prime } ( k ) = ( 1 - \epsilon ) \delta _ { k , y } + \frac { \epsilon } { K }<br>$$<br>$q(k)$意味着取值不是1就是0，而$q ^ { \prime } ( k )$意味着所有不正确的分类都有一个正下界。</p><p>我们还可以用交叉熵来理解LSR：<br>$$<br>H \left( q ^ { \prime } , p \right) = - \sum _ { k = 1 } ^ { K } \log p ( k ) q ^ { \prime } ( k ) = ( 1 - \epsilon ) H ( q , p ) + \epsilon H ( u , p )<br>$$<br>其中，$H(u,p)$可以用KL散度来表示：<br>$$<br>H ( u , p ) = D _ { K L } ( u | p ) + H ( u )<br>$$<br>由于KL散度用于衡量两个分布之间的距离，当$u$代表均匀分布时，$H(u)$是固定的，则$H(u,p)$代表预测分布$p$与均匀分布的差异程度。</p><h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h2><p>Inception-v3其实也是👆🏻那篇论文提出的。文章表明移除GoogleNet中的辅助分类器，并没有对实验结果产生不利的影响。实验结果表示，将辅助分类器作为正则项，当支路采用BN的时候，能够使主分类器有更好的结果。因此，本文将Inception-v2加上使用BN的支路称为Inception-v3。</p><h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception-v4"></a>Inception-v4</h2><p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</a>探索了ResNet给Inception组件带来的变化，提出了Inception-ResNet-v1和Inception-ResNet-v2，同时加深了网络结构。文章中有很多Inception组件，感觉不是很有意思，就大概滴看一下🤐</p><h2 id="Questions💫"><a href="#Questions💫" class="headerlink" title="Questions💫"></a>Questions💫</h2><ol><li>在Inception组件中，concat操作是直接将特征拼在一起的，若维度采用交叉拼接是否会使特征分布更均匀？</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GoogleNet&quot;&gt;&lt;a href=&quot;#GoogleNet&quot; class=&quot;headerlink&quot; title=&quot;GoogleNet&quot;&gt;&lt;/a&gt;GoogleNet&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.cv-foundation.org/ope
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Interleaved Group Convolutions</title>
    <link href="http://yoursite.com/2018/07/17/Interleaved-Group-Convolutions/"/>
    <id>http://yoursite.com/2018/07/17/Interleaved-Group-Convolutions/</id>
    <published>2018-07-17T02:29:32.000Z</published>
    <updated>2018-07-17T06:44:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文作者提出了一个名为交叉组卷积(Interleaved Group Convolution, IGC)的神经网络模块。它由两个连续的交叉组卷积构成：主组卷积和次组卷积。主组卷积中的操作时空间卷积，而次组卷积中的操作是逐点卷积。作者的关注点在减少卷积核的冗余，这个冗余来自空间范围和通道范围。在空间范围，发展出了小的卷积核，比如$3 \times 3，3 \times 1, 1 \times 3$。在通道范围，发展出了组卷积、逐通道卷积。本文研究的是通道范围的卷积核设计。</p><p>IGC的结构如下图所示：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftcny366afj310g0b6jw8.jpg" alt="interleaved_group_convolution"></p><p>令$L$表示主组卷积的分组数，$M$表示次组卷积的分组数(亦即主组卷积中每组的通道数)。上图展示的是$L=2$，$M=3$。</p><p>主组卷积可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftco2e2f78j30ho0520t0.jpg" alt="primary_group_convolution"></p><p>其中，$z_l$表示大小为$(MS)$的向量，$S$为卷积核大小。e.g. conv $ 3 \times 3$ -&gt; $S = 9$。$W_{ll}^p$表示第$l$组的卷积核，大小为$M \times (MS)$。则输出$y_l$为大小为$M$的向量。</p><p>将${y_1, y_2, \dots, y_l }$重新排列(permutation)作为次组卷积的输入：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftcollshyyj30q402kq31.jpg" alt="permutation1"></p><p>$\overline { y } _ { m }$表示次组卷积中的第$m$个分组，由主组卷积中不同组的第$m$个通道构成，大小为$L$。</p><p>次组卷积可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp07vq3vj30pq02a0ss.jpg" alt="secondary_group_convolution"></p><p>其中，$W_{mm}^s$表示第$m$组的$1\times1$卷积，大小为$L \times L$。则每组的输出$\overline { z } _ { m }$大小仍然为$L$。</p><p>最后对次组卷积的输出进行重新排列：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp2dp0rwj30ps028aa5.jpg" alt="permutation2"></p><p>整个IGC块可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp37eoqyj30pm02474b.jpg" alt="igc_block"></p><p>令$W = PW^sP^TW^p$为组合卷积核，则我们有：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcu3e9arnj30pe028dft.jpg" alt="composite_convolution_kernel"></p><p>表示由两个稀疏卷积相乘得到的正常卷积。</p><p>IGC的参数量为：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1ftcu6ltmeij30pi03ywes.jpg" alt="igc_num_parameters"></p><p>假设输入输出的通道数为$C$，正常卷积的参数量为：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcu8q7aixj30pe02sq2y.jpg" alt="rc_num_parameters"></p><p>假设$T_{igc} = T_{rc}$，可以得到：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcuahfrqvj30p0030q33.jpg" alt="compare_igc_rc"></p><p>对于典型的$S=3\times3$，当$L&gt;1$就有$G&gt;C$。也就是说，除了极端的$L=1$情况下，IGC内所含通道数都比正常卷积来得多，也就能包含更多的信息。</p><p>接下来作者分析了何种情况下，$G$能够最大。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcug9o7hxj30pu08wq3w.jpg" alt="when_widest"></p><p>由基本不等式得出公式(12)。因此当$L=MS$时，通道数最多。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcuqfychdj30s208adig.jpg" alt="classification_accuracy_comparison"></p><p>实验结果表明，在计算代价近似，甚至IGC-L24M2更少的情况下，IGC的总体表现还是比较好的。正是由于IGC增加了宽度，更有效地利用了参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文作者提出了一个名为交叉组卷积(Interleaved Group Convolution, IGC)的神经网络模块。它由两个连续的交叉组卷积构成：主组卷积和次组卷积。主组卷积中的操作时空间卷积，而次组卷积中的操作是逐点卷积。作者的关注点在减少卷积核的冗余，这个冗余来自空
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ShuffleNet</title>
    <link href="http://yoursite.com/2018/07/16/ShuffleNet/"/>
    <id>http://yoursite.com/2018/07/16/ShuffleNet/</id>
    <published>2018-07-16T07:14:32.000Z</published>
    <updated>2018-07-20T03:09:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>ShuffleNet是一个高效计算的CNN结构。该结构使用了两种新的操作：逐点组卷积(pointwise group convolution)和通道混洗(channel shuffle)。</p><h2 id="Gconv和channel-shuffle"><a href="#Gconv和channel-shuffle" class="headerlink" title="Gconv和channel shuffle"></a>Gconv和channel shuffle</h2><p>1）逐点组卷积</p><p>在较小的网络中，对所有通道进行逐点卷积计算代价高昂。一个直接的解决办法就是利用通道稀疏连接。在本文中，作者使用的方法是组卷积，即对通道进行分组，在组内进行逐点卷积操作。</p><p>2）通道混洗</p><p>如果仅仅使用组卷积，那么最终每组输出仅于该组的输入相关联，阻碍了通道组之间的信息流，减弱了特征信息的表达。因此作者提出了通道混洗操作。通过将通道组细分为子通道组，将子通道组重新排列，作为下一层网络的输入。</p><p>具体地，使用了通道混洗操作的逐点组卷积如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftbq9e31puj31ca0iumzu.jpg" alt="channel_shuffle"></p><h2 id="ShuffleNet单元"><a href="#ShuffleNet单元" class="headerlink" title="ShuffleNet单元"></a>ShuffleNet单元</h2><p>基于上面提到的两个操作，作者提出了ShuffleNet单元：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbqcw622hj318k0m2gp1.jpg" alt="shufflenet_units"></p><p>在传统的ResNet单元基础上，将头尾的1x1 Conv改为1x1 Gconv，中间的3x3 Conv改为3x3 DWConv，并且在DWConv之前加入了单元混洗操作。假设输入大小为$c \times h \times w$，bottelneck中的通道数为m。ResNet计算代价为$hw(2cm + 9m^2) $，而ShuffleNet的计算代价为$hw(2cm/g + 9m) ​$，其中$g$代表组卷积的分组数。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>ShuffleNet的网络结构如下图所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbquzmce7j31eo0k4gpf.jpg" alt="ShuffleNet_architecture"></p><p>由于输入Stage2的通道数较小，因此Stage2的第一个逐点卷积层不进行分组。</p><p>❗️看了代码才注意到，论文中有说明，bottleneck中的通道数是output channels/4</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者首先实验了不同分组数在不同大小网络上的分类效果：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbqz2ulxmj313s07sjsy.jpg" alt="scale_group"></p><p>从结果可以看出，使用分组卷积($g&gt;1$)与没有使用分组卷积($g=1$)相比，逐渐地表现越来越好。网络越小，从分组卷积中受益得越多。作者表示，在限制复杂度的情况下，分组卷积能够包含更多地特征图通道，从而可以编码更多的信息。因为较小的网络包含更窄的特征图，意味着能从扩大特征图中收益更多。</p><p>作者接下来比较了有无通道混洗在不同分组以及不同大小网络上的分类效果：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftbr72m6a9j316o0bcdid.jpg" alt="is_shuffle"></p><p>通道混洗操作的目标是使多组卷积层之间能够产生跨组信息流。实验结果表明，使用了通道混洗操作的网络分类效果都比没有使用通道混洗操作来的好。并且分组越多($g=8$)，通道混洗操作的效果就越明显。</p><p>作者还在同等计算代价的情况下，将ShuffleNet与其他网络做比较：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftbrebkp2xj31c006odha.jpg" alt="various_structures"></p><p>以及与MobileNet做比较：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbrf6b15rj319e0g642f.jpg" alt="shufflenet_mobilenet"></p><p>可以看出ShuffleNet的分类结果是优于其他网络结构的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ShuffleNet是一个高效计算的CNN结构。该结构使用了两种新的操作：逐点组卷积(pointwise group convolution)和通道混洗(channel shuffle)。&lt;/p&gt;
&lt;h2 id=&quot;Gconv和channel-shuffle&quot;&gt;&lt;a href
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>压缩网络模型性能比较</title>
    <link href="http://yoursite.com/2018/07/14/comparison_models/"/>
    <id>http://yoursite.com/2018/07/14/comparison_models/</id>
    <published>2018-07-14T02:07:32.000Z</published>
    <updated>2019-05-05T01:39:07.658Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th style="text-align:center">Million  Mult-Adds</th><th style="text-align:center">Million Parameters</th><th style="text-align:center">ImageNet  Accuracy</th></tr></thead><tbody><tr><td>Conv MobileNet</td><td style="text-align:center"><strong>4866</strong></td><td style="text-align:center"><strong>28.3</strong>(29.3)</td><td style="text-align:center">71.7%</td></tr><tr><td>MobileNet</td><td style="text-align:center"><strong>568</strong>(575)</td><td style="text-align:center"><strong>3.2</strong>(4.2)</td><td style="text-align:center">70.6%</td></tr><tr><td>MobileNetv2</td><td style="text-align:center"><strong>312</strong>(300)</td><td style="text-align:center"><strong>2.2</strong>(3.4)</td><td style="text-align:center">72.0%</td></tr><tr><td>ShuffleNet(1.5)</td><td style="text-align:center">292?</td><td style="text-align:center">3.4?</td><td style="text-align:center">71.5%</td></tr><tr><td>ShuffleNet(x2)</td><td style="text-align:center">524?</td><td style="text-align:center">5.4?</td><td style="text-align:center">73.7%</td></tr></tbody></table><p>黑体部分为我计算的结果，括号内为MobileNet2论文中列出的数值。均未加上最后的fc。论文中没有指出ShuffleNet使用的模型结构，因此在下面「计算过程」中参照原始论文做了单独的计算。</p><h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>Standard Convolution: $h _ { i } \cdot w _ { i } \cdot d _ { i } \cdot d _ { j } \cdot k \cdot k$</p><p>Depthwise separable convolution: $h _ { i } \cdot w _ { i } \cdot d _ { i } \left( k ^ { 2 } + d _ { j } \right)$</p><p>Bottleneck convolution: $h_{i} \cdot w_{i} \cdot d_{i} \cdot t \left( d_{i} + k ^ { 2 } + d_{j} \right)$</p><p>输入是 $h_{in} _ w_{in} _c_{in}$，输出时$h_{out} _ w_{out} _ c_{out}$，卷积核为$d _ d _ c_{in} * c_{out}$</p><p><strong>Multi-adds:</strong></p><p>$c_{in} _ c_{out} _ d _ d _ w_{out} * h_{out}$</p><p><strong>1/ Conv MobileNet</strong></p><p>❶ Mult-Adds: </p><p>9x3x32x112^2</p><p>+9x32x64x112^2</p><p>+9x64x128x56^2</p><p>+9x128x128x56^2</p><p>+9x128x256x28^2</p><p>+9x256x256x28^2</p><p>+9x256x512x14^2</p><p>+5x(9x512x512x14^2)</p><p>+9x512x1024x7^2</p><p>+9x1024x1024x7^2</p><p>=<strong>4,866,269,184</strong></p><p>➋ Parameters:</p><p>9x3x32+9x32x64+9x64x128+9x128x128+9x128x256+9x256x256</p><p>+9x256x512+5x(9x512x512)+9x512x1024+9x1024x1024 = <strong>28,257,120</strong></p><p><strong>2/ MobileNet</strong></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ft80stio5xj30tg0vkgrg.jpg" alt="MobileNet_architecture"></p><p>❗️14个block的stride应该是1，已通过阅读模型更正。</p><p>Seperable Depth convolution的参数数量：$d_i\times(9+d_j)$</p><p>➊ Mult-Adds: </p><p>9x3x32x112^2</p><p>+112^2 x (9x32 + 32x64) </p><p>+56^2 x (9x64 + 64x128)</p><p>+56^2 x (9x128 + 128x128)</p><p>+28^2 x (9x128 + 128x256)</p><p>+28^2 x (9x256 + 256x256)</p><p>+14^2 x (9x256 + 256x512)</p><p>+5x14^2 x (9x512 + 512x512) </p><p>+7^2 x (9x512 + 512x1024) </p><p>+7^2 x (9x1024 + 1024x1024) </p><p>=<strong>567,716,352</strong></p><p>➋ Parameters:</p><p>9x3x32+(9x32+32x64)+(9x64+64x128)+(9x128+128x128)+(9x128+128x256)+(9x256+256x256)+(9x256+256x512)+5x(9x512+512x512)+9x512+512x1024+9x1024+1024x1024 = <strong>3,185,088</strong></p><p><strong>3/ MobileNetV2</strong></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ft87vpxvhrj30qk0icgok.jpg" alt="MobileNetV2_architecture"><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ft88vgekt5j30v40be0up.jpg" alt="Bottlenet_residual_block"></p><p>Bottleneck residual block的参数数量： $k \cdot tk + 3^2 \cdot tk  + tk \cdot k’ = tk(k+9 + k’)$</p><p>Bottleneck residual block的Mult-adds：$h \cdot w \cdot tk(k+9/s^2 + k’/s^2)$</p><p>➊ Mult-Adds: </p><p>112^2x3x9x32</p><p>+112^2x32x(32+9+16)</p><p>+112^2x6x16x(16+9/4+24/4) + 56^2x6x24x(24+9+24)</p><p>+56^2x6x24x(24+9/4+32/4) + 28^2x6x32x(32+9+32)x2</p><p>+28^2x6x32x(32+9/4+64/4) + 14^2x6x64x(64+9+64)x3</p><p>+14^2x6x64x(64+9+96) + 14^2x6x96x(96+9+96)x2</p><p>+14^2x6x96x(96+9/4+160/4) + 7^2x6x160x(160+9+160)x2</p><p>+7^2x6x160x(160+9+320)</p><p>+7^2x320x1280</p><p>=<strong>312,339,328</strong></p><p>➋ Parameters:</p><p>9x3x32</p><p>+32x(32+9+16)</p><p>+6x16x(16+9+24)+6x24x(24+9+24)</p><p>+6x24x(24+9+32)+6x32x(32+9+32)x2</p><p>+6x32x(32+9+64)+6x64x(64+9+64)x3</p><p>+6x64x(64+9+96)+6x96x(96+9+96)x2</p><p>+6x96x(96+9+160)+6x160x(160+9+160)x2</p><p>+6x160x(160+9+320)</p><p>+320x1280</p><p>=<strong>2,190,784</strong></p><p><strong>3/ ShuffleNet</strong></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbie6hfd7j31960icjuo.jpg" alt="ShuffleNet"></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbid72bosj30oc0kawgc.jpg" alt="ShuffleNet_unit"></p><p>假设bottleneck的输入通道数为c，输出通道数为$m$，ShuffleNet中设置中间通道数为$m/4$</p><p>$s=1$的ShuffleNet Units使用相加操作，Mult-adds：$hwm(2c/4g + 9/4) $</p><p>$s=2$的ShuffleNet Units使用拼接操作，Mult-adss：$hw(m-c)(c/4g + (9/4 + (m-c)/4g)/s^2$</p><p>令$c=m-c$，当且仅当$m=2c$。所以ShuffleNet中每个阶段的输出通道皆为输入的两倍。</p><p>所以$s=2$时的计算公式可以转化为：$hwc(c/4g + (9/4 + c/4g)/s^2$</p><p>➊ Mult-Adds(g=2): </p><p>112^2x3x9x24</p><p>+56^2x24x44 + 28^2x(9x44 + 44x176/2)</p><p>+28^2x(200x50/2 + 9x50 + 50x200/2) x 3</p><p>+28^2x200x50/2 + 14^2x(9x50+50x200/2)</p><p>+14^2x(400x100/2 + 9x100 + 100x400/2) x 7</p><p>+14^2x400x100/2 + 7^2x(9x100+100x400/2) </p><p>+7^2x(800x200/2 + 9x200 + 200x800/2) x 3</p><p>=<strong>129,196,340</strong></p><p><strong>Shufflenetx2 (g=3)</strong></p><p>112^2x3x9x48</p><p>+56^2x48x108 + 28^2x(9x108 + 108x432/3)</p><p>+28^2x(480x120/3 + 9x120 + 120x480/3) x 3</p><p>+28^2x480x120/3 + 14^2x(9x120+120x480/3)</p><p>+14^2x(960x240/3 + 9x240 + 240x960/3) x 7</p><p>+14^2x960x240/3 + 7^2x(9x240+240x960/2) </p><p>+7^2x(1920x480/3 + 9x480 + 480x1920/3) x 3</p><p>= 482,811,504</p><p>❗️和文章中的140M有些差距，在下面列出具体的计算过程：</p><table><thead><tr><th>Layer</th><th>params</th><th>macs</th></tr></thead><tbody><tr><td>Conv2D</td><td>3x9x24 = 648✔︎</td><td>112^2 x 648 = 8,128,512</td></tr><tr><td>Stage2_1 / 1x1conv</td><td>24x(200-24)/4 = 1,056✔︎</td><td>56^2 x1056 = 3,311,616</td></tr><tr><td>Stage2_1 / 3x3DWconv(s2)</td><td>9x44 = 396✔︎</td><td>28^2 x 396 = 310,464</td></tr><tr><td>Stage2_1 / 1x1Gconv</td><td>44/2 x 176/2 x 2 =  3,872✔︎</td><td>28^2 x 3872 = 3,035,648</td></tr><tr><td>Stage2_2 / 1x1Gconv</td><td>200/2 x 200/4/2 x 2 = 5,000✔︎</td><td>28^2 x 5000 = 3,920,000</td></tr><tr><td>Stage2_2 / 3x3DWconv</td><td>9x50 = 450✔︎</td><td>28^2 x 450 = 352,800</td></tr><tr><td>Stage2_2 / 1x1Gconv</td><td>50/2 x 200/2 x 2 = 5,000✔︎</td><td>28^2 x 5,000 = 3,920,000</td></tr><tr><td>Stage2_2 x 3</td><td>-</td><td>-</td></tr><tr><td>Stage3_1 / 1x1Gconv</td><td>200/2 x (400-200)/4/2 x2 = 5,000✔︎</td><td>28^2 x 5000 = 3,920,000</td></tr><tr><td>Stage3_1 / 3x3DWconv(s2)</td><td>9x50= 450✔︎</td><td>14^2 x 450 = 88,200</td></tr><tr><td>Stage3_1 / 1x1Gconv</td><td>50/2 x 200/2 x 2 = 5,000✔︎</td><td>14^2 x 5000 = 980,000</td></tr><tr><td>Stage3_2 / 1x1Gconv</td><td>400/2 x 400/4/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage3_2 / 3x3DWconv</td><td>9x100 = 900✔︎</td><td>14^2 x 900 = 176,400</td></tr><tr><td>Stage3_2 / 1x1Gconv</td><td>100/2 x 400/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage3_2 x 7</td><td>-</td><td>-</td></tr><tr><td>Stage4_1 / 1x1Gconv</td><td>400/2 x (800-400)/4/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage4_1 / 3x3DWconv(s2)</td><td>9x100= 900✔︎</td><td>7^2 x 900 = 44,100</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>200/2 x 800/4/2  x 2 = 20,000✔︎</td><td>7^2 x 20000 = 980,000</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>800/2 x 800/4/2 x 2 = 80,000✔︎</td><td>7^2 x 80,000 = 3,920,000</td></tr><tr><td>Stage4_2 / 3x3DWconv</td><td>9x200 = 1,800✔︎</td><td>7^2 x 1800 = 88,200</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>200/2 x 800/2 x 2 = 80,000✔︎</td><td>7^2 x 80,000 = 3,920,000</td></tr><tr><td>Stage4_2 x 3</td><td>-</td><td>-</td></tr></tbody></table><p>看到一篇超级好的比较文章：</p><p><a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="external">Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast</a></p><p><strong>Questions：</strong></p><ol><li>论文中关于Mut-adds的计算公式，是否仅考虑s=1的情况？比如在Depthwise separable convolution中，若s=2，那么计算代价应为$h _ { i }/ s \cdot w _ { i }/s \cdot d _ { i } \left( k ^ { 2 } + d _ { j } \right)$</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Million  Mult-Adds&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Million Parameters&lt;/th&gt;
&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MobileNets</title>
    <link href="http://yoursite.com/2018/07/06/MobileNets/"/>
    <id>http://yoursite.com/2018/07/06/MobileNets/</id>
    <published>2018-07-06T02:07:32.000Z</published>
    <updated>2018-11-13T13:09:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p><p>本文提出了一个MobileNets网络结构和两个超参数，可以用于移动和嵌入式视觉应用。与传统的卷积网络相比，大大减少了参数个数。构建较小网络的方法大致可以分为两类：1）压缩预训练模型；2）直接训练小的网络。</p><p>MobileNets基于流线型架构，使用<strong>深度分离卷积</strong>来构建轻量级深度神经网络。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>假设输入特征图$F$的大小为$D_F \times D_F\times M$，输出特征图$G$的大小为$D_G \times D_G \times N$，卷积核$K$的大小为$D_K \times D_K \times M \times N$</p><p>那么<strong>标准卷积</strong>可以表示为：<br>$$<br>\mathbf { G } _ { k , l , n } = \sum _ { i , j , m } \mathbf { K } _ { i , j , m , n } \cdot \mathbf { F } _ { k + i - 1 , l + j - 1 , m }<br>$$<br>计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot N \cdot D _ { F } \cdot D _ { F }<br>$$<br>深度分离卷积将标准卷积分解为<strong>深度卷积(depthwise convolutions)</strong>和称为<strong>逐点卷积(point wise convolutions)</strong>的1×1卷积。</p><p>每个输入通道使用一个卷积核进行<strong>深度卷积</strong>可以表示为：<br>$$<br>\hat { \mathbf { G } } _ { k , l , m } = \sum _ { i , j } \hat { \mathbf { K } } _ { i , j , m } \cdot \mathbf { F } _ { k + i - 1 , l + j - 1 , m }<br>$$<br>计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot D _ { F } \cdot D _ { F }<br>$$<br>深度卷积只对输入的通道进行了处理，相当于生成了M个大小为$D_G \times D_G$的特征图。接着我们利用<strong>逐点卷积</strong>将<strong>深度卷积</strong>的输出线性连接起来，产生新的特征图。</p><p>总的计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot D _ { F } \cdot D _ { F } + M \cdot N \cdot D _ { F } \cdot D _ { F }<br>$$<br>标准卷积过滤器与深度分离卷积过滤器的比较如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fszxe85zmsj30ni0vmq6k.jpg" alt="convolutional_filters"></p><p>MobileNet的网络结构如下表所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fszypbbny0j30os0qejym.jpg" alt=""></p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>为了更进一步减少参数个数，本文接着提出了两个超参数：宽度乘数(width multiplier)和分辨率乘数(resolution multiplier)</p><p><strong>宽度乘数$\alpha$</strong>的作用是在每层均匀地稀疏网络，含$\alpha$的深度分离卷积的计算成本可以表示为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot \alpha M \cdot D _ { F } \cdot D _ { F } + \alpha M \cdot \alpha N \cdot D _ { F } \cdot D _ { F }<br>$$<br><strong>分辨率乘数$\rho$</strong>的作用是在每层减少输入图像的大小，含$\alpha$和$\rho$的深度分离卷积的计算成本可以表示为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot \alpha M \cdot \rho D _ { F } \cdot \rho D _ { F } + \alpha M \cdot \alpha N \cdot \rho D _ { F } \cdot \rho D _ { F }<br>$$</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>MobileNet模型可以应用于许多设备智能的识别任务中，包括物体检测、细粒度分类、面部属性、地标识别等。本文通过将MobileNet与当前流行的其他网络模型作比较，可以看出在各项任务中，MobileNet虽然准确率不如大的网络，但是它大大减少了参数的个数，提升了模型计算效率，正如它的名字一样，是有志于应用在移动设备上的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1704.04861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文提出了一个MobileNets网络
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>模型误差从哪儿来？</title>
    <link href="http://yoursite.com/2018/06/22/where-does-the-error-come-from/"/>
    <id>http://yoursite.com/2018/06/22/where-does-the-error-come-from/</id>
    <published>2018-06-22T02:41:29.000Z</published>
    <updated>2018-07-16T08:15:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>开始学李宏毅的机器学习，用pokerman来举线性回归的例子实在是太有意思啦～有受到一些启发，以此巩固一下基础知识。</p><h2 id="误差来源"><a href="#误差来源" class="headerlink" title="误差来源"></a>误差来源</h2><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fsjrf9bj1gj31g4122h0d.jpg" alt="estimator"></p><p>在前面几节课里提到，机器学习的过程其实就是在一堆模型里找到那个最符合所给数据的最优模型$f^\ast$。这个$f^\ast$实际上是真实数据模型$\hat{f}$的一个估计。</p><p>只要是估计就会存在误差，那么这个误差是从哪里来的呢？它有两个来源，一是偏差（Bias），二是方差（Variance）。</p><h2 id="均值和方差"><a href="#均值和方差" class="headerlink" title="均值和方差"></a>均值和方差</h2><p>基于大数据的机器学习很大程度上可以看作是在利用数理统计的知识来解决问题。在数理统计中，我们利用样本的信息（统计量）来推断总体的未知信息（估计量）。</p><p>我们可以利用样本的均值来估计数学期望：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fsjs1yq7isj31g211udli.jpg" alt="bias-estimator"></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fsjsaxm0uuj31gg134jxg.jpg" alt="unbiased-estimator"></p><p>也可以利用样本的方差来估计方差：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fsjs9tt7k6j31ge11s7a2.jpg" alt="variance-estimator"></p><p>假设我们在100个平行宇宙里做实验，获取到100个$f^_$，我们可以计算出这100个$f^_$的均值，来判断这些模型与$\hat{f}$之间的偏差：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fsjsjesachj31gs124x0f.jpg" alt="bias"></p><p>我们还可以计算出这100个$f^*$的方差，来判断这些模型的分散程度：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fsjskoyd96j31gg12q4kf.jpg" alt=""></p><h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p>将偏差和方差产生的两条误差曲线和在一起，就变成了观察到的误差曲线。因此也引出了过拟合（Overfitting）和欠拟合（Underfitting）的概念。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fsjsob3ibhj31g412edr3.jpg" alt="overfitting-underfitting"></p><p><strong>欠拟合：</strong>当我们发现模型不能很好地拟合训练样本，很可能是偏差较大。可以通过1）增加数据特征 2）建立更复杂的模型来解决欠拟合。</p><p><strong>过拟合：</strong>当我们的模型很好的拟合训练样本，但在测试样本上产生了巨大的误差，很可能是方差较大。可以通过1）增加数据量 2）正则化来解决过拟合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;开始学李宏毅的机器学习，用pokerman来举线性回归的例子实在是太有意思啦～有受到一些启发，以此巩固一下基础知识。&lt;/p&gt;
&lt;h2 id=&quot;误差来源&quot;&gt;&lt;a href=&quot;#误差来源&quot; class=&quot;headerlink&quot; title=&quot;误差来源&quot;&gt;&lt;/a&gt;误差来源&lt;/h2
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>那些年使用GPR的超分辨率重建技术</title>
    <link href="http://yoursite.com/2018/05/24/sr-gpr/"/>
    <id>http://yoursite.com/2018/05/24/sr-gpr/</id>
    <published>2018-05-24T01:06:00.000Z</published>
    <updated>2018-05-25T03:51:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-SRGPR"><a href="#1-SRGPR" class="headerlink" title="1.SRGPR"></a>1.SRGPR</h2><p><a href="https://hhexiy.github.io/docs/papers/srgpr.pdf" target="_blank" rel="external">Single Image Super-Resolution using Gaussian Process Regression</a></p><p>这是最早将GPR引入超分辨率重建问题的论文。令 $y$ 表示一个观测值，$\epsilon$ 为高斯噪声，高斯过程回归模型表示如下：<br>$$<br>y = f(x) + \epsilon, \epsilon \sim \mathcal{N}(n, \sigma^2_n)<br>$$<br>均值函数为零的观测值 $\mathbf { y }$ 和输出 $f_*$ 的联合分布为：<br>$$<br>\left[\begin{array}{c} \mathbf { y } \\ f_*\end{array}\right] \sim \mathcal{N}\left( \begin{array}{cc}0, \left[\begin{array}{cc} K_y \triangleq K(X,X)+\sigma^2_nI &amp; K(X,X_*)\\K(X_*,X) &amp; K(X_*,X_*)\end{array} \right]\end{array} \right)<br>$$<br>其中$X$代表训练集，$X_*$代表测试集。我们可以推导出条件分布：<br>$$<br>f_*|X,y,X_* \sim \mathcal{N}(\bar{f}_*, V(f_*))<br>$$<br>其中，<br>$$<br>\bar{f}_* = K(X_*, X)K_y^{-1}\mathbf { y } = K(X_*, X)\alpha<br>$$</p><p>$$<br>V(f_*) = K(X_*, X_*) -K(X_*, X)K_y^{-1}K(X,X_*)<br>$$</p><p>GPR最大的问题就是计算复杂度大，为了解决这个问题研究者们提出了许多方法，下面3篇文章可以说是一脉相承，毕竟是出自同一个作者，就来学习一下如何可以新(shui)突(lun)破(wen)吧～</p><h2 id="2-AGPR"><a href="#2-AGPR" class="headerlink" title="2.AGPR"></a>2.AGPR</h2><p><a href="https://ieeexplore.ieee.org/document/7364246/" target="_blank" rel="external">Single-Image Super-Resolution Using Active-Sampling Gaussian Process Regression</a></p><p>本文使用了<strong>主动采样（Active-samplng）</strong>，即从训练集中提取含有较多信息的图像对构成子集，利用子集构建GPR模型，以减少计算复杂度。</p><p>算法的可视化过程如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frm6jky61bj30uw0fetbj.jpg" alt=""></p><p><strong>输入</strong>包括低分辨率测试图像 $I$，放大倍率 $s$，训练图像对$\left\{ I_q, H_q\right\}_{q=1}^m$。训练图像对包括经过差值放大的$LR$图像和$HR$图像。<strong>输出</strong>为重建结果$S_H$。</p><p><strong>训练阶段：</strong>随机从$\left\{ I_q, H_q\right\}_{q=1}^m$中选择$n$对样本图像$\left\{P_l^{(I)},P_l^{(H)} \right\}_{l=1}^n$，利用中心像素值计算$D \triangleq \left\{ &lt; \mathbf { x } _ { l } ,y _ { l } &gt; \right\} = \left\{ &lt; P _ { l } ^ { ( I ) } ,\operatorname{cen} \left( P _ { l } ^ { ( H ) } \right) - \operatorname{cen} \left( P _ { l } ^ { ( I ) } \right) &gt; \right\}$；使用主动采样获取样本子集 $D ^ { \prime } = \left\{ &lt; P _ { u } ^ { ( I ) } ,y _ { u } &gt; \right\} _ { u = 1} ^ { r }$ ；利用$D ^ { \prime }$训练GPR模型$f$，同时事先计算出投影矩阵$P \triangleq K _ { y } ^ { - 1} y$  。</p><p><strong>测试阶段：</strong>计算LR的差值图像$S_I$，令$S_H=S_I$，循环$S_I$中的所有样本像素计算：<br>$$<br>\begin{align}&amp; y _ { k } ^ { * } = K \left( \mathbf { x } _ { k } ^ { * } ,X \right) * P ,\text{ where } X \triangleq \left\{ P _ { u } ^ { ( I ) } \right\} _ { u = 1} ^ { r } \\  &amp;y _ { k } ^ { * } =  y _ { k } ^ { * }  + cen( x _ { k } ^ { * })  \\&amp;更新对应的S_H的像素值\end{align}<br>$$<br>最后用IBP算法重建出高分辨率图像。</p><p>在进行主动采样时，定义了两个采样标准：代表性（representativeness）和多样性（diversity） 。</p><p>对于一个样本$s_i \triangleq &lt; \mathbf { x } _ { i } ,y _ { i } &gt; \in D$ ，其中$D$为原始训练样本集。 </p><p><strong>代表性</strong>可以表示为：<br>$$<br>R \left( s _ { i } \right) = \frac { 1} { | \mathcal { N } _ { i } | } \sum _ { j \in \mathcal { N } _ { i } } \exp \left( - | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} / 2\sigma _ { R } ^ { 2} \right)<br>$$<br>其中，$\sigma _ { R } ^ { 2}$为高斯核的带宽，可以表示为：<br>$$<br>\sigma _ { R } ^ { 2} = \rho \times \left( \text{ medians } _ { i } ,s _ { j } \in D ,i \neq j | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} \right)<br>$$<br><strong>多样性</strong>可以表示为：<br>$$<br>D \left( \mathbf { s } _ { i } \right) = \min _ { \mathbf { s } _ { j } \in \mathcal { S } } \left[ - \exp \left( - | \mathbf { s } _ { i } - \mathbf { s } _ { j } | _ { 2} ^ { 2} / 2\sigma _ { R } ^ { 2} \right) \right]<br>$$<br>其中$S$为挑选出的信息样本。对于自然图像，边缘和纹理可以表现多样性特性。</p><p>使用凸组合在剩余的候选样本集$C=D-S$中，重复选择出具有较多信息的样本。<br>$$<br>argmax _ { s _ { i } \in \mathcal { C } } \left( \lambda R \left( \mathbf { s } _ { i } \right) + ( 1- \lambda ) D \left( \mathbf { s } _ { i } \right) \right)<br>$$</p><h2 id="3-SGPR"><a href="#3-SGPR" class="headerlink" title="3.SGPR"></a>3.SGPR</h2><p><a href="https://www.sciencedirect.com/science/article/pii/S0165168416302973" target="_blank" rel="external">Fast single image super-resolution using sparse Gaussian process regression</a></p><p>本文利用 👆🏻那篇论文提出的主动采样生成训练子集，使用<a href="https://pdfs.semanticscholar.org/01ae/ae41bb2d49740826b06bc8668372c3fe778d.pdf" target="_blank" rel="external">GPLasso</a>来产生稀疏投影向量。GPLasso降低GPR计算复杂度的方法主要是通过稀疏均值函数中的变量$\alpha$。GPLasso利用相对熵和L1正则项定义$\alpha$的损失函数：<br>$$<br>Q \left( \alpha ^ { \star } \right) = 2K L \left( p ( f | \mathcal { D } ) | _ { H } ( f ) \right) + \lambda | \alpha ^ { \star } | _ { 1}<br>$$<br>基于最小角回归（LAR）算法来最小化损失函数。</p><h2 id="4-DSGPR"><a href="#4-DSGPR" class="headerlink" title="4.DSGPR"></a>4.DSGPR</h2><p><a href="https://ieeexplore.ieee.org/document/7918590/" target="_blank" rel="external">Single Image Super-Resolution Using Gaussian Process Regression With Dictionary-Based Sampling and Student- tLikelihood</a></p><p>本文使用基于稀疏表示的字典采样来生成训练子集，并将高斯过程的高斯分布改为了学生t分布（Student-t）。</p><p>关于字典学习以及稀疏表示，觉得介个 <a href="http://www.cnblogs.com/hdu-zsk/p/5954658.html" target="_blank" rel="external">链接</a> 解释的通俗易懂。稀疏字典学习包含两个阶段：①字典构建阶段 ②利用字典（稀疏的）表示样本阶段。对庞大的数据构建字典实际上就是一个降维的过程，这个过程尝试学习蕴藏在样本背后最质朴的特征。将学习到的字典用于提取特征即输入的稀疏表示。</p><h2 id="3-NGPR"><a href="#3-NGPR" class="headerlink" title="3.NGPR"></a>3.NGPR</h2><p><a href="https://www.sciencedirect.com/science/article/pii/S0925231216002332" target="_blank" rel="external">Image super-resolution using non-local Gaussian process regression</a></p><p>本文作者提出了一种非局部的基于GPR模型的NGPR超分辨率重建方法。该方法是在LR层面学习NGPR模型用于细节合成，在HR层面利用学习到的模型预测高频细节。之前也整理过论文啦～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-SRGPR&quot;&gt;&lt;a href=&quot;#1-SRGPR&quot; class=&quot;headerlink&quot; title=&quot;1.SRGPR&quot;&gt;&lt;/a&gt;1.SRGPR&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://hhexiy.github.io/docs/papers/srgp
      
    
    </summary>
    
    
      <category term="SISR" scheme="http://yoursite.com/tags/SISR/"/>
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>读论文《Image super-resolution using non-local Gaussian process regression》</title>
    <link href="http://yoursite.com/2018/05/21/paper-ngpr/"/>
    <id>http://yoursite.com/2018/05/21/paper-ngpr/</id>
    <published>2018-05-21T03:02:06.000Z</published>
    <updated>2018-05-24T02:08:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>大部分使用GPR的SR方法是局部采样的，这些方法将图像分成几个固定大小（e.g  $45\times45$）并且有一小部分重叠的大块，对每个大块，只使用到其中小块的自相似性（e.g $3\times3$）来建立GPR模型，因此很难完全利用到自然图像中的自相似性。本文作者提出了一种非局部的基于GPR模型的NGPR超分辨率重建方法。</p><h2 id="系统总览"><a href="#系统总览" class="headerlink" title="系统总览"></a>系统总览</h2><p>该系统包括两个阶段：</p><ol><li>学习阶段：在LR层面学习NGPR模型用于细节合成</li><li>预测阶段：在HR层面预测高频细节用于SR估计</li></ol><p>系统流程图如下所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1frj17sdk7qj31180iidqf.jpg" alt=""></p><p>具体地，在学习阶段可以分成两个主要部分：1）生成训练图像对；2）学习NGPR模型。</p><p>首先获取由原始图像 $I$ 进行上采样再下采样生成的辅助差值图像 $I_I$ ，将 $I$ 和  $I_I$ 分别减去均值图像 $I_M$，提取出高频特征 $I’$ 和 $I’_I$ 。其中 $I_M$ 是用 $3\times3$ 的平均过滤器在 $I_I$ 上的过滤结果。接下来提取出 $I’_I$ 中图像块的邻近像素序列 $\left\{ {x_i}\right\}^n_{i=1}$ ，以及 $I’$ 中对应图像块的中心像素序列 $\left\{ {y_i}\right\}^n_{i=1}$ 作为训练集，用于学习NGPR模型的映射关系。</p><p>在预测阶段，利用插值法获得上采样图像 $S_I$，减去均值图像 $S_M$ 以提取图像的高频特征 $S’_I$ 。利用学习阶段学习出的模型，将 $S’_I$ 中图像块的邻近像素序列 $\left\{ x _ { j * } \right\} _ { j = 1} ^ { m } $ 映射为缺失的高频细节 $\left\{ { f(x_{j*}) } \right\}^m_{j=1}$。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>作者提出了在特征提取过程中，使用到的三个方法。</p><h4 id="1）非局部采样"><a href="#1）非局部采样" class="headerlink" title="1）非局部采样"></a>1）非局部采样</h4><p>在生成训练图像对的时候，采样间隔为4，将训练集大小降低为原来的1/16。在提高重建效率的同时保持重建图像的质量。</p><h4 id="2）块剪枝"><a href="#2）块剪枝" class="headerlink" title="2）块剪枝"></a>2）块剪枝</h4><p>在训练样本中排除了标准差接近0的图像块，避免一些不含有信息的图像块影响学习过程。</p><h4 id="3）块标准化"><a href="#3）块标准化" class="headerlink" title="3）块标准化"></a>3）块标准化</h4><p>在训练前对训练图像对进行标准化处理：<br>$$<br>&lt; x _ { i } ,y _ { i } &gt; \leftarrow &lt; x _ { i } / || x _ { i } || ,y _ { i } / || y _ { i } || &gt; ,\quad \forall i = 1,2,\dots ,n<br>$$</p><p>同时也对测试样本$\left\{ x _ { j * } \right\} _ { j = 1} ^ { m } $ 进行标准化处理，并且在预测后进行恢复：</p><p>$$<br>f \left( \mathbf { x } _ { j * } \right) \leftarrow f \left( \mathbf { x } _ { j * } \right) \times || \mathbf { x } _ { j * } || ,\quad \forall j = 1,2,\dots ,m<br>$$</p><h2 id="NGPR模型"><a href="#NGPR模型" class="headerlink" title="NGPR模型"></a>NGPR模型</h2><p>作者提到了三个核函数：</p><h4 id="1-径向基函数核（RBF-kernel）"><a href="#1-径向基函数核（RBF-kernel）" class="headerlink" title="1)  径向基函数核（RBF kernel）"></a>1)  径向基函数核（RBF kernel）</h4><p>$$<br>k _ { S E i so} \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \sigma _ { f } ^ { 2} \exp \left( - \frac { 1} { 2l ^ { 2} } \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right) ^ { T } \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right) \right)<br>$$</p><p>其中，$ \sigma _ { f } ^ { 2}$  代表信号方差， $l$ 代表长度尺度。RBF核是各向同性的，可以用来描述具有较小欧拉距离的样本。但是各向同性会忽视图像块的结构信息，导致协方差矩阵产生线性依赖。</p><h4 id="2）加性测量噪声核（Additive-measurement-noise-kernel）"><a href="#2）加性测量噪声核（Additive-measurement-noise-kernel）" class="headerlink" title="2）加性测量噪声核（Additive measurement noise kernel）"></a>2）加性测量噪声核（Additive measurement noise kernel）</h4><p>$$<br>k _ { N o i s e } \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \sigma _ { n } ^ { 2} \delta \left( \mathbf { x } - \mathbf { x } ^ { \prime } \right)<br>$$</p><p>其中，$\sigma _ { n } ^ { 2}$ 代表噪声方差，$\delta$ 代表克罗内克函数。</p><h4 id="3）线性核（Linear-kernel）"><a href="#3）线性核（Linear-kernel）" class="headerlink" title="3）线性核（Linear kernel）"></a>3）线性核（Linear kernel）</h4><p>$$<br>k _ { L I N } \left( \mathbf { x } ,\mathbf { x } ^ { \prime } \right) = \mathbf { x} ^ { T } \mathbf { x} ^ { \prime }<br>$$</p><p>线性核是对称但是各向异性的，可以用来测量结构信息。RBF核的协方差总是大于0，忽略了负协方差也可获得的事实。线性核比RBF核更有利于反向相关的训练样本。因此线性核比RBF核能揭示更内在的结构。</p><p>最后作者将三个核函数进行组合，提出了一个新的核函数：<br>$$<br>k = k_{SEiso} + k_{Noise} + c \times k_{LIN}<br>$$<br>其中 $c$ 是一个常数，用来调整线性核的重要性。</p><h2 id="超参数确定"><a href="#超参数确定" class="headerlink" title="超参数确定"></a>超参数确定</h2><h4 id="1）噪声标准差-sigma-n"><a href="#1）噪声标准差-sigma-n" class="headerlink" title="1）噪声标准差 $\sigma _ { n }$"></a>1）噪声标准差 $\sigma _ { n }$</h4><p>由于 $I’_I$ 可以看作 $I’$ 的估计，那么 $I’-I’_I$ 可以看作噪声的估计。因此我们利用 $I’-I’_I$ 的标准差对 $\sigma _ { n }$进行初始化。<br>$$<br>\sigma _ { n } ^ { 2} = \frac { 1} { R _ { L R } \times C _ { L R } - 1} \sum _ { i = 1} ^ { R _ { 1k } \times C _ { 18} } \left[ \left( I ^ { \prime } ( i ) - I _ { I } ^ { \prime } ( i ) \right) - \left( \overline { I } ^ { \prime } - \overline { I } _ { I } ^ { \prime } \right) \right] ^ { 2}<br>$$</p><h4 id="2）长度尺度-l"><a href="#2）长度尺度-l" class="headerlink" title="2）长度尺度 $l$"></a>2）长度尺度 $l$</h4><p>$l$ 是RBF核的标准差，可以通过输入训练样本之间距离矩阵的标准差来进行初始化。<br>$$<br>l ^ { 2} = \frac { 1} { (R _ { L R } \times C _ { L R })^2 - 1} \sum _ { i, j=1} ^{ { R _ { L R } \times C _ { L R } }}(||P(i)-P(j)|| - m_d)^2<br>$$</p><p>$$<br>m_d = \frac { 1} { (R _ { L R } \times C _ { L R })^2 } \sum _ { i, j=1} ^{ { R _ { L R } \times C _ { L R } }}(||P_E(i)-P_E(j)||)<br>$$</p><h4 id="3）-信号标准差-sigma-f"><a href="#3）-信号标准差-sigma-f" class="headerlink" title="3） 信号标准差 $ \sigma _ { f }$"></a>3） 信号标准差 $ \sigma _ { f }$</h4><p>通过从 $I’$ 中提取的样本的标准差来进行初始化。<br>$$<br>\sigma _ { n } ^ { 2} = \frac { 1} { R _ { L R } \times C _ { L R } - 1} \sum _ { i = 1} ^{ { R _ { L R } \times C _ { L R } }}(I’(i) -\overline { I’ })^2<br>$$</p><h4 id="4-常数-c"><a href="#4-常数-c" class="headerlink" title="4) 常数 $c$"></a>4) 常数 $c$</h4><p>$$<br>c = \tau \sigma_f^2<br>$$</p><p>其中，$R_{LR}$ ，$ C_{LR}$ 分别代表 $I’$ 的行数和列数。$P(i)$ 代表 $I’_I$ 的 $i$ 个块。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;大部分使用GPR的SR方法是局部采样的，这些方法将图像分成几个固定大小（e.g  $45\times45$）并且有一小部分重叠的大块，对每个大块，只使用到其中小块的自相似性（e.g $3\times3$）来建立GPR模型，因此很难完全利用到自然图像中的自相似性。本文作者提出
      
    
    </summary>
    
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
      <category term="SSIR" scheme="http://yoursite.com/tags/SSIR/"/>
    
  </entry>
  
  <entry>
    <title>读论文《Single Image Super-Resolution using Gaussian Process Regression》</title>
    <link href="http://yoursite.com/2018/05/16/paper-ssir-using-gpr/"/>
    <id>http://yoursite.com/2018/05/16/paper-ssir-using-gpr/</id>
    <published>2018-05-16T07:01:06.000Z</published>
    <updated>2018-05-24T02:45:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文的作者提出了利用高斯回归过程来实现超分辨率重建的方法。该方法只需要输入低分辨率图像，而不需要额外的数据集和样本图像。</p><p>作者的灵感来自于自然图像中的结构冗余。两个像素之间的相似性可以定义为它们局部几何结构的差异。一个像素的邻近像素可以表明该像素的局部特征（比如平滑或边缘区域），因此可以预测基于回归的模型。</p><h2 id="高斯过程回归"><a href="#高斯过程回归" class="headerlink" title="高斯过程回归"></a>高斯过程回归</h2><p>高斯过程（GP）定义了函数 $f$ 的分布，$f$ 将输入空间从 𝒳 映射到 ℛ。对于 𝒳 的任意有限子集，其边缘分布 $P(f(x_1), f(x_2)…f(x_n))$ 是一个多元高斯分布，其中 $x$ 代表一个输入向量。通过均值函数 $m(x)$ 和协方差函数 $k(x_i, x_j)$ 我们有：<br>$$<br>f(x) \sim \mathcal{GP}(m(x), k(x_i,x_j))<br>$$<br>其中矩阵 $X​$ 的每一行为输入向量，$f​$ 是函数值的向量，$K(X, X)​$ 代表 $n \times n​$ 的协方差矩阵，其中 $K_(i,j) = k(x_i, x_j)​$</p><p>高斯过程回归（GPR）即我们假设了目标函数为高斯过程的先验分布。令 $y$ 表示一个观测值，$\epsilon$ 为高斯噪声，高斯过程回归模型表示如下：<br>$$<br>y = f(x) + \epsilon, \epsilon \sim \mathcal{N}(n, \sigma^2_n)<br>$$<br>均值函数为零的观测值 $\mathbf { y }$ 和输出 $f_*$ 的联合分布为：<br>$$<br>\left[\begin{array}{c} \mathbf { y } \\ f_*\end{array}\right] \sim \mathcal{N}\left( \begin{array}{cc}0, \left[\begin{array}{cc}K(X,X)+\sigma^2_nI &amp; K(X,X_*)\\K(X_*,X) &amp; K(X_*,X_*)\end{array} \right]\end{array} \right)<br>$$<br>其中$X$代表训练集，$X_*$代表测试集。我们可以推导出条件分布：<br>$$<br>f_*|X,y,X_* \sim \mathcal{N}(\bar{f}_*, V(f_*))<br>$$<br>其中，<br>$$<br>\bar{f}_* = K(X_*, X)[K(X,X) + \sigma^2_nI]^{-1}\mathbf { y }<br>$$</p><p>$$<br>V(f_*) = K(X_*, X_*) -K(X_*, X)[K(X,X) + \sigma^2_nI]^{-1}K(X,X_*)<br>$$</p><h2 id="单帧图像重建"><a href="#单帧图像重建" class="headerlink" title="单帧图像重建"></a>单帧图像重建</h2><p>高分辨率图像块由对应的低分辨率图像块逐像素地预测得出。取 $ 3\times3$ 大小的低分辨率图像块，观测值 $\mathbf { y }$ 为图像块的中心像素，$x$ 是一个8维向量，代表观测值的临近像素。</p><p>高分辨率图像的预测由两个由粗到细的阶段组成。</p><ol><li>上采样阶段：首先利用双线性插值法获得差值后的图像 $H_b$。然后将低分辨率图像 $L$ 分块，利用每个块的中心像素 $\mathbf { y }$ （目标像素）和它的临近像素 $X_{NL}$ 训练GPR模型 $M$ 。将 $H_b$ 中每个像素的临近像素$X_{NH_b}$ 输入 $M$ 计算出预测值 $p_\tilde{H}$。最后通过 $p_\tilde{H}$获得大致的上采样图像 $\tilde{H}$。</li><li>去模糊阶段：计算 $\tilde{H}$  的下采样图像 $\tilde{L}$ ，并对其进行分块（同上采样阶段 $L$ 的分块操作）。利用每个块的目标像素 $\mathbf { y }$ 和与其对应的 $\tilde{L}$ 中的临近像素 $X_{N\tilde{L}}$ 训练GPR模型 $M$。将 $\tilde{H}$ 中每个像素的临近像素$X_{N\tilde{H}}$ 输入 $M$ 计算出预测值 $p_H$。最后通过 $ p_H$获得最终的重建图像 $H$ 。</li></ol><p>完整的算法描述过程如下：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fre5esj0u5j30l00riaf8.jpg" alt=""></p><h2 id="协方差函数"><a href="#协方差函数" class="headerlink" title="协方差函数"></a>协方差函数</h2><p>在高斯过程回归中，协方差函数通过定义函数的相似性，编码了潜在的预测过程，因此是相当重要的。作者选择了平方指数协方差函数：</p><p>$$<br>k(x_i, x_j) = \sigma_f^2 exp(-\frac{1}{2}\frac{(x_i-x_j)’(x_i-x_j)}{ℓ^2})<br>$$<br>其中 $\sigma^2_f$ 代表信号方差(signal variance)，$ℓ$ 代表特征长度尺度(characteristic length scale)。由于目标像素周围强度值(intensity values)的差异表明了其所处位置，因此相似性的计算基于两个8维临近像素向量紧张值的欧拉距离。</p><p>通过可视化协方差矩阵 $K$， 作者表示平方指数协方差函数能很好地捕捉到图像块之间局部的以及全局的相似性。</p><p>在这个过程中我们主要需要学习的参数有：</p><ol><li>信号方差 $\sigma^2_f$</li><li>典型长度倍数 $ℓ$ ：可以看作是控制一个级别 $u$ 在一个1维高斯过程中向上交叉数量的参数</li><li>噪声方差 $\sigma^2_n$</li></ol><p>在贝叶斯回归模型中，超参数 $\theta$ 的后验概率可以表示为：<br>$$<br>p ( \mathbf { \theta } | \mathbf { X } ,\mathbf { y } ,\mathcal { H } ) = \frac { p ( \mathbf { y } | \mathbf { X } ,\theta ,\mathcal { H } ) p ( \mathbf { \theta } | \mathcal { H } ) } { \int p ( \mathbf { y } | \mathbf { X } ,\theta ,\mathcal { H } ) p ( \theta | \mathcal { H } ) d \theta }<br>$$<br>但是这个后验概率很难求解。在超分辨率问题中，可以通过最大化边际似然(marginal likelihood)来解决这个问题。在我们的模型中，边际似然可以表示为：<br>$$<br>p ( \mathbf { y } | \mathbf { X } ) = \int p ( \mathbf { y } | \mathbf { f } ,\mathbf { X } ) p ( \mathbf { f } | \mathbf { X } ) d \mathbf { f }<br>$$<br>已知 $\mathbf { y } | \mathbf { f } \sim \mathcal { N } \left( \mathbf { f } ,\sigma _ { n } ^ { 2} I \right)$ 和高斯过程先验分布 $f(x) \sim \mathcal{GP}(m(x), k(x_i,x_j))$，可以将上式转化为<br>$$<br>\log p ( \mathbf { y } | \mathbf { X } ,\theta ) = - \frac { 1} { 2} \mathbf { y } ^ { T } \mathbf { K } _ { y } ^ { - 1} \mathbf { y } - \frac { 1} { 2} \log | \mathbf { K } _ { y } | - \frac { n } { 2} \log 2\pi<br>$$<br>其中$ K _ { y } = K ( \mathbf { X } ,\mathbf { X } ) + \sigma _ { n } ^ { 2} I $，对参数求偏导得到：<br>$$<br>\frac { \partial \mathcal { L } } { \partial \theta _ { i } } = \frac { 1} { 2} \mathbf { y } ^ { T } \mathbf { K } ^ { - 1} \frac { \partial \mathbf { K } } { \partial \theta _ { i } } \mathbf { K } ^ { - 1} \mathbf { y } - \frac { 1} { 2} \operatorname{tr} \left( \mathbf { K } ^ { - 1} \frac { \partial \mathbf { K } } { \partial \theta _ { i } } \right)<br>$$<br>然后我们就可以利用梯度下降法来求解最优参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文的作者提出了利用高斯回归过程来实现超分辨率重建的方法。该方法只需要输入低分辨率图像，而不需要额外的数据集和样本图像。&lt;/p&gt;
&lt;p&gt;作者的灵感来自于自然图像中的结构冗余。两个像素之间的相似性可以定义为它们局部几何结构的差异。一个像素的邻近像素可以表明该像素的局部特征（比
      
    
    </summary>
    
    
      <category term="SISR" scheme="http://yoursite.com/tags/SISR/"/>
    
      <category term="GPR" scheme="http://yoursite.com/tags/GPR/"/>
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-gan</title>
    <link href="http://yoursite.com/2018/03/14/cs231n-assignment3-gan/"/>
    <id>http://yoursite.com/2018/03/14/cs231n-assignment3-gan/</id>
    <published>2018-03-14T11:44:48.000Z</published>
    <updated>2018-07-11T01:50:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>最后一份作业来会一会生成式对抗网络（Generative Adversarial Networks, GAN）。网络中有两个模型，生成模型 \(G\)（Generator）和判别模型 \(D\)（Discriminator）。\(D\) 的目标是判断一个图像是真的还是假的，\(G\) 的目标是欺骗 \(D\) 使其相信它生成的图像是真的。 <a href="https://www.msra.cn/zh-cn/news/features/gan-20170511" target="_blank" rel="external">到底什么是生成式对抗网络GAN？</a> 其中以男女朋友做比喻还蛮可爱的。 </p><a id="more"></a><p><img src="cs231n-assignment3-gan/model.png" alt="cross-validation"></p><p>整个训练过程其实是 \(G\) 和 \(D\) 之间的博弈，在这个博弈中有两个场景。第一个场景的输入是真实图像 <strong>x</strong>，输出概率 \(D(x)\) 指一个图片是真实图片的概率，在这个场景中 \(D(x)\) 努力接近1。第二场景的输入是噪音 <strong>z</strong>， \(G\) 生成一个新的图像 \(G(z)\) 努力欺骗 \(D\) 使 \(D(G(z))\) 接近1， \(D\) 努力让 \(D(G(z))\) 接近0。 </p><p>在本次作用中我们采用以下的更新规则：</p><ol><li>更新生成模型 (\(G\)) ，最大化判别模型对于生成的图像，作出<strong>错误选择</strong>的概率：<br>$$\underset{G}{\text{maximize}}\;  \mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$</li><li>更新判别模型 (\(D\))，最大化判别模型对于真实和生成的图像，作出<strong>正确选择</strong>的概率：<br>$$\underset{D}{\text{maximize}}\; \mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] + \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$</li></ol><h3 id="Vanilla-GAN"><a href="#Vanilla-GAN" class="headerlink" title="Vanilla GAN"></a>Vanilla GAN</h3><p>图像判别模型D（Discriminator）的损失函数：</p><p>$$ \ell_D = -\mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] - \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$</p><p>图像生成模型G（Generator）的损失函数：</p><p>$$\ell_G  =  -\mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$</p><p>我们可以通过计算二元交叉熵损失（binary cross entropy loss）来计算logits的对数概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bce_loss</span><span class="params">(input, target)</span>:</span></div><div class="line">    neg_abs = - input.abs()</div><div class="line">    loss = input.clamp(min=<span class="number">0</span>) - input * target + (<span class="number">1</span> + neg_abs.exp()).log()</div><div class="line">    <span class="keyword">return</span> loss.mean()</div></pre></td></tr></table></figure><p>交叉熵用于衡量两个取值为正数的函数的相似性，取值越小差异越小，因此最小化损失函数相当于最小化交叉熵。</p><p>对应上面的两个场景，<strong>D</strong> 判断真实图片的labels都为1，生成图片的labels都为0，可以得到下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_loss</span><span class="params">(logits_real, logits_fake)</span>:</span></div><div class="line">    N = logits_real.shape[<span class="number">0</span>]</div><div class="line">    real_labels = Variable(torch.ones(N)).type(dtype)</div><div class="line">    fake_labels = Variable(torch.zeros(N)).type(dtype)</div><div class="line">    loss = bce_loss(logits_real, real_labels) + bce_loss(logits_fake, fake_labels)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>上面我们说到 <strong>G</strong> 努力欺骗 <strong>D</strong> 使 <strong>D(G(z))</strong> 接近1，因此要将labels都设置为1，得到下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_loss</span><span class="params">(logits_fake)</span>:</span></div><div class="line">    N = logits_fake.shape[<span class="number">0</span>]</div><div class="line">    fake_labels = Variable(torch.ones(N)).type(dtype)</div><div class="line">    loss = bce_loss(logits_fake, fake_labels)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>最终训练结果：</p><p><img src="cs231n-assignment3-gan/vanilla_result.png" alt="cross-validation"></p><h3 id="Least-Square-GAN"><a href="#Least-Square-GAN" class="headerlink" title="Least Square GAN"></a>Least Square GAN</h3><p>图片生成模型G（Generator）的损失函数：</p><p>$$\ell_G  =  \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_generator_loss</span><span class="params">(scores_fake)</span>:</span></div><div class="line">    loss = ((scores_fake<span class="number">-1</span>)**<span class="number">2</span>).mean()</div><div class="line">    <span class="keyword">return</span> loss / <span class="number">2</span></div></pre></td></tr></table></figure><p>图像判别模型的损失函数：<br>$$ \ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_discriminator_loss</span><span class="params">(scores_real, scores_fake)</span>:</span></div><div class="line">    loss = ((scores_real<span class="number">-1</span>)**<span class="number">2</span>).mean() + (scores_fake**<span class="number">2</span>).mean()</div><div class="line">    <span class="keyword">return</span> loss / <span class="number">2</span></div></pre></td></tr></table></figure><p><img src="cs231n-assignment3-gan/ls_result.png" alt="cross-validation"></p><p>作业中提出了几个问题</p><blockquote><p>Describe how the visual quality of the samples changes over the course of training. Do you notice anything about the distribution of the samples? How do the results change across different training runs?</p></blockquote><p>可以看到通过训练，生成图像从噪音逐渐变为能辨认出来的数字。生成的图像中似乎1、3、9比较多。</p><h3 id="Deeply-Convolutional-GANs"><a href="#Deeply-Convolutional-GANs" class="headerlink" title="Deeply Convolutional GANs"></a>Deeply Convolutional GANs</h3><p>前面的网络结构都是使用全连接网络，搞事的科学家们加上了几层卷积网络，虽然训练速度很慢，但生成的图片可骗人了。。</p><p><img src="cs231n-assignment3-gan/dc_result.png" alt="cross-validation"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最后一份作业来会一会生成式对抗网络（Generative Adversarial Networks, GAN）。网络中有两个模型，生成模型 \(G\)（Generator）和判别模型 \(D\)（Discriminator）。\(D\) 的目标是判断一个图像是真的还是假的，\(G\) 的目标是欺骗 \(D\) 使其相信它生成的图像是真的。 &lt;a href=&quot;https://www.msra.cn/zh-cn/news/features/gan-20170511&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;到底什么是生成式对抗网络GAN？&lt;/a&gt; 其中以男女朋友做比喻还蛮可爱的。 &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-styletransfer</title>
    <link href="http://yoursite.com/2018/03/14/cs231n-assignment3-styletransfer/"/>
    <id>http://yoursite.com/2018/03/14/cs231n-assignment3-styletransfer/</id>
    <published>2018-03-14T01:42:13.000Z</published>
    <updated>2018-04-12T00:55:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>用神经网络基于两张画来生成一副新的画，一张画表现内容，一张画代表风格，科学家的脑洞也是大！作业里就实现了这种技术，下面一起来学习一下吧～</p><p><img src="cs231n-assignment3-styletransfer/style_content.png" alt=""></p><a id="more"></a><p>我们知道神经网络的损失函数非常重要，在图像风格转换的技术中，损失函数由三个部分组成：<strong>content loss + style loss + total variation loss</strong></p><h3 id="内容损失（Content-loss）"><a href="#内容损失（Content-loss）" class="headerlink" title="内容损失（Content loss）"></a>内容损失（Content loss）</h3><p>内容损失代表生成图的特征图(feature map)和原图的特征图的差异。我们仅关注网络的一层\(\ell\)，它的特征图为 \(A^\ell \in \mathbb{R}^{1 \times C_\ell \times H_\ell \times W_\ell}\)。我们会将特征图在空间上合并到一个维度，并利用这个变形后的版本。用 \(F^\ell \in \mathbb{R}^{N_\ell \times M_\ell}\) 代表当前图像的特征图，\(P^\ell \in \mathbb{R}^{N_\ell \times M_\ell}\) 代表原图的特征图，其中 \(M_\ell=H_\ell\times W_\ell\)。\(F^\ell\) 和 \(P^\ell\) 的每一行代表特定滤波器的激活向量，是由图像的所有位置卷积而成的。\(w_c\) 是内容损失的权重。</p><p>内容损失用公式表示为：</p><p>\(L_c = w_c \times \sum_{i,j} (F_{ij}^{\ell} - P_{ij}^{\ell})^2\)</p><p>不太理解作业里关于特征图的解释，因此结合之前整理过的CNN学习笔记，说一下自己理解的特征图。我觉得特征图其实就是卷积层的输出，卷积层通过滑动滤波器（filter，即卷积层的参数）来得到特征图。通常会有N个滤波器，每个滤波器的深度都是C。将每个滤波器的输出组合到一起构成的N行矩阵，因此<strong>每一行代表特定滤波器的激活向量</strong>。所以最终的代码很简洁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> content_weight * torch.sum((content_current - content_original).pow(<span class="number">2</span>))</div></pre></td></tr></table></figure><h3 id="风格损失（Style-loss）"><a href="#风格损失（Style-loss）" class="headerlink" title="风格损失（Style loss）"></a>风格损失（Style loss）</h3><p>​首先我们需要计算格拉姆矩阵(Gram matrix) G, 代表每个过滤器之间的相关性。利用上面提到的 \(F^\ell \in \mathbb{R}^ {1 \times C_\ell \times M_\ell} \)。那么 \(G^\ell \in \mathbb{R}^ {1 \times C_\ell \times C_\ell} \)：</p><p>$$G_{ij}^\ell  = \sum_k F^{\ell}_{ik} F^{\ell}_{jk}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(features, normalize=True)</span>:</span></div><div class="line">    N, C, H, W = features.shape</div><div class="line">    reshaped_features = features.view(N, C, <span class="number">-1</span>)</div><div class="line">    gram = reshaped_features.matmul(reshaped_features.transpose(<span class="number">1</span>, <span class="number">2</span>))</div><div class="line">    <span class="keyword">if</span>(normalize):</div><div class="line">        gram /= H * W * C</div><div class="line">    <span class="keyword">return</span> gram</div></pre></td></tr></table></figure><p>❗️计算格拉姆矩阵时应该要利用矩阵乘法，即<strong>torch.matmul()</strong></p><p>假设 \(G^\ell\) 是生成图的特征图的格拉姆矩阵，\(A^\ell\) 是原图的特征图的格拉姆矩阵，那么 \(\ell\) 层的损失函数：</p><p>$$L_s^\ell = w_\ell \sum_{i, j} \left(G^\ell_{ij} - A^\ell_{ij}\right)^2$$</p><p>在实际应用中，我们通常会多计算几个层的风格损失，并把它们加起来：</p><p>$$L_s = \sum_{\ell \in \mathcal{L}} L_s^\ell$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(feats, style_layers, style_targets, style_weights)</span>:</span></div><div class="line">    layers = len(style_layers)</div><div class="line">    loss = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(layers):</div><div class="line">        layer = style_layers[i]</div><div class="line">        loss += torch.sum(style_weights[i] * (gram_matrix(feats[layer]) - style_targets[i]).pow(<span class="number">2</span>))</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>❗️计算每层的loss得到的是一个大小等于G的Tensor，要用<strong>torch.sum()</strong>把它们统统加起来，变成一个数值</p><h3 id="总变分正则化（Total-variation-regularization）"><a href="#总变分正则化（Total-variation-regularization）" class="headerlink" title="总变分正则化（Total-variation regularization）"></a>总变分正则化（Total-variation regularization）</h3><p>为了使图像更加平滑，我们需要加入总变分项，可以通过计算相邻像素对（水平和垂直方向），像素值差异的平方和得到。计算公式如下：</p><p>$L_{tv} = w_t \times \sum_{c=1}^3\sum_{i=1}^{H-1} \sum_{j=1}^{W-1} \left( (x_{i,j+1, c} - x_{i,j,c})^2 + (x_{i+1, j,c} - x_{i,j,c})^2  \right)$</p><p>根据公式我们可以写出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span><span class="params">(img, tv_weight)</span>:</span></div><div class="line">    loss = <span class="number">0.0</span></div><div class="line">    _, _, H, W = img.shape</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H<span class="number">-1</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(W<span class="number">-1</span>):</div><div class="line">            loss += tv_weight*torch.sum((img[:, :, i, j+<span class="number">1</span>] - img[:, :, i, j]).pow(<span class="number">2</span>) + (img[:, :, i+<span class="number">1</span>, j] - img[:, :, i, j]).pow(<span class="number">2</span>))</div><div class="line">        </div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>但是作业要求不用循环，然后我又写不出来，参考了网上的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hor = torch.dist(img[:, :, :<span class="number">-1</span>, :], img[:, :, <span class="number">1</span>:, :])**<span class="number">2</span></div><div class="line"><span class="comment"># equivalent to (img[:, :, i+1, j] - img[:, :, i, j]).pow(2)</span></div><div class="line">ver = torch.dist(img[:, :, :, :<span class="number">-1</span>], img[:, :, :, <span class="number">1</span>:])**<span class="number">2</span></div><div class="line"><span class="comment"># equivalent to (img[:, :, i, j+1] - img[:, :, i, j]).pow(2)</span></div><div class="line"><span class="keyword">return</span> tv_weight * (hor + ver)</div></pre></td></tr></table></figure><p>这里的torch.dist()使用的是2范数，相当于差异平方和先根号再平方，理解的基础上倾向于下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hor = torch.sum((img[:, :, :<span class="number">-1</span>, :] - img[:, :, <span class="number">1</span>:, :])**<span class="number">2</span>)</div><div class="line">ver = torch.sum((img[:, :, :, :<span class="number">-1</span>] - img[:, :, :, <span class="number">1</span>:])**<span class="number">2</span>)</div></pre></td></tr></table></figure><p>循环法和向量法的效率还是差距很大的，用循环法测试时需要2.2s，而用向量法测试只要53ms，在后面训练的时候效果就很明显了，循环法基本是跑不动的。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用神经网络基于两张画来生成一副新的画，一张画表现内容，一张画代表风格，科学家的脑洞也是大！作业里就实现了这种技术，下面一起来学习一下吧～&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;cs231n-assignment3-styletransfer/style_content.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment3-rnn</title>
    <link href="http://yoursite.com/2018/02/08/cs231n-assignment3-rnn/"/>
    <id>http://yoursite.com/2018/02/08/cs231n-assignment3-rnn/</id>
    <published>2018-02-08T10:41:35.000Z</published>
    <updated>2018-02-09T07:51:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>递归神经网络（Recurrent Neural Network, RNN）是为了处理序列数据。之前学的一些机器学习算法的输入数据之间相互是没有联系的，当下一个数据出现的可能依赖于之前出现过的数据，爱捅幺蛾子的科学家们就想出了RNN。</p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>对比之前学习过的CNN，CNN的输入仅为数据，中间网络层的输入即上一层输出的向量，使用不同的参数计算得到该层的输出向量，最后再使用softmax或svm得到输出值。</p><p>而RNN加入了一个玩意儿叫<strong>状态（state）</strong>，RNN的输入包括序列数据的第一步和初始状态，中间的网络层的输入是时间序列中对应的那一步和上一层的状态，使用<strong>相同的参数</strong>计算得到该层的状态，通过这个状态计算输出。</p><p><img src="cs231n-assignment3-rnn/rnn_graph.png" alt=""></p><p>最普通的RNN使用tanh作为激活函数，计算公式如下：</p><p><img src="cs231n-assignment3-rnn/rnn_formula.png" alt=""></p><a id="more"></a><p>单步rnn的过程比较简单，tanh的求导需要利用到状态值<br>$$<br>f(x) = tanh = \frac{e^{x}-e^{-x}}{e^x+e^{-x}}<br>$$</p><p>$$<br>tan’h = \frac{(e^x-(-e^{-x}))(e^x+e^{-x})-(e^{x}-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}<br>$$</p><p>所以<br>$$<br>tan’h =1-f^2(x)<br>$$<br>在处理整个序列反向传播的时候出现了问题，在<a href="https://www.reddit.com/r/cs231n/comments/48c7wh/question_about_rnn_backward/" target="_blank" rel="external">Question about RNN backward</a>里明白了我出问题的地方。</p><p>首先python函数中，参数传递的是对象(call by object)，当我们引用数组对象并进行操作时，是会改变这个数组的。因此在进行数值验证的时候，使用到的dh实际上改变了，结果自然不正确。解决方法是使用<code>dh_copy = dh.copy()</code></p><p>其次通过<code>nn_step_backward</code>计算出的dprev_h，需要添加到dh里，即<code>dh[:, t, :] += dprev_h</code>，最后一次的dprev_h即为dh0。</p><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>完成了RNN最主要的计算接下来需要进行词嵌入啦～</p><p>假设输入2个句子(N)，每个句子有4个单词(T)，词汇表共有5个词语(V)，输入<code>X = [[0, 3, 1, 2], [2, 1, 0, 3]]</code>，词嵌入后的第一个句子：<br>$$<br>\begin{bmatrix}<br>1 &amp;0&amp;0&amp;0&amp;0\\<br>0&amp;0&amp;0&amp;1&amp;0\\<br>0&amp;1&amp;0&amp;0&amp;0\\<br>0&amp;0&amp;1&amp;0&amp;0<br>\end{bmatrix}<br>$$<br>正向传播我首先利用了一个循环来完成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</div><div class="line">    seq[n] = np.zeros((T, V))</div><div class="line">    seq[n][list(range(T)), x] = <span class="number">1</span> <span class="comment">#indexing</span></div><div class="line">    out[n] = seq[n].dot(W)</div></pre></td></tr></table></figure><p>反向传播利用seq进行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dW = np.zeros((V, D))</div><div class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(N):</div><div class="line">    dW += seq[n].T.dot(dout[n, :, :])</div></pre></td></tr></table></figure><p>但是当词汇表很大的时候，将seq保存在cache中，会占用了很大的内存，一定有更好的办法😏</p><p>再仔细研究了一下python的indexing，其实可以同时扩展seq的三个维度，这样就省略了for循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rows = np.array(range(N))[:, np.newaxis] </div><div class="line">seq = np.zeros((N, T, V))</div><div class="line">seq[np.tile(rows, T), [list(range(T))]*N, x] = <span class="number">1</span></div><div class="line">out = seq.dot(W)</div></pre></td></tr></table></figure><p>至于反向传播没有使用到np.add.at，说明代码还不够高效。我们已经假设T=4，V=5，那么写出<code>seq[0].T.dot(dout[0, :, :])</code>一个具体公式（dout当然不可能这么大，我随便写的）：<img src="cs231n-assignment3-rnn/word_back1.png" alt=""></p><p>可以看到这个矩阵乘法是在进行行变化，这就是矩阵左乘的变换意义。再来仔细分析一下左边这个伪初等矩阵。<img src="cs231n-assignment3-rnn/word_back2.png" alt=""></p><p>它的每一行其实代表一个单词，每一列代表一个时刻，dw的结果即为每个单词在它出现时刻的误差的累加。还是用上面的例子来解释，为方便说明假设下标从1开始。w1在t1时刻出现，dw1就应该加上dout1，w2在t3时刻出现，dw2就应该加上dout3，w5没有出现，就没啥好加了。因此俺们的代码终于可以写出来了！（竟然这么简洁）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">np.add.at(dW, x, dout)</div></pre></td></tr></table></figure><p><strong>参考链接</strong></p><ul><li><a href="https://foofish.net/python-function-args.html" target="_blank" rel="external">Python 函数中，参数是传值，还是传引用</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;递归神经网络（Recurrent Neural Network, RNN）是为了处理序列数据。之前学的一些机器学习算法的输入数据之间相互是没有联系的，当下一个数据出现的可能依赖于之前出现过的数据，爱捅幺蛾子的科学家们就想出了RNN。&lt;/p&gt;
&lt;h3 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h3&gt;&lt;p&gt;对比之前学习过的CNN，CNN的输入仅为数据，中间网络层的输入即上一层输出的向量，使用不同的参数计算得到该层的输出向量，最后再使用softmax或svm得到输出值。&lt;/p&gt;
&lt;p&gt;而RNN加入了一个玩意儿叫&lt;strong&gt;状态（state）&lt;/strong&gt;，RNN的输入包括序列数据的第一步和初始状态，中间的网络层的输入是时间序列中对应的那一步和上一层的状态，使用&lt;strong&gt;相同的参数&lt;/strong&gt;计算得到该层的状态，通过这个状态计算输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;cs231n-assignment3-rnn/rnn_graph.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最普通的RNN使用tanh作为激活函数，计算公式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;cs231n-assignment3-rnn/rnn_formula.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n-assignment2-pytorch</title>
    <link href="http://yoursite.com/2018/01/26/cs231n-assignment2-pytorch/"/>
    <id>http://yoursite.com/2018/01/26/cs231n-assignment2-pytorch/</id>
    <published>2018-01-26T09:12:01.000Z</published>
    <updated>2018-01-28T05:56:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>用pytorch来构建自己的网络啦～记录一下训练过程</p><a id="more"></a><p>先搬运一下作业给的Hints：</p><h3 id="Things-you-should-try"><a href="#Things-you-should-try" class="headerlink" title="Things you should try:"></a>Things you should try:</h3><ul><li><strong>Filter size</strong>: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient</li><li><strong>Number of filters</strong>: Above we used 32 filters. Do more or fewer do better?</li><li><strong>Pooling vs Strided Convolution</strong>: Do you use max pooling or just stride convolutions?</li><li><strong>Batch normalization</strong>: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?</li><li><strong>Network architecture</strong>: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:<ul><li>[conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li><li>[conv-relu-conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li><li>[batchnorm-relu-conv]xN -&gt; [affine]xM -&gt; [softmax or SVM]</li></ul></li><li><strong>Global Average Pooling</strong>: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">Google’s Inception Network</a> (See Table 1 for their architecture).</li><li><strong>Regularization</strong>: Add l2 weight regularization, or perhaps use Dropout.</li></ul><h3 id="Tips-for-training"><a href="#Tips-for-training" class="headerlink" title="Tips for training"></a>Tips for training</h3><p>For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:</p><ul><li>If the parameters are working well, you should see improvement within a few hundred iterations</li><li>Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.</li><li>Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.</li><li>You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.</li></ul><p>看完我的内心是崩溃的，每次构建一个网络都要调节学习率，所以我在<strong>train</strong>函数里设置迭代200次就退出循环，先通过loss大致比较一下不同网络结构的效率，最后再仔细学习～</p><h3 id="Try-1"><a href="#Try-1" class="headerlink" title="Try 1"></a>Try 1</h3><p>首先我将第一个卷积层的filter size改成了3，改变卷积层过滤器的数量。然后看了半天不知道strided convolution是啥找到了一个学习笔记视频☞ <a href="http://www.bilibili.com/video/av15968996/" target="_blank" rel="external">什么是strided convolution? 跳出格怎么处理？</a> 原来strided convolution就是stride不为1的卷积层:) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Strided convolution</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"><span class="comment"># no Max Pooling</span></div><div class="line">...</div><div class="line">nn.Linear(<span class="number">7200</span>, <span class="number">1024</span>), <span class="comment"># affine layer (32-3+1)/2</span></div><div class="line"></div><div class="line"><span class="comment"># Pooling</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</div><div class="line">...</div><div class="line">nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</div><div class="line">...</div><div class="line">nn.Linear(<span class="number">7200</span>, <span class="number">1024</span>), <span class="comment"># affine layer</span></div></pre></td></tr></table></figure><p>设置stride为2刚好最后进入全连接时都是7200个神经元～先大致查找了一下学习率的范围，然后缩小范围learning_rate = [10 ** np.random.uniform(-5, -2) for i in range(20)]分别尝试两个模型的准确率。</p><p>Strided convolution同学的准确率在50%以上的不多，然鹅Pooling同学准确率达到50%的就比较多了，说明使用Pooling的效果比较好。</p><p><img src="cs231n-assignment2-pytorch/strided_conv.png" alt="cross-validation"></p><p><img src="cs231n-assignment2-pytorch/pooling.png" alt="cross-validation"></p><p>猜想：因为Strided convolution多跳了几个格子，可能会遗漏某些图像特征，而Max-pooling过滤出最大的像素值，反而强调了图像特征。</p><h3 id="Try-2"><a href="#Try-2" class="headerlink" title="Try 2"></a>Try 2</h3><p>比较一下过滤器数量为50和20两个模型的效率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">10</span> ** np.random.uniform(<span class="number">-5</span>, <span class="number">-3</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</div><div class="line"><span class="comment"># Convolution of 50 filters</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">50</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>) </div><div class="line">...</div><div class="line">nn.Linear(<span class="number">11250</span>, <span class="number">1024</span>) <span class="comment"># (32-3+1)/2 **2 x 50</span></div><div class="line"><span class="comment"># Convolution of 20 filters</span></div><div class="line">nn.Conv2d(<span class="number">3</span>, <span class="number">20</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>) </div><div class="line">...</div><div class="line">nn.Linear(<span class="number">4500</span>, <span class="number">1029</span>) <span class="comment"># (32-3+1)/2 **2 x 20</span></div></pre></td></tr></table></figure><p><img src="cs231n-assignment2-pytorch/conv_num_filters.jpg" alt="cross-validation"></p><p>随机了10个学习率，可以看到相对来说还是过滤器比较多的结果比较好，但同时也比较耗时</p><h3 id="Try-3"><a href="#Try-3" class="headerlink" title="Try 3"></a>Try 3</h3><p>使用上面50个过滤器的模型，在第一个卷积层之后、全连接层之后加Batch normalization。</p><p><img src="cs231n-assignment2-pytorch/batch_norm.png" alt="cross-validation"></p><p>虽然准确率最高的没有上面不使用Batch normalization的高，但是整体水平提升了。但是Hints里貌似是说训练速度变快了？但是俺忘记打印出时间了。。。</p><h3 id="Try-4"><a href="#Try-4" class="headerlink" title="Try 4"></a>Try 4</h3><p>之前总结神经网络的时候就总结过，softmax代表交叉熵损失（cross-entropy loss），SVM是折页损失（hinge loss），这个在pytorch里都有对应的loss函数。接下来就是要改变网络结构啦～</p><p>模型结构：[conv-relu-pool]x2 -&gt; [affine]x2 -&gt; [softmax]（在每层中间都插入了batchnorm）</p><table><thead><tr><th>type</th><th>patch size/stride</th><th>input size</th></tr></thead><tbody><tr><td>conv</td><td>3 x 3 / 1</td><td>32x32x3</td></tr><tr><td>pool</td><td>2 x 2 / 2</td><td>30x30x50</td></tr><tr><td>conv</td><td>3 x 3 / 1</td><td>15x15x50</td></tr><tr><td>pool</td><td>2 x 2 / 2</td><td>13x13x30</td></tr><tr><td>linear</td><td>logits</td><td>1x1x1080</td></tr><tr><td>linear</td><td>logits</td><td>1x1x500</td></tr><tr><td>softmax</td><td>classifier</td><td>1x1x10</td></tr></tbody></table><p>搜索一下合适的学习率～</p><p><img src="cs231n-assignment2-pytorch/network1.png" alt="cross-validation"></p><p>可以看到最好的准确率接近0.6啦，撒花～现在用学习率训练1个epoch试试</p><p><img src="cs231n-assignment2-pytorch/network1_epoch1.png" alt="cross-validation"></p><p>训练5个epoch～</p><p><img src="cs231n-assignment2-pytorch/network1_epoch5.png" alt="cross-validation"></p><p>准确率达到70%啦，撒花~</p><h3 id="Try-5"><a href="#Try-5" class="headerlink" title="Try 5"></a>Try 5</h3><p>现在我们再改变一下网络结构，尝试一下<strong>整体平均汇聚</strong>，也就是不在中间使用多个全链接层，而是将图片训练得足够小大约（7, 7），在最后使用一个汇聚层使得图像变成 (1, 1 , Filter#)。</p><p>模型结构：[conv-relul]x4 -&gt; [pool]x1 -&gt; [softmax]</p><table><thead><tr><th>type</th><th>patch size/stride</th><th>input size</th></tr></thead><tbody><tr><td>conv</td><td>3 x 3 / 1</td><td>32x32x3</td></tr><tr><td>conv</td><td>3 x 3 / 2</td><td>30x30x32</td></tr><tr><td>conv</td><td>3 x 3 / 1</td><td>14x14x64</td></tr><tr><td>conv</td><td>3 x 3 / 2</td><td>12x12x80</td></tr><tr><td>pool</td><td>5 x 5</td><td>5x5x100</td></tr><tr><td>linear</td><td>logits</td><td>1x1x100</td></tr><tr><td>softmax</td><td>classifier</td><td>1x1x10</td></tr></tbody></table><p>尝试了几个迭代，在一开始测试随机学习率的时候都不如上面一个模型，决定再换一个结构～</p><h3 id="Try-5-1"><a href="#Try-5-1" class="headerlink" title="Try 5"></a>Try 5</h3><p>[batchnorm-relu-conv]x7 -&gt; [pool]x1 -&gt; [softmax]</p><p>太久了。。放弃。。我决定回到Try4的模型多来几层。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">    </div><div class="line">model1 = nn.Sequential( </div><div class="line">                    nn.Conv2d(<span class="number">3</span>, <span class="number">50</span>, kernel_size=<span class="number">3</span>, stride = <span class="number">1</span>), <span class="comment"># input: 32x32x3</span></div><div class="line">                    nn.BatchNorm2d(<span class="number">50</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.BatchNorm2d(<span class="number">50</span>),</div><div class="line">                    nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># input: 28x28x32</span></div><div class="line">                    </div><div class="line">                    nn.Conv2d(<span class="number">50</span>, <span class="number">80</span>, kernel_size=<span class="number">3</span>, stride = <span class="number">1</span>), <span class="comment"># input: 14x14x32</span></div><div class="line">                    nn.BatchNorm2d(<span class="number">80</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.BatchNorm2d(<span class="number">80</span>),</div><div class="line">                    nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 8x8x150</span></div><div class="line">    </div><div class="line">                    Flatten(), </div><div class="line">                    nn.Linear(<span class="number">2880</span>, <span class="number">1000</span>),</div><div class="line">                    nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                    nn.Linear(<span class="number">1000</span>,<span class="number">10</span>)</div><div class="line">            )</div></pre></td></tr></table></figure><p><img src="cs231n-assignment2-pytorch/val_accr.png" alt="cross-validation"></p><p>经过10个epoch，loss已经挺小的了，而且达到了74%的准确率，现在在测试集上尝试</p><p><img src="cs231n-assignment2-pytorch/test_accr.png" alt="cross-validation"></p><p>完成啦～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用pytorch来构建自己的网络啦～记录一下训练过程&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
