<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-15T11:20:47.822Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/11/05/Scalable%20training%20of%20artificial%20neural%20networks%20with%20adaptive%20sparse%20connectivity%20inspired%20by%20network%20science/"/>
    <id>http://yoursite.com/2019/11/05/Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science/</id>
    <published>2019-11-05T01:59:32.849Z</published>
    <updated>2019-11-15T11:20:47.822Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prune-during-training"><a href="#Prune-during-training" class="headerlink" title="Prune during training"></a>Prune during training</h1><blockquote><p>Sparse evolutionary training (SET) simplifies pruneâ€“regrowth cycles by using heuristics for random growth at the end of each training epoch.</p></blockquote><p>ä¸»è¦é’ˆå¯¹å…¨è¿æ¥å±‚ï¼Œæå‡ºåˆ©ç”¨ sparse connected layerä»£æ›¿fully connected layerã€‚</p><p>sparse connected layeré¦–å…ˆæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œåˆ©ç”¨çš„æ˜¯ErdoÌˆsâ€“ReÌnyi random graphæ¥ç”Ÿæˆæ‹“æ‰‘ç»“æ„ï¼Œå…¶ä¸­$h^k_i$å’Œ$h_j^{k-1}$çš„è¿æ¥æ¦‚ç‡ä¸ºï¼š<br>$$<br>p\left(W_{i j}^{k}\right)=\frac{\varepsilon\left(n^{k}+n^{k-1}\right)}{n^{k} n^{k-1}}<br>$$<br>è¶…å‚$\epsilon$æ§åˆ¶ç¨€ç–åº¦ã€‚åœ¨æ¯æ¬¡è®­ç»ƒæ—¶ä¸€éƒ¨åˆ†å…·æœ‰smallest positive weightså’Œlargest negative weightsè¢«ç§»é™¤ã€‚ä¸‹ä¸€é˜¶æ®µå†ç”Ÿæˆä¸å‰ªæ‰connectionsç›¸åŒæ•°é‡çš„weightsã€‚</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191105100616370.png" alt="image-20191105100616370"></p><h3 id="NeST-A-Neural-Network-Synthesis-Tool-Based-on-a-Grow-and-Prune-Paradigm"><a href="#NeST-A-Neural-Network-Synthesis-Tool-Based-on-a-Grow-and-Prune-Paradigm" class="headerlink" title="NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm"></a>NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm</h3><ul><li><p>gorw connections and neurons based on gradient information </p></li><li><p>prune away insignificant connections based on magnitude information</p></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191105103114233.png" alt="image-20191105103114233"></p><p>å‰ªæï¼š<br>$$<br>\mathbf{u}^{l}=\left[\left(\mathbf{W}^{l} \mathbf{x}^{l-1}+\mathbf{b}^{l}\right)-\mathbf{E}\right] \oslash \mathbf{V}=\mathbf{W}_{_}^{l} \mathbf{x}+\mathbf{b}_{_}^{l}<br>$$</p><p>$$<br>\mathbf{W}_{_}^{l}=\mathbf{W}^{l} \oslash \mathbf{V}, \mathbf{b}_{_}^{l}=\left(\mathbf{b}^{l}-\mathbf{E}\right) \oslash \mathbf{V}<br>$$</p><h3 id="Parameter-Efficient-Training-of-Deep-Convolutional-Neural-Networks-by-Dynamic-Sparse-Reparameterization"><a href="#Parameter-Efficient-Training-of-Deep-Convolutional-Neural-Networks-by-Dynamic-Sparse-Reparameterization" class="headerlink" title="Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization"></a>Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization</h3><p>ICML2019</p><blockquote><p>Dynamic Sparse Reparameterization (DSR) (Mostafa &amp; Wang, 2019) implements a pruneâ€“ redistributeâ€“regrowth cycle where target sparsity levels are redistributed among layers, based on loss gradients </p></blockquote><p>æœ¬æ–‡è§£å†³çš„å…¶å®ä¸‹åˆ—é—®é¢˜ï¼šåœ¨è®­ç»ƒä¸­ç»™å®šä¸€ä¸ªfixed budget of parameters, how to train it to yield the best generalization performance.</p><p>é¦–å…ˆæ‰€æœ‰çš„å‚æ•°éšæœºåˆå§‹åŒ–ä¸ºç›¸åŒçš„sparsityã€‚è®­ç»ƒè¿‡ç¨‹ä¹Ÿåˆ†ä¸ºä¸¤æ­¥</p><ol><li>magnitude-based pruningã€‚åˆ©ç”¨ä¸€ä¸ªå…¨å±€threshold $H$è¿›è¡Œå‰ªæã€‚</li><li>random growthã€‚å½“ç§»é™¤æ‰Kä¸ªparametersåï¼ŒKä¸ªzero-initializedçš„å‚æ•°ä¼šè¢«é‡æ–°åˆ†é…ï¼ŒåŸºäºä»¥ä¸‹å‡†åˆ™ï¼šæœ‰æ›´å¤šä¸ä¸º0å€¼æƒé‡çš„å±‚ä¼šæœ‰æ›´å¤šçš„free parametersã€‚ä¹Ÿå°±æ˜¯è¯´free parametersåº”è¯¥è¦è¢«å†åˆ†å¸ƒåˆ°ä¸€äº›èƒ½å¤Ÿæ¥å—larger loss gradientsçš„æƒé‡ä¸Šã€‚ï¼ˆå°±åƒNesTä¸­growçš„æ€æƒ³ï¼‰</li></ol><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191105103724605.png" alt="image-20191105103724605"></p><p>ä¸ºäº†è®©pruneå’Œgrowçš„æƒé‡æ•°é‡æ˜¯ç›¸åŒçš„ï¼Œæ–‡ç« ç”¨äº†é¢å¤–çš„ä¿æŠ¤æªæ–½ã€‚</p><p>æœ¬æ–‡çš„å¯¹æ¯”å®éªŒæ¯”è¾ƒå……åˆ†ï¼Œä¸6ä¸ªbaselineè¿›è¡Œäº†æ¯”è¾ƒ</p><ol><li>Full dense: åŸå§‹çš„denseç½‘ç»œ</li><li>Thin denseï¼šæœ‰æ›´å°‘å±‚çš„æ¨¡å‹</li><li>Static sparse: è®­ç»ƒè¿‡ç¨‹ä¸­sparseçš„åœ°æ–¹æ˜¯ä¸å˜çš„</li><li>Compress sparse: é€šè¿‡prune-retrainä¸€ä¸ªå¤§çš„æ¨¡å‹</li><li>DeepR</li><li>SET</li></ol><p>thin denseå’Œstatic sparseç½‘ç»œè®­äº†æ›´å¤šçš„epochsã€‚</p><h3 id="Dynamic-pruning-with-feedback"><a href="#Dynamic-pruning-with-feedback" class="headerlink" title="Dynamic pruning with feedback"></a>Dynamic pruning with feedback</h3><p>ICCV2020</p><p>æœ¬æ–‡æå‡ºçš„DPFï¼Œé€šè¿‡åé¦ˆæœºåˆ¶è¿›è¡Œå‰ªæã€‚åœ¨pruendçš„æ¨¡å‹ä¸Šè®¡ç®—çš„æ¢¯åº¦$\tilde{\mathbf{w}}_{t}=\mathbf{m}_{t} \odot \mathbf{w}_{t}$ï¼Œåº”ç”¨åœ¨dense æ¨¡å‹çš„æƒé‡$w_t$ä¸Šï¼š<br>$$<br>\mathbf{w}_{t+1}:=\mathbf{w}_{t}-\gamma_{t} \mathbf{g}\left(\mathbf{m}_{t} \odot \mathbf{w}_{t}\right)=\mathbf{w}_{t}-\gamma_{t} \mathbf{g}\left(\tilde{\mathbf{w}}_{t}\right)<br>$$<br>æ–‡ç« è¯´è¿™æ ·å¯ä»¥è®©æ¨¡å‹ recover form â€errorsâ€ã€‚æš‚æ—¶maskæ‰ä¸€äº›ç‰¹å®šweightï¼Œè¿™äº›weightåœ¨åç»­è®­ç»ƒä¸­èƒ½å¤Ÿå†æ¬¡è¢«æ¿€æ´»ã€‚</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191105102431690.png" alt="image-20191105102431690"></p><h3 id="Sparse-Networks-from-Scratch-Faster-Training-without-Losing-Performance"><a href="#Sparse-Networks-from-Scratch-Faster-Training-without-Losing-Performance" class="headerlink" title="Sparse Networks from Scratch: Faster Training without Losing Performance"></a>Sparse Networks from Scratch: Faster Training without Losing Performance</h3><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191115191831435.png" alt="image-20191115191831435"></p><p>ç”¨momentumæ¥è®¡ç®—magnitudeã€‚ä¸»è¦ç”±ä¸‰ä¸ªé˜¶æ®µæ„æˆï¼š</p><ul><li>redistribution of weights</li><li>pruning weights</li><li>regrowing weights</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Prune-during-training&quot;&gt;&lt;a href=&quot;#Prune-during-training&quot; class=&quot;headerlink&quot; title=&quot;Prune during training&quot;&gt;&lt;/a&gt;Prune during training&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/11/02/Approximated%20Oracle%20FIlter%20Pruning/"/>
    <id>http://yoursite.com/2019/11/02/Approximated Oracle FIlter Pruning/</id>
    <published>2019-11-02T07:18:15.182Z</published>
    <updated>2019-11-04T10:51:09.066Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Approximated-Oracle-FIlter-Pruning"><a href="#Approximated-Oracle-FIlter-Pruning" class="headerlink" title="Approximated Oracle FIlter Pruning"></a>Approximated Oracle FIlter Pruning</h1><p>æœ¬æ–‡æå‡ºäº†AOFPæ¡†æ¶ï¼Œé€šè¿‡binary searchæœç´¢ä¸‹ä¸€ä¸ªå‰ªæfiltersï¼Œå¹¶ä¸”åŒæ—¶finetuneæ¨¡å‹ã€‚</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191102155014824.png" alt="image-20191102155014824"></p><p>æ¯ä¸ªç½‘ç»œå±‚$i$ï¼Œåˆ©ç”¨å‰ä¸€å±‚çš„è¾“å‡ºå’Œå½“å‰å±‚çš„filtersåšå¦‚ä¸‹æ˜ å°„ï¼š<br>$$<br>\boldsymbol{M}^{(i)}=\zeta^{(i)}\left(\boldsymbol{M}^{(i-1)}, \mathcal{F}_{i}\right)<br>$$<br>ç”¨$T$æ¥ä»£è¡¨filterçš„é‡è¦æ€§ï¼Œoracleçš„æ ‡å‡†ï¼š<br>$$<br>T(\boldsymbol{F})=\sum_{(x, y) \in(X, Y)}(L(x, y, \mathcal{F}-\boldsymbol{F})-L(x, y, \mathcal{F}))<br>$$<br>æ¯æ¬¡åªç ä¸€ä¸ªfilterså¤ªè€—æ—¶ï¼Œå¼•å…¥å‰ªæç²’åº¦$g$ï¼Œå¯¹å°‘æ•°filtersä¸€èµ·å‰ªæã€‚</p><h3 id="å­˜åœ¨é—®é¢˜ï¼š"><a href="#å­˜åœ¨é—®é¢˜ï¼š" class="headerlink" title="å­˜åœ¨é—®é¢˜ï¼š"></a>å­˜åœ¨é—®é¢˜ï¼š</h3><ol><li>Oracle Pruningçš„åé¦ˆæœºåˆ¶å¤ªè€—æ—¶ </li><li>æ¯æ¬¡feedbackåªèƒ½åˆ¤æ–­ä¸€å±‚çš„é‡è¦æ€§ã€‚ </li></ol><p>éœ€è¦å¯»æ‰¾æ–¹æ¡ˆç¼©çŸ­åé¦ˆå¾ªç¯ï¼Œè¿˜è¦èƒ½å¤Ÿå¹¶è¡Œçš„ç‹¬ç«‹åˆ¤æ–­æ¯å±‚çš„é‡è¦æ€§ã€‚</p><h3 id="è§£å†³é—®é¢˜ï¼š"><a href="#è§£å†³é—®é¢˜ï¼š" class="headerlink" title="è§£å†³é—®é¢˜ï¼š"></a>è§£å†³é—®é¢˜ï¼š</h3><ol><li><p>ç¼©çŸ­åé¦ˆå¾ªç¯ï¼Œæå‡ºäº†<strong>Damage isolation</strong>ï¼šå°†CNNå¯ä»¥çœ‹ä½œä¸€ä¸ªçŠ¶æ€æœºï¼Œ$i$å±‚çš„æ”¹å˜ä¸èƒ½è¢«$i+2$å±‚çœ‹åˆ°ï¼Œç”±äº$i+1$å±‚çš„éš”ç¦»ã€‚å› æ­¤å°†fiiltersçš„é‡è¦æ€§$Tâ€™$è®¾ä¸ºå¯¹$i+1$è¾“å‡ºçš„å½±å“ï¼š<br>$$<br>T^{\prime}(\boldsymbol{F})=\frac{1}{|X|} \sum_{x \in X} t(\boldsymbol{F}, x)<br>$$</p><p>$$<br>t(\boldsymbol{F}, x)=\frac{\left|\boldsymbol{M}^{(i+1)}(x)-\zeta^{(i+1)}\left(\boldsymbol{M}_{\boldsymbol{F}}^{(i)}(x), \mathcal{F}^{(i+1)}\right)\right|_{2}^{2}}{\left|\boldsymbol{M}^{(i+1)}(x)\right|_{2}^{2}}<br>$$</p><p>$t(\boldsymbol{F}, x)$åæ˜ äº†å¯¹$i$å±‚è¿›è¡Œå‰ªæå¯¹$i+1$å±‚è¾“å‡ºçš„å½±å“ã€‚</p><p>ç”¨ä¸Šé¢é‚£å¼ å›¾ç‰‡ç†è§£ï¼Œæˆ‘ä»¬å¯¹conv1è¿›è¡Œå‰ªæï¼Œç”¨conv2çš„è¾“å‡ºæ¥è®¡ç®—conv1çš„é‡è¦æ€§ã€‚</p></li><li><p>æ¯”è¾ƒå¸¸è§åœ°æ˜¯scoreå®Œfinetuneï¼ˆä¸²è¡Œï¼‰ï¼Œæœ¬æ–‡çš„æå‡ºçš„æ¡†æ¶å¹¶è¡Œåœ°è¿›è¡Œscoreå’Œfinetuneã€‚</p><p>å¯¹$i$å±‚è¿›è¡Œå‰ªæï¼Œä¹‹åå°†è®¡ç®—æµåˆ†æˆä¸¤æ”¯base pathå’Œscore pathã€‚</p><ul><li>bath pathåŒ…å«mask $u$ï¼Œç”¨è¿™ä¸ªéƒ¨åˆ†æ¥è®¡ç®—lossï¼Œæ›´æ–°ç½‘ç»œå‚æ•°</li><li>score pathåŒ…å«mask $v$ï¼Œç”¨è¿™ä¸ªéƒ¨åˆ†è®¡ç®—$t$</li></ul><p>è¾“å…¥batch dataï¼ŒåŸºäºbase pathï¼Œåœ¨score pathéšæœºå°†ä¸€äº›filtersç½®ä¸º0ã€‚å¯¹æ¯”baseå’Œscore pathçš„æ•°å€¼ã€‚</p><blockquote><p>ç”±äºåœ¨score pathè®¡ç®—åˆ†æ•°ï¼Œå…¶å®ä¸å½±å“base pathçš„è¾“å‡ºï¼Œæ‰€ä»¥ä¹Ÿä¸å½±å“æ›´é«˜levelçš„fitersã€‚</p></blockquote><p>å½“æ‰€æœ‰filterséƒ½è¢«æ‰“åˆ†ä»¥ä»¥åï¼Œå°†$g$ä¸ªæœ€ä½$T$å€¼çš„filtersæ°¸ä¹…ç½®0ã€‚</p><p>æœ¬æ–‡å°†å¤„ç†ä¸€ä¸ªæˆ–å¤šä¸ªfiltersçš„è¿‡ç¨‹ç§°ä¸ºmoveæ“ä½œã€‚</p></li></ol><p>ç®—æ³•æµç¨‹ğŸ‘‡</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20191104142609953.png" alt="image-20191104142609953"></p><h3 id="å…·ä½“æ“ä½œï¼š"><a href="#å…·ä½“æ“ä½œï¼š" class="headerlink" title="å…·ä½“æ“ä½œï¼š"></a>å…·ä½“æ“ä½œï¼š</h3><p>ä½œè€…é’ˆå¯¹è‡ªå·±æå‡ºçš„æ¡†æ¶æå‡ºäº†å‡ ç‚¹éœ€è¦è§£å†³çš„é—®é¢˜:</p><ol><li>é’ˆå¯¹ç‰¹å®šå±‚ï¼Œç”±äºfinetuneå’Œpruneä¸¤ä¸ªæ“ä½œæ˜¯åŒæ—¶è¿›è¡Œçš„ï¼Œæ¯æ¬¡finetuneçš„filterså’Œpruneçš„filtersæ˜¯ä¸åŒçš„ï¼Œå¯¼è‡´æ¯ä¸ªfilterçš„åˆ†æ•°æ˜¯ä¸å‡†ç¡®çš„ã€‚</li><li><p>ä¸èƒ½å¾ˆå¥½åœ°è§£å†³æ¯æ¬¡çš„å‰ªæç²’åº¦$g$ã€‚</p></li><li><p>ä¸çŸ¥é“å•¥æ—¶å€™ç»ˆæ­¢å‰ªæã€‚</p></li></ol><p>ä¸€ç‚¹ç‚¹çš„æ¥æ”»ç ´ï¼š</p><ol><li>æŠŠå¯¹æ‰€æœ‰filters rankçš„æ“ä½œåˆ©ç”¨äºŒåˆ†æœç´¢çš„æ–¹å¼è¿›è¡Œè¿‘ä¼¼ï¼Œç”±<strong>ç²—åˆ°ç»†</strong>çš„è¿›è¡Œæœç´¢ã€‚é’ˆå¯¹ç‰¹å®šå±‚ï¼šé¦–å…ˆç”±ä¿ç•™çš„filtersæ„æˆæœç´¢ç©ºé—´$A$ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­éšæœºé€‰æ‹©$|A|/2$ çš„filtersè¿›è¡Œå‰ªæï¼Œfinetuneå¹¶æ”¶é›†$t$ï¼Œæ€»å…±è¿­ä»£$\phi$ä¸ªbatchesã€‚å¯¹æ”¶é›†åˆ°çš„$\hat{T}$å–å‡å€¼è¿‘ä¼¼ä½œä¸ºæ¯ä¸ªfiltersçš„é‡è¦æ€§ã€‚é€‰æ‹©$\hat{T}$æœ€å°çš„$|A|/2$æ„æˆpicked set $B$ã€‚ç„¶åç¼©å°æœç´¢ç©ºé—´$A \leftarrow B$ã€‚ç›´åˆ°$B$ä¸º0æˆ–è€…max damage å°äºæŸä¸ªé˜ˆå€¼ã€‚å¦‚æœmax damageå°äºé˜ˆå€¼ï¼Œè¯´æ˜è¯¥å±‚è¿˜å¯ä»¥ç»§ç»­å‰ªã€‚å¦‚æœmax damgeå¤§äºæŸä¸ªé˜ˆå€¼å¹¶ä¸”å‰©ä½™é€šé“ä¸º1æ—¶ï¼Œè¯´æ˜è¯¥å±‚å°±ä¸èƒ½å‰ªäº†ã€‚ç„¶åæ”¹å˜$u$ï¼Œè¿›è¡Œé™æ€å‰ªæã€‚</li><li>å‰ªæç²’åº¦$g$çš„é—®é¢˜å…¶å®åŒ…å«åœ¨ä¸Šè¿°ç®—æ³•ä¸­ï¼Œå½“$B$è¶³å¤Ÿå¥½æ—¶ï¼Œç»“æŸæŸå±‚çš„æœç´¢å³$g=B$</li></ol><p>Pros: </p><ul><li>å¹¶è¡Œæ“ä½œæ¯å±‚çš„æ€æƒ³å¾ˆå¥½ï¼Œå› æ­¤å¯ä»¥åœ¨æ¯å±‚è¿›è¡Œç‹¬ç«‹åœ°äºŒåˆ†æœç´¢ï¼Œç”¨äºå†³å®šwhich filters are importantã€‚</li></ul><p>Cons:</p><ul><li>æœ¬æ–‡è‡ªå·±ä¹Ÿè¯´äº†ä¹‹å‰çš„filtersä¼šå¯¹ä¹‹åçš„filtersäº§ç”Ÿå½±å“ï¼Œé‚£å®ƒæå‡ºçš„åªè®¡ç®—ä¸‹ä¸€å±‚è¾“å‡ºçš„damageè¿˜åˆç†å—ï¼Ÿ(ä½†æ˜¯ä¸è¿™æ ·å°±æ²¡åŠæ³•å¹¶è¡Œäº†å•Šï¼‰</li><li>ç”¨äºŒåˆ†æœç´¢çš„æ–¹å¼å¯ä»¥ç¡®å®šæ¯æ¬¡çš„å‰ªæç²’åº¦ï¼Œä½†æ˜¯è¿™ä¸ªç²’åº¦å…¶æ˜¯å—åˆ°ä¸€ä¸ªå…¨å±€è¶…å‚$\theta$è¿›è¡Œæ§åˆ¶çš„ï¼Œè¯´æ˜æ¯å±‚éƒ½ä¸€æ ·ï¼Ÿæ„Ÿè§‰è¿™ä¸ªè¶…å‚å¾ˆéš¾è¿›è¡Œé€‰æ‹©ã€‚å¦‚ä½•æ”¹è¿›ï¼Ÿ</li></ul><blockquote><p>å‰ªæå…¶å®è¦è§£å†³çš„ä¸¤ä¸ªé—®é¢˜å°±æ˜¯æ¯æ¬¡å‰ªå“ªé‡Œï¼Œæ¯æ¬¡å‰ªå¤šå°‘ã€‚å‰ªå“ªé‡Œå³ä»£è¡¨äº†filtersçš„é‡è¦æ€§ã€‚å‰ªå¤šå°‘åˆ™ä¸ºå‰ªæç²’åº¦ã€‚æœ¬æ–‡æŠŠå±‚å’Œå±‚åˆ†ç¦»å¼€æ¥ï¼ŒåŸºäºå±‚å†…è¿›è¡Œé‡è¦æ€§çš„åˆ¤æ–­ã€‚å…ˆåˆ¤æ–­è¯¥å‰ªå“ªäº›filtersï¼Œæ…¢æ…¢å‰ªæ‰æœ€è¯¥å‰ªæ‰çš„é‚£äº›ï¼Œåœ¨æ¯æ¬¡å‰ªæçš„æ—¶å€™é€šè¿‡ä¸€ä¸ªå…¨å±€è¶…å‚æ¥æ§åˆ¶æ¯æ¬¡å‰ªæçš„ç²’åº¦ã€‚</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Approximated-Oracle-FIlter-Pruning&quot;&gt;&lt;a href=&quot;#Approximated-Oracle-FIlter-Pruning&quot; class=&quot;headerlink&quot; title=&quot;Approximated Oracle FIlt
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.11</title>
    <link href="http://yoursite.com/2019/09/17/weekly-paper-11/"/>
    <id>http://yoursite.com/2019/09/17/weekly-paper-11/</id>
    <published>2019-09-17T02:07:11.000Z</published>
    <updated>2019-09-21T05:06:08.085Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-EFFICIENT-MULTI-OBJECTIVE-NEURAL-ARCHITEC-TURE-SEARCH-VIA-LAMARCKIAN-EVOLUTION"><a href="#1ï¸âƒ£-EFFICIENT-MULTI-OBJECTIVE-NEURAL-ARCHITEC-TURE-SEARCH-VIA-LAMARCKIAN-EVOLUTION" class="headerlink" title="1ï¸âƒ£ EFFICIENT MULTI-OBJECTIVE NEURAL ARCHITEC-TURE SEARCH VIA LAMARCKIAN EVOLUTION"></a>1ï¸âƒ£ EFFICIENT MULTI-OBJECTIVE NEURAL ARCHITEC-TURE SEARCH VIA LAMARCKIAN EVOLUTION</h3><ul><li>å¤šç›®æ ‡ä¼˜åŒ–<ul><li>æå‡ºLEMONADEè¿›åŒ–ç®—æ³•</li></ul></li><li>è®¡ç®—å¤æ‚åº¦åº¦é«˜<ul><li>approximate network morphisms</li></ul></li></ul><p><strong>LEMONADE</strong></p><p>å¤šç›®æ ‡ï¼š</p><ul><li>expensive-to-evaluate objectives (valid error)</li><li>cheap-to-evaluate objects (model size)</li></ul><p>å˜å¼‚ï¼š</p><ul><li><p>network morphism operators (insert convolution, insert skip connection, increase number of filters)</p></li><li><p>approximate network morphism operators (remove layer, prune filters, replace layer)</p></li></ul><p>ç®—æ³•è¿­ä»£ï¼š</p><ol><li><p>åŸºäºcheapç›®æ ‡è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼Œä»è¯¥æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·å‡ºçˆ¶ä»£ç½‘ç»œ</p></li><li><p>åˆ©ç”¨NM/ANMå˜å¼‚ç”Ÿæˆå­ä»£ç½‘ç»œ</p></li><li><p>åŸºäºcheapç›®æ ‡è®¡ç®—æ¦‚ç‡åˆ†å¸ƒï¼Œä»è¯¥æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·å‡ºå­ä»£ç½‘ç»œ</p></li><li><p>å¯¹é‡‡æ ·å‡ºçš„å­ä»£è¿›è¡Œè¯„ä¼°</p></li><li>åˆ©ç”¨å½“å‰ç§ç¾¤å’Œç”Ÿæˆçš„å­ä»£è®¡ç®—Pareto frontï¼Œç”Ÿæˆæ–°ä¸€ä»£ç§ç¾¤</li></ol><p>æ­¥éª¤1.ä¸­çš„æ¦‚ç‡åˆ†å¸ƒåŸºäºä¸€ä¸ªæ ¸å¯†åº¦ä¼°è®¡ï¼š<br>$$<br>p_{\mathcal{P}}(N)=\frac{c}{p_{K D E}\left(f_{\text {cheap }}(N)\right)}<br>$$<br>â€‹                           </p><p>è¿›è¡Œä¸¤æ¬¡é‡‡æ ·çš„åŸå› ï¼šå˜å¼‚å‡ºçš„å­ä»£ç½‘ç»œcheapç›®æ ‡ä¸çˆ¶ä»£å¾ˆæ¥è¿‘ï¼Œå¾ˆå¯èƒ½åœ¨$f_{cheap}$å¯†åº¦è¾ƒä½çš„åŒºåŸŸç”Ÿæˆå­ä»£ã€‚</p><h3 id="2ï¸âƒ£-Deep-Learning-Architecture-Search-by-Neuro-Cell-based-Evolution-with-Function-Preserving-Mutations"><a href="#2ï¸âƒ£-Deep-Learning-Architecture-Search-by-Neuro-Cell-based-Evolution-with-Function-Preserving-Mutations" class="headerlink" title="2ï¸âƒ£ Deep Learning Architecture Search by Neuro-Cell-based Evolution with Function-Preserving Mutations"></a>2ï¸âƒ£ Deep Learning Architecture Search by Neuro-Cell-based Evolution with Function-Preserving Mutations</h3><p>æœ¬æ–‡ä¸»è¦æ˜¯Chen et al.çš„æ‰©å±•å·¥ä½œï¼ŒåŠ å…¥äº†separable conv.</p><p>å‡è®¾ï¼šå·ç§¯ç½‘ç»œç”±ä¸€ç³»åˆ—ç¥ç»å…ƒç»†èƒæ„æˆï¼Œå¹¶ä¸”åˆ©ç”¨function-preservingæ“ä½œä¸æ–­è¿›è¡Œæ‚äº¤ã€‚</p><p><strong>Function-Preseving</strong></p><p>å°†teacher network $f$ è½¬åŒ–ä¸ºstudent network $g$ çš„å½“ä¸”ä»…å½“è¾“å‡ºä¿æŒä¸å˜çš„æ“ä½œï¼š<br>$$<br>\forall \mathrm{x} : f\left(\mathrm{x} | \boldsymbol{\theta}^{(f)}\right)=g\left(\mathrm{x} | \boldsymbol{\theta}^{(g)}\right)<br>$$</p><ul><li><p>Layer Widening</p><p>å°†filtersæ•°ä»oæ‰©å……åˆ°$oâ€™$ï¼Œæ‰©å……éƒ¨åˆ†ä»å·²æœ‰éƒ¨åˆ†ä¸­éšæœºé‡‡æ ·ã€‚</p></li></ul><p>  $$<br>  V_{\cdot, \cdot, j}^{(l)}=\left\{\begin{array}{ll}{W_{\cdot, \cdot, j}^{(l)},} &amp; {j \leq o} \\ {W_{\cdot, \cdot, r}^{(l)}} &amp; {r \text { uniformly sampled from }\{1, \ldots, o\}}\end{array}\right.<br>  $$<br>  ä¸ºäº†ä¿æŒfunction-preservingç‰¹æ€§ï¼Œéœ€è¦åœ¨ä¸‹ä¸€å±‚æƒé‡ä¹Ÿåšå¤„ç†ï¼Œé™¤ä»¥é‡å¤æ•°é‡$n_j$ï¼š<br>  $$<br>  V_{\cdot, j,}^{(l+1)}=\frac{1}{n_{j}} W_{\cdot, \cdot, j}^{(l+1)}<br>  $$<br>  æœ¬æ–‡å°†è¿™ä¸ªæœºåˆ¶æ‰©å±•åˆ°äº†separable conv</p><ul><li><p>Layer Deepeningï¼šä¹Ÿæ˜¯æ‰©å±•åˆ°äº†seprable conv</p></li><li><p>Kernel Wideningï¼š padding</p></li><li>Insert Skip Connectionsï¼šæ·»åŠ æƒé‡ä¸º0çš„æ®‹å·®è¿æ¥</li><li>Branch Layers</li></ul><h3 id="3ï¸âƒ£-RETHINKING-THE-SMALLER-NORM-LESS-INFORMATIVE-ASSUMPTION-IN-CHANNEL-PRUNING-OF-CONVOLUTION-LAYERS-ICLR-2018"><a href="#3ï¸âƒ£-RETHINKING-THE-SMALLER-NORM-LESS-INFORMATIVE-ASSUMPTION-IN-CHANNEL-PRUNING-OF-CONVOLUTION-LAYERS-ICLR-2018" class="headerlink" title="3ï¸âƒ£ RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS [ICLR 2018]"></a>3ï¸âƒ£ RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS [ICLR 2018]</h3><p>æœ¬æ–‡åˆ†æäº†regularizationå¯èƒ½å¤±è´¥å¹¶ä¸”é€‚ç”¨èŒƒå›´å±€é™çš„ä¸¤ä¸ªç‚¹ï¼š</p><ol><li><p>Model Reparameterization</p><p>æ§åˆ¶ç½‘ç»œä¸åŒå±‚çš„æƒé‡normæ˜¯å¾ˆéš¾çš„ã€‚éœ€è¦ä¸reparameterization patternsä½œæ–—äº‰ã€‚ä»¥Lassoä¸ºä¾‹ï¼š<br>$$<br>\min _{\left\{W_{i}\right\}_{i=1}^{2 n}} \mathbb{E}_{(x, y) \sim \mathcal{D}}\left|W_{2 n} _ \ldots _ W_{2} _ W_{1} _ x-y\right|^{2}+\lambda \sum_{i=1}^{n}\left|W_{2 i}\right|_{j}<br>$$<br>ç”±äºè§£å¹¶ä¸æ˜¯å”¯ä¸€çš„ï¼Œæ°¸è¿œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå…¶å®ƒå‚æ•°é›†åˆ$\left\{W_{i}^{\prime}\right\}_{i=1}^{2 n}$ ï¼Œä½¿å¾—total lossæ›´å°ï¼Œä½†ä¹Ÿæ»¡è¶³$l_0$normä¸å˜ï¼š<br>$$<br>W_{i}^{\prime}=\alpha W_{i}, i=1,3, \ldots, 2 n-1 \text { and } W_{i}^{\prime}=W_{i} / \alpha, i=2,4, \ldots, 2 n<br>$$<br>åŸºäºæ¢¯åº¦çš„å­¦ä¹ åœ¨æ¢ç´¢è¿™æ ·çš„reparameterization patternsæ—¶å¾ˆä¸é«˜æ•ˆã€‚</p></li><li><p>Transform Invariance</p><p>batch normalizationä¸weight regularizationæ˜¯ä¸å…¼å®¹çš„ã€‚<br>$$<br>x^{l+1}=\max \left\{\gamma \cdot \mathrm{BN}_{\mu, \sigma, \epsilon}\left(W^{l} * x^{l}\right)+\beta, 0\right\}<br>$$<br>å½“å¯¹$W^l$è¿›è¡Œuniform scalingï¼Œç”±äºåç»­çš„BNæ“ä½œï¼Œå®é™…ä¸Šå¯¹è¾“å‡º$x^{l+1}$æ˜¯æ²¡æœ‰å½±å“çš„ã€‚æ­¤å¤–ï¼Œå¦‚æœå¯¹å¤šå±‚æƒé‡ä¸€èµ·normï¼Œæ˜¯ä¸çŸ¥é“å¦‚ä½•é€‰æ‹©æ¯å±‚åˆé€‚çš„penaltyã€‚</p></li></ol><p>æœ¬æ–‡çš„å‰ªæç­–ç•¥æ˜¯é€šè¿‡ISTAç®—æ³•ç¨€ç–BNçš„$\gamma$ã€‚å…·ä½“ç®—æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="/var/folders/57/3j4d0hq111q_7_xy_7jvjbm40000gn/T/ro.nextwave.Snappy/ro.nextwave.Snappy/B6E153F0-7C45-490E-824D-FC3D5E9B6E50.png" alt="B6E153F0-7C45-490E-824D-FC3D5E9B6E50"></p><p>æœ€ä¸»è¦çš„ä¸‰ä¸ªè¶…å‚ï¼š</p><ol><li>$\mu$ (learning rate)ï¼šåŠ é€Ÿæ”¶æ•›å’Œç¨€ç–ã€‚ä½†æ˜¯è¿‡å¤§ä¼šå¯¼è‡´SGDä¸æ”¶æ•›ã€‚</li><li>$\rho$ (sparse penalty)ï¼šæ”¶æ•›æ—¶çš„ç¨€ç–ç‡ã€‚</li><li>$\alpha$ (rescaling): ç”¨äºpre-trainedæ¨¡å‹çš„trickï¼Œå¯ä»¥è®©æƒé‡çš„ä¼˜åŒ–æ…¢äº$\lambda$çš„ä¼˜åŒ–ã€‚</li></ol><p>æœ¬æ–‡æäº†å‡ ç‚¹è¶…å‚ä¼˜åŒ–ç­–ç•¥ï¼š</p><ol><li>æ£€æŸ¥ cross-enterpy loss å’Œ regularizatoin lossï¼Œé€‰æ‹©ä¸€ä¸ª$\rho$ä½¿å¾—ä¸¤ä¸ªlossåœ¨ä¸€å¼€å§‹æ¯”è¾ƒæ¥è¿‘ã€‚</li><li>é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å­¦ä¹ ç‡ã€‚</li><li>å¦‚æœæ˜¯pretrainedæ¨¡å‹ï¼Œæ£€æŸ¥æ¨¡å‹ä¸­ $\lambda$ çš„å¹³å‡æ•°å€¼ï¼Œé€‰æ‹©ä¸€ä¸ª $\alpha$ ä½¿å¾— $\gamma^l$ è¿‘ä¼¼äº$100\mu\lambda^l\rho$ã€‚</li><li>å¦‚æœåœ¨ä¸€å¼€å§‹ regularizatoin loss çº¿æ€§ä¸‹é™ï¼Œ$\lambda$s åœ¨0å‘¨å›´ï¼Œå¯èƒ½decrease $\alpha$ and restartï¼›å¦‚æœ$\lambda$s çš„ç¨€ç–ç‡å¾ˆå¿«æ¥è¿‘100%ï¼Œå¯èƒ½éœ€è¦decrease $\rho$ and restartï¼›å¦‚æœ cross-entropy lossä¿æŒä¸å˜/å¿«é€Ÿä¸Šå‡ï¼Œå¯èƒ½éœ€è¦decrease $\mu$ or $\rho$ and restartã€‚</li></ol><blockquote><p>æ„Ÿè§‰å’ŒNetwork slimmingç›¸æ¯”ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆå¾ˆå¤§çš„äº®ç‚¹ã€‚é™¤äº†æå‡ºçš„$\gamma-W$ rescaling tirckï¼Œå’Œç”¨ISTAç®—æ³•æ¥ç¨€ç–$\gamma$ã€‚</p></blockquote><h3 id="4ï¸âƒ£-Net2Net-ACCELERATING-LEARNING-VIA-KNOWLEDGE-TRANSFER"><a href="#4ï¸âƒ£-Net2Net-ACCELERATING-LEARNING-VIA-KNOWLEDGE-TRANSFER" class="headerlink" title="4ï¸âƒ£ Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER"></a>4ï¸âƒ£ Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER</h3><p>æœ¬æ–‡æå‡ºäº†function-preseving transformationsï¼Œè®©çŸ¥è¯†ä»å°ç½‘ç»œåˆ°å¤§ç½‘ç»œè¿ç§»ã€‚</p><h3 id="5ï¸âƒ£-Data-Distillation-Towards-Omni-Supervised-Learning"><a href="#5ï¸âƒ£-Data-Distillation-Towards-Omni-Supervised-Learning" class="headerlink" title="5ï¸âƒ£ Data Distillation: Towards Omni-Supervised Learning"></a>5ï¸âƒ£ Data Distillation: Towards Omni-Supervised Learning</h3><p>self-trainingï¼šåœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”¨å®ƒä»¬æ¥æ›´æ–°æ¨¡å‹ã€‚</p><p>æœ¬æ–‡é€šè¿‡ensembleå¤šä¸ªdata transformationsæ¥è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚</p><p>self-pruningï¼Ÿ</p><ol><li>åœ¨labeled dataä¸Šè®­ç»ƒæ¨¡å‹</li><li>å°†è®¯å·çš„æ¨¡å‹åº”ç”¨åœ¨ä½¿ç”¨å¤šä¸ªtransformationsçš„unlabeld dataæ•°æ®é›†ä¸Š</li><li>å°†unlabeld dataçš„é¢„æµ‹ç»“æœé€šè¿‡ensembleè½¬ä¸ºlabels</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-EFFICIENT-MULTI-OBJECTIVE-NEURAL-ARCHITEC-TURE-SEARCH-VIA-LAMARCKIAN-EVOLUTION&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-EFFICIENT-MULTI-OBJECTIVE-NEURAL-A
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>è’¸é¦ä¹±ç‚–</title>
    <link href="http://yoursite.com/2019/09/17/%E8%92%B8%E9%A6%8F%E4%B9%B1%E7%82%96/"/>
    <id>http://yoursite.com/2019/09/17/è’¸é¦ä¹±ç‚–/</id>
    <published>2019-09-17T02:07:11.000Z</published>
    <updated>2019-12-10T17:02:28.554Z</updated>
    
    <content type="html"><![CDATA[<p>### </p><h1 id="è’¸é¦ä¹±ç‚–"><a href="#è’¸é¦ä¹±ç‚–" class="headerlink" title="è’¸é¦ä¹±ç‚–"></a>è’¸é¦ä¹±ç‚–</h1><p>å‚è€ƒ<a href="https://github.com/HobbitLong/RepDistiller" target="_blank" rel="external">RepDistiller</a>ï¼Œæ•´ç†ä¸€ä¸‹ç›®å‰SOTAçš„è’¸é¦æ–¹æ³•ã€‚</p><ol><li><p><strong>(KD) - Distilling the Knowledge in a Neural Network </strong></p><p>Hintonæçš„ï¼Œåœ¨æ¸©åº¦çš„ä½œç”¨ä¸‹ï¼Œæœ€å°åŒ–Tå’ŒSè¾“å‡ºæ¦‚ç‡çš„äº¤å‰ç†µï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">p_s = F.log_softmax(y_s/T, dim=<span class="number">1</span>)</div><div class="line">p_t = F.softmax(y_t/T, dim=<span class="number">1</span>)</div><div class="line">loss = F.kl_div(p_s, p_t, size_average=<span class="keyword">False</span>) * (T**<span class="number">2</span>) / y_s.shape[<span class="number">0</span>]</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(FitNet) - Fitnets: hints for thin deep net,  _ICLR2015_</strong></p><p>å¯ä»¥è§£å†³Tå’ŒSçš„feature mapå°ºå¯¸ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¹Sçš„feature mapè¿›è¡Œå˜æ¢åï¼Œå’ŒTçš„feature mapåšMSE lossï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">regress_s = ConvReg(feat_s[opt.hint_layer].shape, feat_t[opt.hint_layer].shape)</div><div class="line">f_s = regress_s(feat_s[opt.hint_layer])</div><div class="line">f_t = feat_t[opt.hint_layer]</div><div class="line">loss = F.mse_loss(f_s, f_t)</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(AT) - Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, _ICLR2017_</strong></p><p>æŠŠTå’ŒSçš„ä¿¡æ¯å‹ç¼©åˆ°spatialç»´åº¦ $\mathcal{F}: R^{C \times H \times W} \rightarrow R^{H \times W}$ï¼Œæœ€å°åŒ–å®ƒä»¬ä¹‹é—´çš„è·ç¦»ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">at</span><span class="params">(x)</span>:</span> F.normalize(x.pow(<span class="number">2</span>).mean(<span class="number">1</span>).view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</div><div class="line">loss = (at(f_s) - at(f_t)).pow(<span class="number">2</span>).mean()</div></pre></td></tr></table></figure></li><li><p><strong>(SP) - Similarity-Preserving Knowledge Distillation, _ICCV2019_</strong></p><p>åŸºäºATçš„æ”¹è¿›ï¼Œè®©Tå’ŒSçš„ä¿¡æ¯åœ¨mini-batchçš„å›¾åƒä¹‹é—´å°½å¯èƒ½ç›¸ä¼¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bsz = f_s.shape[<span class="number">0</span>]</div><div class="line">f_s = f_s.view(bsz, <span class="number">-1</span>)</div><div class="line">f_t = f_t.view(bsz, <span class="number">-1</span>)</div><div class="line"></div><div class="line">G_s = F.normalize(torch.mm(f_s, torch.t(f_s)))</div><div class="line">G_t = F.normalize(torch.mm(f_t, torch.t(f_t)))</div><div class="line"></div><div class="line">loss = (G_t - G_s).pow(<span class="number">2</span>).view(<span class="number">-1</span>, <span class="number">1</span>).sum(<span class="number">0</span>) / (bsz * bsz)</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(CC) - Correlation Congruence for Knowledge Distillation, _ICCV2019_</strong></p><p>åœ¨embedded spaceï¼Œä¸åŒå±‚çš„feature mapä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œè®©è¿™ä¸ªç›¸å…³æ€§ä¿æŒä¸€è‡´ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">embed_s = LinearEmbed(feat_s[<span class="number">-1</span>].shape[<span class="number">1</span>], opt.feat_dim)</div><div class="line">embed_t = LinearEmbed(feat_t[<span class="number">-1</span>].shape[<span class="number">1</span>], opt.feat_dim)</div><div class="line">delta = torch.abs(embed_s - embed_t)</div><div class="line">loss = torch.mean((delta[:<span class="number">-1</span>] * delta[<span class="number">1</span>:]).sum(<span class="number">1</span>))</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(VID) - Variational Information Distillation for Knowledge Transfer, _CVPR2019_</strong></p><p>æœ€å¤§åŒ–Tå’ŒSä¹‹é—´çš„äº’ä¿¡æ¯$I(t ; s)$ã€‚è¿™ä¸ªäº’ä¿¡æ¯å¯ä»¥ç†è§£ä¸ºå·²çŸ¥å­¦ç”ŸSçš„ä¿¡æ¯ï¼Œå¯¹Tä¸­çŸ¥è¯†ä¸ç¡®å®šæ€§çš„å‡å°‘é‡ã€‚æœ€å¤§åŒ–äº’ä¿¡æ¯å³å¸Œæœ›è¿™ä¸ªä¸ç¡®å®šæ€§å‡å°‘å¾—å°½å¯èƒ½å¤šã€‚ç”±äºäº’ä¿¡æ¯ä¸­çš„æ¦‚ç‡åˆ†å¸ƒéš¾ä»¥è®¡ç®—ï¼Œç”¨å˜åˆ†æ¦‚ç‡è¿›è¡Œè¿‘ä¼¼ï¼Œå³æœ€å¤§åŒ–activationsé—´çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$\mathbb{E}_{t, s}[\log q(\boldsymbol{t} | s)]$ã€‚æœ¬æ–‡é‡‡ç”¨çš„$q$æ˜¯é«˜æ–¯åˆ†å¸ƒï¼ˆè¿™ä¸ªè¿‡ç¨‹ç”¨äº†å¾ˆå¤šlog, expè¿›è¡Œæ•°å€¼è½¬æ¢ï¼Œéœ€è¦æ¶è¡¥æ•°å­¦ï¼‰</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">self.log_scale = torch.nn.Parameter(</div><div class="line">np.log(np.exp(init_pred_var-eps)<span class="number">-1.0</span>) * torch.ones(num_target_channels)</div><div class="line">)</div><div class="line"></div><div class="line">pred_mean = self.regressor(f_s)</div><div class="line">pred_var = torch.log(<span class="number">1.0</span>+torch.exp(self.log_scale))+self.eps</div><div class="line">pred_var = pred_var.view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)</div><div class="line">neg_log_prob = <span class="number">0.5</span>*(</div><div class="line">(pred_mean-target)**<span class="number">2</span>/pred_var+torch.log(pred_var)</div><div class="line">)</div><div class="line">loss = torch.mean(neg_log_prob)</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(RKD) - Relational Knowledge Distillation, _CVPR2019_</strong></p><p>è¿™ä¸ªä¸­å¿ƒæ€æƒ³å’ŒCCæ˜¯ä¸€è‡´çš„ï¼Œéƒ½æ˜¯è®©Tä¸­ä¿¡æ¯çš„ç›¸å¯¹ç»“æ„åœ¨Sä¸­ä¹Ÿä¿æŒä¸€è‡´ã€‚ä½†æ˜¯CCé¦–å…ˆå¯¹featureåšäº†ä¸€ä¸ªæ˜ å°„ï¼Œæœ¬æ–‡æäº†pairwiseçš„distance losså’Œternaryçš„angle lossã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">student = f_s.view(f_s.shape[<span class="number">0</span>], <span class="number">-1</span>)</div><div class="line">teacher = f_t.view(f_t.shape[<span class="number">0</span>], <span class="number">-1</span>)</div><div class="line"></div><div class="line"><span class="comment"># RKD distance loss</span></div><div class="line">t_d = self.pdist(teacher, squared=<span class="keyword">False</span>)</div><div class="line">mean_td = t_d[t_d &gt; <span class="number">0</span>].mean()</div><div class="line">t_d = t_d / mean_td</div><div class="line"></div><div class="line">d = self.pdist(student, squared=<span class="keyword">False</span>)</div><div class="line">mean_d = d[d &gt; <span class="number">0</span>].mean()</div><div class="line">d = d / mean_d</div><div class="line"></div><div class="line">loss_d = F.smooth_l1_loss(d, t_d)</div><div class="line"></div><div class="line"><span class="comment"># RKD Angle loss</span></div><div class="line"><span class="keyword">with</span> torch.no_grad():</div><div class="line">td = (teacher.unsqueeze(<span class="number">0</span>) - teacher.unsqueeze(<span class="number">1</span>))</div><div class="line">norm_td = F.normalize(td, p=<span class="number">2</span>, dim=<span class="number">2</span>)</div><div class="line">t_angle = torch.bmm(norm_td, norm_td.transpose(<span class="number">1</span>, <span class="number">2</span>)).view(<span class="number">-1</span>)</div><div class="line"></div><div class="line">sd = (student.unsqueeze(<span class="number">0</span>) - student.unsqueeze(<span class="number">1</span>))</div><div class="line">norm_sd = F.normalize(sd, p=<span class="number">2</span>, dim=<span class="number">2</span>)</div><div class="line">s_angle = torch.bmm(norm_sd, norm_sd.transpose(<span class="number">1</span>, <span class="number">2</span>)).view(<span class="number">-1</span>)</div><div class="line"></div><div class="line">loss_a = F.smooth_l1_loss(s_angle, t_angle)</div><div class="line"></div><div class="line">loss = self.w_d * loss_d + self.w_a * loss_a</div><div class="line"></div><div class="line"><span class="keyword">return</span> loss</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(PKT) - Learning Deep Representations with Probabilistic Knowledge Transfer, _ECCV2018_</strong></p><p>åŸºäºKDEè®¡ç®—featuresçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ©ç”¨cosine similarity-basedä½œä¸ºå¯†åº¦ä¼°è®¡çš„æ ¸ã€‚ç”¨KLæ•£åº¦è®©è€å¸ˆå’Œå­¦ç”Ÿä¿¡æ¯çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒç›¸ä¼¼ã€‚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Normalize each vector by its norm</span></div><div class="line">output_net_norm = torch.sqrt(torch.sum(output_net ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>))</div><div class="line">output_net = output_net / (output_net_norm + eps)</div><div class="line">output_net[output_net != output_net] = <span class="number">0</span></div><div class="line"></div><div class="line">target_net_norm = torch.sqrt(torch.sum(target_net ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>))</div><div class="line">target_net = target_net / (target_net_norm + eps)</div><div class="line">target_net[target_net != target_net] = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># Calculate the cosine similarity</span></div><div class="line">model_similarity = torch.mm(output_net, output_net.transpose(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">target_similarity = torch.mm(target_net, target_net.transpose(<span class="number">0</span>, <span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># Scale cosine similarity to 0..1</span></div><div class="line">model_similarity = (model_similarity + <span class="number">1.0</span>) / <span class="number">2.0</span></div><div class="line">target_similarity = (target_similarity + <span class="number">1.0</span>) / <span class="number">2.0</span></div><div class="line"></div><div class="line"><span class="comment"># Transform them into probabilities</span></div><div class="line">model_similarity = model_similarity / torch.sum(model_similarity, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)</div><div class="line">target_similarity = target_similarity / torch.sum(target_similarity, dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># Calculate the KL-divergence</span></div><div class="line">loss = torch.mean(target_similarity * torch.log((target_similarity + eps) / (model_similarity + eps)))</div><div class="line"></div><div class="line"><span class="keyword">return</span> loss</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(AB) - Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons, _AAAI2019_</strong></p><p>æœ¬æ–‡æ¢ç´¢çš„æ˜¯æƒé‡çš„åˆå§‹åŒ–æ–¹å¼ã€‚é€šè¿‡è®©Tå’ŒSä¸­activationså¤§äº0å€¼çš„æ•°é‡ç›¸ä¼¼ï¼Œæå‡ºäº†activation transfer lossã€‚è¿™ä¸ªlosså¯ä»¥ç›´æ¥è®¡ç®—å‡ºæ¥æ˜¯ä¸€ä¸ªå¸¸æ•°ä¸å¯å¯¼ï¼Œäºæ˜¯å°†å…¶è½¬åŒ–ä¸ºhinge lossï¼Œæœ€å°åŒ–activation transfer lossç›¸å½“äºä»¥teacherçš„$\rho(x)$ä½œä¸ºlabelï¼Œå­¦ä¹ ä¸€ä¸ªäºŒå€¼åˆ†ç±»å™¨ã€‚<br>$$<br>\begin{aligned} \mathcal{L}(\boldsymbol{I})=&amp; | \rho(\mathcal{T}(\boldsymbol{I})) \odot \sigma(\mu \mathbf{1}-\mathcal{S}(\boldsymbol{I})) \\ &amp;+(\mathbf{1}-\rho(\mathcal{T}(\boldsymbol{I}))) \odot \sigma(\mu \mathbf{1}+\mathcal{S}(\boldsymbol{I})) |_{2}^{2} \end{aligned}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">loss = ((source + self.margin) ** <span class="number">2</span> * ((source &gt; -self.margin) &amp; (target &lt;= <span class="number">0</span>)).float() +</div><div class="line">(source - self.margin) ** <span class="number">2</span> * ((source &lt;= self.margin) &amp; (target &gt; <span class="number">0</span>)).float())</div><div class="line">loss = torch.abs(loss).sum()</div></pre></td></tr></table></figure></li></ol><ol><li><p><strong>(FT) - Paraphrasing Complex Network: Network Compression via Factor Transfer, _NIPS2018_</strong></p><p>æå‡ºäº†ä¸¤ä¸ªå·ç§¯æ¨¡å—paraphraserå’Œtranslatorã€‚å‰è€…é€šè¿‡æ— ç›‘ç£è®­ç»ƒï¼ˆæœ€å°åŒ–input feature mapså’Œoutput feature maspï¼‰æ¥æå–teacher factorsï¼Œåè€…ç”¨äºæå–student factorsã€‚(è¿™ä¸ªæ„Ÿè§‰æ²¡å•¥æ„æ€å°±ä¸å»çœ‹ä»£ç äº†ï¼‰</p></li></ol><ol><li><p><strong>(FSP) - A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning, _CVPR2017_</strong></p><p>æœ¬æ–‡çš„æ–¹æ³•ä¹Ÿæ˜¯ç”¨äºåˆå§‹åŒ–Sæƒé‡çš„ã€‚çŸ¥è¯†ç”±ä¸¤ä¸ªä¸åŒå±‚çš„features mapsä¹‹é—´çš„inner productæ„æˆï¼Œæœ€å°åŒ–Tå’ŒSçš„FSPçŸ©é˜µã€‚</p></li></ol><ol><li><p><strong>(NST) - Like what you like: knowledge distill via neuron selectivity transfer</strong></p><p>æœ¬æ–‡æå–çš„çŸ¥è¯†æ˜¯neuron selectivityï¼Œé€šè¿‡æœ€å°åŒ–åˆ†å¸ƒä¹‹é—´çš„Maximum Mean Discrepancy(MMD) metric è¿›è¡Œè’¸é¦ã€‚</p></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;### &lt;/p&gt;
&lt;h1 id=&quot;è’¸é¦ä¹±ç‚–&quot;&gt;&lt;a href=&quot;#è’¸é¦ä¹±ç‚–&quot; class=&quot;headerlink&quot; title=&quot;è’¸é¦ä¹±ç‚–&quot;&gt;&lt;/a&gt;è’¸é¦ä¹±ç‚–&lt;/h1&gt;&lt;p&gt;å‚è€ƒ&lt;a href=&quot;https://github.com/HobbitLong/RepDistil
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multi-task Learning</title>
    <link href="http://yoursite.com/2019/07/26/Multi-task%20Learning/"/>
    <id>http://yoursite.com/2019/07/26/Multi-task Learning/</id>
    <published>2019-07-26T15:48:10.000Z</published>
    <updated>2019-12-10T17:00:50.702Z</updated>
    
    <content type="html"><![CDATA[<p>### </p><h1 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h1><p>å¤šä»»åŠ¡åŒæ—¶è¿›è¡Œä¸åˆ†ä¸»æ¬¡ï¼Œå¤šä¸ªç›¸å…³çš„ä»»åŠ¡æ”¾åœ¨ä¸€èµ·å­¦ä¹ ï¼Œä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†å…±äº«å’Œå…±åŒå­¦ä¹ ã€‚</p><ul><li><p>Cross-stitch Networks for Multi-task Learning </p><ul><li>æå‡ºä¸€ä¸ªâ€cross-stitchâ€å•å…ƒï¼Œå­¦ä¹ çš„<strong>activation</strong>ä¹‹é—´çš„çº¿æ€§æ˜ å°„ï¼ŒæœŸæœ›æ‰¾åˆ°æœ€ä¼˜çš„share / task-specificç‰¹å¾ç»„åˆã€‚</li><li>æ–¹æ³•ï¼šåŸºäºä¸€ä¸ªAlexNetï¼ˆone-task networkï¼‰ï¼Œç„¶ååœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šåˆ†åˆ«è¿›è¡Œfinetuneè·å¾—ç½‘ç»œAå’ŒBï¼Œå¼•å…¥corss-stitchingã€‚</li><li>æ•°æ®é›†å’Œä»»åŠ¡ï¼š<ul><li>Semantic segmentationï¼ˆSemSegï¼‰and Surface Normal Predictionï¼ˆSNï¼‰on NYU-v2 </li><li>object detection and attribute prediction on PASCAL VOC 2008</li></ul></li><li>å®éªŒï¼š<ul><li>åˆå§‹å€¼$\alpha$çš„å½±å“ï¼Œone-task / ensembleä¸¤ä¸ªç½‘ç»œ / split architecture / MTL-shared</li></ul></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803113347827.png" alt="image-20190803113347827"></p><blockquote><p>åªè€ƒè™‘æƒé‡ï¼Œæ²¡æœ‰è€ƒè™‘ç½‘ç»œç»“æ„çš„å½±å“ã€‚è€Œä¸”å¢åŠ äº†æ¨¡å‹å¤§å°ã€‚</p></blockquote></li><li><p><strong>DAN</strong>: Incremental Learning Through Deep Adaptation (ICLR2018)</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803113821256.png" alt="image-20190803113821256"></p></li><li><ul><li><p>å‡è®¾åŸºäºçš„æ˜¯ä¿æŒç½‘ç»œç»“æ„ä¸å˜ï¼Œåœ¨T1ä»»åŠ¡ä¸Šè®­ç»ƒå¥½çš„ç½‘ç»œNï¼Œé€šè¿‡æ”¹å˜ç½‘ç»œæƒé‡èƒ½å¤Ÿè¿ç§»åˆ°T2ä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼šåŸºäºVGG-Bï¼Œå¼•å…¥controller modulesï¼Œå°è£…äº†åŸå…ˆçš„å·ç§¯å±‚ï¼Œå¯¹åŸå§‹æƒé‡åšäº†ä¸€ä¸‹çº¿æ€§å˜æ¢ï¼Œæ¯ä¸ªä»»åŠ¡æœ‰ä¸€ä¸ªäºŒå€¼å˜é‡$\alpha$ï¼Œæ§åˆ¶é€‰æ‹©åŸå§‹æƒé‡oræ–°çš„æƒé‡ã€‚</p></li><li><p>æ•°æ®é›†å’Œä»»åŠ¡ï¼šCaltech-256, CIFAR-10, Daimler, GTSR, Omniglot, Plankton imagery data, Human Sketch dataset, SVHN</p></li><li><p>å®éªŒï¼š</p><ul><li>control-moduleä¸­Wçš„åˆå§‹åŒ–æ–¹å¼</li><li>base networkçš„é€‰æ‹©</li><li>Visual Decathlon Challengeï¼š</li></ul><blockquote><p>ä¸åŒäºcross-stitchçš„jointly learningï¼Œè€Œæ˜¯one-by-oneï¼Œåœ¨ä¸€ä¸ªç½‘ç»œçš„åŸºç¡€ä¸Šè®­ç»ƒå‡ºå¦å¤–ä¸€ä¸ªã€‚</p></blockquote></li></ul></li></ul><ul><li><p>Learning multiple visual domains with residual adaptersï¼ˆNIPS2017ï¼‰</p></li><li><p>Efficient parametrization of multi-domain deep neural networksï¼ˆCVPR2018ï¼‰</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190805213652297.png" alt="image-20190805213652297"></p></li></ul><ul><li><ul><li>å’ŒğŸ‘†ä¸€ç¯‡æ˜¯åŒä¸€ä¸ªä½œè€…ï¼Œä¸åŒçš„æ˜¯ä¸€ä¸ªä¸Šä¸€ç¯‡ç”¨ä¸²è¡Œçš„adapatersï¼Œè¿™ç¯‡ç”¨å¹¶è¡Œçš„ã€‚</li><li>æ–¹æ³•ï¼šåŸºäºResNetç»“æ„å¼•å…¥residual adaptersï¼Œå¼•å…¥è¾ƒå°‘çš„å‚æ•°å¯¹featureè¿›è¡Œäº†å˜æ¢ã€‚</li><li>å®éªŒï¼š<ul><li>adaptersçš„ä½ç½®ï¼ˆearly / mid / lateï¼‰</li><li>å’Œåœ¨å„è‡ªä»»åŠ¡ä¸Šè·å¾—finetuneç½‘ç»œè¿›è¡Œæ¯”è¾ƒ</li></ul></li></ul></li></ul><ul><li>LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING ï¼ˆICLR2018ï¼‰</li></ul><ul><li><p><strong>Piggyback</strong>: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</p><p><a href="https://github.com/arunmallya/piggyback" target="_blank" rel="external">https://github.com/arunmallya/piggyback</a></p><ul><li>one-by-one learning</li><li>æ¯ä¸ªå·ç§¯æ ¸å­¦ä¹ ä¸€ä¸ªmaskï¼ˆå‰ªæï¼‰ï¼Œmaskçš„å€¼ä¸º0å’Œ1ï¼ˆé‡åŒ–ï¼‰</li><li>æ•°æ®é›†ï¼šCUBS / Stanford Cars / WikiArt / Sketch</li><li>æ–¹æ³•ï¼šä¸æ”¹å˜pretrainedæ¨¡å‹çš„backboneï¼Œå­¦ä¹ binary maskï¼Œè®©å·ç§¯æ ¸ç¨€ç–ä»¥è¾¾åˆ°é€‚åº”æ–°æ•°æ®é›†çš„ç›®çš„ã€‚</li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190807104608124.png" alt="image-20190807104608124"></p></li><li><p><strong>MTAN</strong>: End-to-End Multi-Task Learning with Attention  (cvpr2019)</p><p><a href="https://github.com/lorenmt/mtan" target="_blank" rel="external">https://github.com/lorenmt/mtan</a></p></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803164905002.png" alt="image-20190803164905002"></p><ul><li><ul><li>jointly learningï¼Œå‡ ä¸ªä»»åŠ¡æ˜¯åŒæ—¶è®­çš„ã€‚</li><li>æœ‰ä¸€ä¸ªbackboneç½‘ç»œä½œä¸ºtask-sharedç½‘ç»œï¼ˆæœ¬æ–‡é‡‡ç”¨SegNetï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡æœ‰å¯¹åº”çš„attention moduleã€‚</li><li>æå‡ºäº†DWAï¼ŒåŠ¨æ€å¹³è¡¡lossç³»æ•°ã€‚</li></ul></li></ul><ul><li><p>Nerttailor (cvpr2019)</p><p><a href="https://github.com/pedro-morgado/nettailor" target="_blank" rel="external">https://github.com/pedro-morgado/nettailor</a></p><p><img src="https://github.com/pedro-morgado/nettailor/raw/master/docs/figs/teaser_row.png" alt="img"></p><ul><li><p>one-by-one learningï¼Œå…ˆåœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒå®Œè¿ç§»åˆ°å¦ä¸€ä¸ªä»»åŠ¡ä¸Šã€‚</p></li><li><p>universarial networkæ˜¯åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šfine-tuneä¸€ä¸ªpre-trainedç½‘ç»œï¼ŒåŒºåˆ«åœ¨äºä¸ä»…æ”¹å˜æƒé‡ï¼Œè¿˜æ”¹å˜äº†ç½‘ç»œçš„ç»“æ„ã€‚soft Attention+NASã€‚</p></li><li><p>åŠ¨æ€æ”¹å˜ç½‘ç»œç»“æ„ï¼Œbackboneæ˜¯åœ¨ä¸€ä¸ªåŸŸä¸Šè®­å¥½çš„ResNetï¼Œé€šè¿‡æœç´¢è¾…åŠ©å•å…ƒã€‚</p></li></ul></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190804144750941.png" alt="image-20190804144750941"></p><ul><li><p>Efficient parametrization of multi-domain deep neural networks</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190805171712077.png" alt="image-20190805171712077"></p><ul><li>ç½‘ç»œç»“æ„ç›¸åŒçš„ç½‘ç»œï¼Œç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¸åŒï¼ˆè¾“å…¥ä¸åŒï¼‰</li><li>é€šè¿‡æƒå€¼å…±äº«çš„æ–¹å¼è¿›è¡Œä¸¤ä¸ªåŸŸæ¨¡å‹çš„å‹ç¼©ã€‚é¦–å…ˆæ€¼äº†GrOWL(Group weighted order lasso)çš„æ–¹æ³•ã€‚</li><li><strong>ä¸‰ä¸ªæ•°æ®é›†ï¼š</strong>SUN-RGBD Datasetï¼ˆRGBå›¾åƒå’Œæ·±åº¦å›¾ï¼‰ï¼ŒUCF-101 Datasetï¼ˆYoutube videosï¼‰ï¼ŒHMDB-51 Datasetï¼ˆvideoï¼‰</li><li><strong>ä¸¤ä¸ªä»»åŠ¡ï¼š</strong>RGB-D Scene Classificationï¼šAlex-Netï¼ŒAction Recognition Tasksï¼šVGG-16</li></ul></li></ul><p>weight sharing?</p><p>åŸºäºç‰¹å¾</p><ul><li>ä¸åŒä»»åŠ¡ç‰¹å¾è½¬æ¢ï¼Œå­¦ä¹ ç‰¹å¾ä¹‹é—´çš„çº¿æ€§ç»„åˆï¼ˆCross-stitchã€Deep Adaptation</li><li><p>ç‰¹å¾é€‰æ‹©ï¼Œç¨€ç–ï¼ˆGroup sparsityï¼‰</p></li><li><p>åˆ†è§£ï¼Œä½ç§©åˆ†è§£</p></li></ul><p>åŸºäºä»»åŠ¡èšç±»</p><ul><li>åŠ æƒæœ€è¿‘é‚»åˆ†ç±»å™¨ã€‚é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ï¼Œé€šè¿‡è°ƒæ•´æƒé‡å®ç°æœ€å°åŒ–ç±»å†…è·ç¦»ï¼Œæœ€å¤§åŒ–ç±»é—´è·ç¦»ã€‚æ¯ä¸ªä»»åŠ¡ä¹‹é—´æ„å»ºè½¬åŒ–çŸ©é˜µAï¼Œå…¶ä¸­$a_{ij}$è¡¨ç¤ºä½¿ç”¨ä»»åŠ¡$T_j$çš„åˆ†ç±»å™¨å¯¹ä»»åŠ¡$T_i$æ ·æœ¬è¿›è¡Œåˆ†ç±»çš„ç¹èŠ±ç²¾åº¦ã€‚åŸºäºçŸ©é˜µAï¼Œå°†$m$ä¸ªä»»åŠ¡èšæˆ$r$ä¸ªç°‡ã€‚ä¸€ä¸ªç°‡é‡Œå„ä¸ªä»»åŠ¡çš„æ ·æœ¬å…±äº«ï¼Œæ¯ä¸ªç°‡è®­ç»ƒå‡ºä¸€ä¸ªå…±åŒçš„åŠ æƒæœ€è¿‘é‚»åˆ†ç±»å™¨ã€‚</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;### &lt;/p&gt;
&lt;h1 id=&quot;Multi-task-Learning&quot;&gt;&lt;a href=&quot;#Multi-task-Learning&quot; class=&quot;headerlink&quot; title=&quot;Multi-task Learning&quot;&gt;&lt;/a&gt;Multi-task Learni
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>The Reparameterization Trick</title>
    <link href="http://yoursite.com/2019/07/26/The%20Reparameterization%20Trick/"/>
    <id>http://yoursite.com/2019/07/26/The Reparameterization Trick/</id>
    <published>2019-07-26T15:48:10.000Z</published>
    <updated>2019-12-10T17:01:35.663Z</updated>
    
    <content type="html"><![CDATA[<p>###</p><h3 id="The-Reparameterization-Trick"><a href="#The-Reparameterization-Trick" class="headerlink" title="The Reparameterization Trick"></a>The Reparameterization Trick</h3><p>æœ€è¿‘çœ‹äº†å¾ˆå¤šè´å¶æ–¯çš„æ–¹æ³•ï¼Œé‡å‚åŒ–æŠ€å·§æ˜¯å…¶ä¸­çš„å…³é”®ï¼Œä¹‹å‰åœ¨gumbel softmaxä¸€å—æœ‰ç¨å¾®äº†è§£äº†ä¸€ä¸‹VAEï¼Œä½†æ„Ÿè§‰è¿˜æ˜¯ç†è§£å¾—ä¸æ·±ï¼ŒåŸºäºåšå®¢<a href="[http://gregorygundersen.com/blog/2018/04/29/reparameterization/">The Reparameterization Trick</a>è®°å½•ä¸€ä¸‹ã€‚</p><p>å…ˆä¸‹å®šä¹‰ï¼š</p><blockquote><p><strong>Kingma:</strong> This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t $q_{\phi}(\textbf{z} \mid \textbf{x})$ such that the Monte Carlo estimate of the expectation is differentiable w.r.t.  $\phi$</p></blockquote><p>å…ˆéªŒçŸ¥è¯†ï¼šæœŸæœ›çš„æ¢¯åº¦ç­‰äºæ¢¯åº¦çš„æœŸæœ›<br>$$<br>\begin{aligned} \nabla_{\theta} \mathbb{E}_{p(z)}\left[f_{\theta}(z)\right] &amp;=\nabla_{\theta}\left[\int_{z} p(z) f_{\theta}(z) d z\right] \\ &amp;=\int_{z} p(z)\left[\nabla_{\theta} f_{\theta}(z)\right] d z \\ &amp;=\mathbb{E}_{p(z)}\left[\nabla_{\theta} f_{\theta}(z)\right] \end{aligned}<br>$$<br>é—®é¢˜å¼•å…¥ï¼šæˆ‘ä»¬å¸Œæœ›è§£å†³çš„é—®é¢˜æ˜¯å½“$p$ä¹Ÿæœ‰å‚æ•°$\theta$ï¼š<br>$$<br>\begin{aligned} \nabla_{\theta} \mathbb{E}_{p_{\theta}(z)}\left[f_{\theta}(z)\right] &amp;=\nabla_{\theta}\left[\int_{z} p_{\theta}(z) f_{\theta}(z) d z\right] \\ &amp;=\int_{z} \nabla_{\theta}\left[p_{\theta}(z) f_{\theta}(z)\right] d z \\ &amp;=\int_{z} f_{\theta}(z) \nabla_{\theta} p_{\theta}(z) d z+\int_{z} p_{\theta}(z) \nabla_{\theta} f_{\theta}(z) d z \\ &amp;=\underbrace{\int_{z} f_{\theta}(z) \nabla_{\theta} p_{\theta}(z) d z}_{\text {What about this? }}  + \mathbb{E}_{p_{\theta}(z)}\left[\nabla_{\theta} f_{\theta}(z)\right]\end{aligned}<br>$$<br>Monte Carloå¹¶ä¸èƒ½ç›´æ¥ä¼°è®¡ç¬¬ä¸€é¡¹ï¼Œäºæ˜¯å¼•å…¥é‡å‚åŒ–æŠ€å·§ï¼š<br>$$<br>\begin{align}<br>\boldsymbol{\epsilon} &amp;\sim p(\boldsymbol{\epsilon})<br>\\ \\<br>\textbf{z} &amp;= g_{\boldsymbol{\theta}}(\boldsymbol{\epsilon}, \textbf{x})<br>\\ \\<br>\mathbb{E}_{p_{\boldsymbol{\theta}}(\textbf{z})}[f(\textbf{z}^{(i)})] &amp;= \mathbb{E}_{p(\boldsymbol{\epsilon})} [f(g_{\theta}(\boldsymbol{\epsilon}, \textbf{x}^{(i)}))]<br>\\ \\<br>\nabla_{\theta} \mathbb{E}_{p_{\boldsymbol{\theta}}(\textbf{z})}[f(\textbf{z}^{(i)})] &amp;= \nabla_{\theta} \mathbb{E}_{p(\boldsymbol{\epsilon})} [f(g_{\boldsymbol{\theta}}(\boldsymbol{\epsilon}, \textbf{x}^{(i)}))] &amp;&amp; \text{(1)}<br>\\<br>&amp;= \mathbb{E}_{p(\boldsymbol{\epsilon})} [f(\nabla_{\boldsymbol{\theta}} g_{\boldsymbol{\theta}}(\boldsymbol{\epsilon}, \textbf{x}^{(i)}))] &amp;&amp; \text{(2)}<br>\\<br>&amp;\approx \frac{1}{L} \sum_{l=1}^{L} f(\nabla_{\boldsymbol{\theta}} g_{\boldsymbol{\theta}}(\epsilon^{(l)}, \textbf{x}^{(i)})) &amp;&amp; \text{(3)}<br>\end{align}<br>$$<br>é‡å‚åŒ–æŠ€å·§è®©ï¼Œå› æ­¤æœŸæœ›çš„æ¢¯åº¦(1)å¯ä»¥å†™æˆæ¢¯åº¦çš„æœŸæœ›(2)ï¼Œç„¶ååˆå¯ä»¥ç”¨Monte Carlo(3)è¿›è¡Œä¼°è®¡ã€‚</p><p>åœ¨ELBOä¸­ï¼Œæˆ‘ä»¬æœ‰ï¼š<br>$$<br>\begin{align}<br>\text{ELBO}(\boldsymbol{\theta}, \boldsymbol{\phi})<br>&amp;= \Big[\mathbb{E}_{q_{\boldsymbol{\phi}}(\textbf{z})}[\log p_{\boldsymbol{\theta}}(\textbf{x}, \textbf{z}) - \log q_{\boldsymbol{\phi}}(\textbf{z} \mid \textbf{x})] \Big] &amp;&amp; \text{(4)} \\<br>&amp;\downarrow \\<br>\nabla_{\theta, \phi} \text{ELBO}(\boldsymbol{\theta}, \boldsymbol{\phi}) &amp;= \underbrace{\nabla_{\theta, \phi} \Big[\mathbb{E}_{q_{\boldsymbol{\phi}}(\textbf{z})}[\log p_{\boldsymbol{\theta}}(\textbf{x}, \textbf{z}) - \log q_{\boldsymbol{\phi}}(\textbf{z} \mid \textbf{x})] \Big]}_{\text{Gradient w.r.t. $\phi$ over expectation w.r.t. $\phi$}}<br>\end{align}<br>$$<br>å¯ä»¥å°†ELBOåˆ†è§£æˆä¸¤éƒ¨åˆ†çš„lossï¼š<br>$$<br>\mathcal{L}^B = - \text{KL}[\overbrace{q_{\phi}(\textbf{z} \mid \textbf{x}^{(i)})}^{\text{Encoder}} \lVert \overbrace{p_{\theta}(\textbf{z})}^{\text{Fixed}}] + \frac{1}{L} \sum_{l=1}^{L} \log \overbrace{p_{\boldsymbol{\theta}}(\textbf{x}^{(i)} \mid \textbf{z}^{(l)})}^{\text{Decoder}}<br>$$</p><p>$$<br>\nabla_{\theta, \phi} \mathcal{L}^B = - \nabla_{\theta, \phi} \overbrace{\Bigg[\text{KL}[q_{\phi}(\textbf{z} \mid \textbf{x}^{(i)}) \lVert p_{\theta}(\textbf{z})]\Bigg]}^{\text{Analytically compute this}} + \nabla_{\theta, \phi} \overbrace{\Bigg[ \frac{1}{L} \sum_{l=1}^{L} \Big( \log p_{\boldsymbol{\theta}}(\textbf{x}^{(i)} \mid \textbf{z}^{(l)}) \Big)\Bigg]}^{\text{Monte Carlo estimate this}}<br>$$</p><p>å‡è®¾å…ˆéªŒå’Œä¼°è®¡åéªŒéƒ½æœä»é«˜æ–¯åˆ†å¸ƒï¼š<br>$$<br>\begin{align}<br>\boldsymbol{\mu}_x, \boldsymbol{\sigma}_x &amp;= M(\textbf{x}), \Sigma(\textbf{x}) &amp;&amp; \text{Push $\textbf{x}$ through encoder}<br>\\ \\<br>\boldsymbol{\epsilon} &amp;\sim \mathcal{N}(0, 1) &amp;&amp; \text{Sample noise}<br>\\ \\<br>\textbf{z} &amp;= \boldsymbol{\epsilon} \boldsymbol{\sigma}_x + \boldsymbol{\mu}_x  &amp;&amp; \text{Reparameterize}<br>\\ \\<br>\textbf{x}_r &amp;= p_{\boldsymbol{\theta}}(\textbf{x} \mid \textbf{z}) &amp;&amp; \text{Push $\textbf{z}$ through decoder}<br>\\ \\<br>\text{recon. loss} &amp;= \text{MSE}(\textbf{x}, \textbf{x}_r) &amp;&amp; \text{Compute reconstruction loss}<br>\\ \\<br>\text{var. loss} &amp;= -\text{KL}[\mathcal{N}(\boldsymbol{\mu}_x, \boldsymbol{\sigma}_x) \lVert \mathcal{N}(0, I)] &amp;&amp; \text{Compute variational loss}<br>\\ \\<br>\text{L} &amp;= \text{recon. loss} + \text{var. loss} &amp;&amp; \text{Combine losses}<br>\end{align}<br>$$</p><p><img src="http://gregorygundersen.com/image/reparam/vae.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;###&lt;/p&gt;
&lt;h3 id=&quot;The-Reparameterization-Trick&quot;&gt;&lt;a href=&quot;#The-Reparameterization-Trick&quot; class=&quot;headerlink&quot; title=&quot;The Reparameterization Tr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.10</title>
    <link href="http://yoursite.com/2019/07/26/weekly-paper-10/"/>
    <id>http://yoursite.com/2019/07/26/weekly-paper-10/</id>
    <published>2019-07-26T15:48:10.000Z</published>
    <updated>2019-07-31T13:30:32.838Z</updated>
    
    <content type="html"><![CDATA[<p>### </p><h3 id="1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks"><a href="#1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks" class="headerlink" title="1ï¸âƒ£ Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks"></a>1ï¸âƒ£ Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks</h3><p><strong>æœ¬æ–‡æ˜¯åŸºäºå¹²æ‰°é¡¹æ–¹æ³•çš„å¯è§£é‡Š</strong>ã€‚å¯è§£é‡Šçš„åŒºåŸŸ$\mathbf{e}^__{C_T}$å¯ä»¥åˆ†ä¸º<strong>æœ€å°ä¿ç•™çš„åŒºåŸŸ</strong>å’Œ_*æœ€å°ç§»é™¤åŒºåŸŸ__ï¼Œå‰è€…æ„å‘³ç€è¿™äº›åŒºåŸŸæ˜¯ä¿è¯æ¨¡å‹åˆ†ç±»æ­£ç¡®çš„éƒ¨åˆ†ï¼Œåè€…æ„å‘³ç€è¿™äº›åŒºåŸŸå¿…é¡»ç§»é™¤ä»¥æ”¹å˜æ¨¡å‹è¾“å‡ºã€‚</p><p>æ·»åŠ å¹²æ‰°é¡¹çš„å›¾åƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š$\mathbf{e}=\mathbf{m} \cdot \mathbf{x}+(1-\mathbf{m}) \cdot \mathbf{r}$ï¼Œé€šè¿‡è®­ç»ƒä½¿maskç¨€ç–ã€‚</p><ul><li><p>ä¿ç•™è§£é‡Šï¼š<br>$$<br>\begin{aligned} \mathbf{e}_{c_{T}}^{_} &amp;=\mathbf{m}_{c_{T}}^{_} \cdot \mathbf{x} \\ \mathbf{m}_{c_{T}}^{*} &amp;=\underset{\mathbf{m}_{c_{T}}}{\arg \min }\left\{\varphi\left(y_{x}^{c_{T}}, y_{e}^{c_{T}}\right)+\lambda \cdot\left|\mathbf{m}_{c_{T}}\right|_{1}\right\} \end{aligned}<br>$$<br>å›¾åƒä¸­çš„eåŒºåŸŸï¼Œä¿è¯æ¨¡å‹çš„åˆ†ç±»æ­£ç¡®ã€‚</p></li><li><p>ç§»é™¤è§£é‡Šï¼š<br>$$<br>\begin{aligned} \mathbf{e}_{c_{T}}^{_} &amp;=\mathbf{m}_{c_{T}}^{_} \cdot \mathbf{x} \\ \mathbf{m}_{c_{T}}^{*} &amp;=\underset{\mathbf{m}_{e_{T}}}{\arg \max }\left\{\varphi\left(y_{x}^{c_{T}}, y_{e}^{c_{T}}\right)+\lambda \cdot\left|\mathbf{m}_{c_{T}}\right|_{1}\right\} \end{aligned}<br>$$<br>å›¾åƒä¸­çš„eåŒºåŸŸï¼Œä½¿å¾—æ¨¡å‹åˆ†ç±»é”™è¯¯ã€‚</p></li></ul><h3 id="2ï¸âƒ£-THE-DEEP-WEIGHT-PRIOR"><a href="#2ï¸âƒ£-THE-DEEP-WEIGHT-PRIOR" class="headerlink" title="2ï¸âƒ£ THE DEEP WEIGHT PRIOR"></a>2ï¸âƒ£ THE DEEP WEIGHT PRIOR</h3><p>ã€ICLR2019ã€‘</p><p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯èƒ½å¤Ÿé€šè¿‡æŸä¸ªæ¦‚ç‡åˆ†å¸ƒç”Ÿæˆç½‘ç»œçš„æƒé‡ã€‚å¯ä»¥çœ‹ä½œæ˜¯å¢å¼ºç½‘ç»œåˆå§‹åŒ–çš„ä¸€ç§æ–¹æ³•ã€‚ä»¥å‰è´å¶æ–¯ç¥ç»ç½‘ç»œéƒ½æ˜¯éœ€è¦å¯¹å‚æ•°çš„å…ˆéªŒåˆ†å¸ƒ$p(W)$è¿›è¡Œå‡è®¾ï¼Œé€šå¸¸æ˜¯log-uniformã€‚<br>$$<br>\mathcal{L}(\theta)=\sum_{i=1}^{N} \mathbb{E}_{q_{\theta}(W)} \log p\left(y_{i} | x_{i}, W\right)-D_{\mathrm{KL}}\left(q_{\theta}(W) | p(W)\right) \rightarrow \max _{\theta}<br>$$<br>VAEæ˜¯é€šè¿‡éšå˜é‡$z_i$ä¼°è®¡åéªŒæ¦‚ç‡åˆ†å¸ƒ$q(z_i|x_i)$çš„æ–¹æ³•ã€‚å…¶ä¸­$x_i$æ˜¯ç”Ÿæˆå›¾åƒã€‚<br>$$<br>\mathcal{L}(\theta, \phi)=\sum_{i=1}^{N} \mathbb{E}_{q_{\theta}\left(z_{i} | x_{i}\right)} \log p_{\phi}\left(x_{i} | z_{i}\right)-D_{\mathrm{KL}}\left(q_{\theta}\left(z_{i} | x_{i}\right) | p\left(z_{i}\right)\right) \rightarrow \max _{\theta, \phi}<br>$$<br>æœ¬æ–‡çš„å‡è®¾æ˜¯åŸºäºé¢„è®­ç»ƒçš„ç½‘ç»œå‚æ•°$\hat{p}_{l}(w)$ï¼Œå‚æ•°çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒä¸ºï¼š<br>$$<br>\hat{p}_{l}(w)=\int p\left(w | z ; \phi_{l}\right) p_{l}(z) d z<br>$$<br>å¼•å…¥auxiliary lower bound KL:<br>$$<br>\begin{array}{l}{D_{\mathrm{KL}}(q(W) | \hat{p}(W))=\sum_{l, i, j} D_{\mathrm{KL}}\left(q\left(w_{i j}^{l} | \theta_{i j}^{l}\right) | \hat{p}_{l}\left(w_{i j}^{l}\right)\right) \leq \sum_{l, i, j}\left(-H\left(q\left(w_{i j}^{l} | \theta_{i j}^{l}\right)\right)+\right.} \\ {+\mathbb{E}_{q\left(w_{i j}^{l} | \theta_{i j}^{l}\right)}\left[D_{\mathrm{KL}}\left(r\left(z | w_{i j}^{l} ; \psi_{l}\right) | p_{l}(z)\right)-\mathbb{E}_{r\left(z | w_{i j}^{l} ; \psi_{l}\right)} \log p\left(w_{i j}^{l} | z ; \phi_{l}\right)\right] )=D_{\mathrm{KL}}^{b o u n d}}\end{array}<br>$$<br>è¿™æ ·å°±å’ŒVAEå¯¹ä¸Šï¼Œç”¨VAEçš„encoderä¼°è®¡å‚æ•°çš„å…ˆéªŒæ¦‚ç‡ï¼ˆæ–‡ç« å‡è®¾éšå˜é‡$z_i$æœä»N(0,1)ï¼‰ï¼Œç„¶åç”¨VAEä¼°è®¡ç½‘ç»œå‚æ•°çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åä»è¯¥åˆ†å¸ƒä¸­ç”Ÿæˆç½‘ç»œçš„å‚æ•°ã€‚</p><p><img src="https://i.loli.net/2019/07/27/5d3c4f29da39269692.png" alt=""></p><h3 id="3ï¸âƒ£-DSC-Dense-Sparse-Convolution-for-Vectorized-Inference-of-Convolutional-Neural-Networks"><a href="#3ï¸âƒ£-DSC-Dense-Sparse-Convolution-for-Vectorized-Inference-of-Convolutional-Neural-Networks" class="headerlink" title="3ï¸âƒ£ DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks"></a>3ï¸âƒ£ DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks</h3><p>ã€CVPR2019ã€‘</p><p>æœ¬æ–‡æ˜¯ä»å¾ˆç°å®çš„è§’åº¦åšå‹ç¼©ï¼ŒåŸºäºå…·ä½“çš„Winograd convolutionçš„å‹ç¼©æ–¹å¼ã€‚</p><p><strong>è®¡ç®—å•å…ƒå‘é‡åŒ–</strong>ï¼šä»ç°å®è§’åº¦æ¥çœ‹ï¼Œä»å†…å­˜ä¸­è¯»å–8-bitæ•´å½¢å’Œ32-bitæµ®ç‚¹å‹çš„èƒ½è€—ç›¸åŒï¼Œä»i7 CPUè¯»å–æ•°æ®64-bitsæ•°æ®å’ŒAltera Arria 10åº¦å»32-bitæ•°æ®çš„èƒ½è€—ç›¸åŒã€‚åªè¯»å–åŒbitæ•°æ®(align data)å¡«å……å¯„å­˜å™¨åªéœ€è¦ä¸€æ¬¡æ“ä½œï¼ŒåŒæ—¶è¯»å–ä¸åŒbitæ•°æ®(unaligned data)åˆ™éœ€è¦ä¸¤æ¬¡æ“ä½œã€‚é€šå¸¸CPUæ•°æ®æµç¼“å­˜å—çš„å¤§å°æ˜¯64bytes (64*8bits)ï¼Œæ„å‘³ç€64x8-bitæ•´å½¢å’Œ 16x32-bitçš„æ•°æ®å¯ä»¥å¹³è¡Œå¡«å……å¯„å­˜å™¨ã€‚</p><p><strong>WInograd convolution</strong>ï¼šåŸºäºWinogradå·ç§¯æ˜¯ç”¨æ›´å¤šçš„åŠ æ³•æ¥å‡å°‘æƒ©ç½šæ“ä½œï¼Œ2D Winograd Convolution F(2x2, 3x3)çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p><p><img src="https://i.loli.net/2019/07/28/5d3cfdb637a8236799.png" alt=""></p><p><strong>Dense-Sparse Convolution</strong></p><p><img src="https://i.loli.net/2019/07/28/5d3cfe28d812326769.png" alt=""></p><p>é’ˆå¯¹Sparse Convolutionï¼ŒæŠŠå·ç§¯æ ¸ç”¨CSRæ ¼å¼å­˜æ”¾ï¼Œè¿›è¡Œdirect sparse convolutionã€‚</p><p>é’ˆå¯¹Sparse-Dense Convolutionï¼Œä½œè€…å…ˆé€šè¿‡ä¸€ä¸ªthresholdåˆ¤æ–­çš„å·ç§¯æ ¸çš„ç¨€ç–ç¨‹åº¦ï¼Œç„¶åç”¨ä¸‹é¢çš„å…¬å¼è¿›è¡Œè®¡ç®—ï¼š</p><p><img src="https://i.loli.net/2019/07/28/5d3cff84895ca39428.png" alt=""></p><h3 id="4ï¸âƒ£-Efficient-Neural-Network-Compression"><a href="#4ï¸âƒ£-Efficient-Neural-Network-Compression" class="headerlink" title="4ï¸âƒ£ Efficient Neural Network Compression"></a>4ï¸âƒ£ Efficient Neural Network Compression</h3><p>ã€CVPR2019ã€‘</p><p>æœ¬æ–‡çš„å‹ç¼©æ–¹æ³•æ˜¯é’ˆå¯¹å·ç§¯æ ¸è¿›è¡Œä½ç§©åˆ†è§£ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°é’ˆå¯¹æ•´ä¸ªç½‘ç»œçš„æœ€ä¼˜rankä»¥è¿›è¡Œå‹ç¼©ï¼ˆç›¸å½“äºé€šé“ratioï¼‰</p><p><img src="https://github.com/Hyeji-Kim/ENC/raw/master/fig/overall2.png" alt="i"></p><p>ä¸¤ç§Layer-wise Accuracy Metrics</p><ul><li><p>PCA energy-basedï¼š$y_{p, l}\left(r_{l}\right)$<br>$$<br>y_{p, l}\left(r_{l}\right)=\frac{\sigma_{l}^{\prime}\left(r_{l}\right)-\sigma_{l}^{\prime}(1)}{\sigma_{l}^{\prime}\left(r_{l}^{\max }\right)-\sigma_{l}^{\prime}(1)}<br>$$<br>å…¶ä¸­ç¬¬$l$å±‚çš„ç§©æ˜¯$r_l$ï¼Œ$\sigma_l(d)$æ˜¯ç»è¿‡åˆ†è§£çš„ç¬¬$d$ä¸ªå¯¹è§’å€¼ï¼Œ$\sigma_{l}^{\prime}\left(r_{l}\right)=\sum^{r_l}_{d=1}\sigma_l(d)$è¡¨ç¤ºå·ç§¯æ ¸åˆ†è§£åå¯¹åº”ç§©çš„å…ƒç´ ä¹‹å’Œï¼Œè¿›è¡Œå½’ä¸€åŒ–ã€‚</p></li><li><p>Measurement-based Metricï¼š$y_{m, l}\left(r_{l}\right)$</p><p>åªæ”¹å˜ç½‘ç»œå±‚$l$çš„ç§©ï¼Œæ‰€è·å¾—çš„æ•´ä½“ç²¾åº¦ã€‚ç”¨VBMFè¿›è¡Œç§©çš„é‡‡æ ·ã€‚</p></li></ul><p>å‡è®¾æ¯å±‚çš„metricæ˜¯ç‹¬ç«‹çš„ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºç½‘ç»œæ•´ä½“çš„accuracy metricï¼š<br>$$<br>\mathrm{P}(A ; R)=\prod_{l=1}^{L} \mathrm{P}\left(a_{l} ; r_{l}\right)<br>$$<br>ä¸‰ç§Overall accuracy metricï¼š</p><ul><li>Measurement-basedï¼š$A_{m}(R)=\prod_{l=1}^{L} y_{m, l}\left(r_{l}\right)$</li><li><p>PCA-basedï¼š$A_{p}(R)=\prod_{l=1}^{L} y_{p, l}\left(r_{l}\right)$</p></li><li><p>combied metricï¼š$A_{c}(R)=\left\{A_{p}(R) \times \frac{C(R)}{C_{\text {orig}}}\right\}+A_{m}(R)$</p></li></ul><p><strong>ENC-Map</strong>ï¼šåˆ©ç”¨Accuracy-Complexityçš„æ˜ å°„æ¥é€‰æ‹©æ¯å±‚çš„ranké…ç½®ã€‚æ–‡ç« è®¤ä¸ºè®©ç½‘ç»œæ¯å±‚çš„å…·æœ‰ç›¸åŒçš„ç²¾åº¦æŸå¤±ä¸å…·æœ‰ç›¸åŒå‹ç¼©ç‡ç›¸æ¯”ï¼Œæ˜¯æ›´åˆç†çš„å‹ç¼©ç­–ç•¥ã€‚å› æ­¤å‡è®¾åœ¨VBMFç”Ÿæˆçš„rankä¸‹ï¼Œæ¯å±‚çš„metricéƒ½ç›¸åŒï¼š<br>$$<br>R_{e}=R | y_{i, l}\left(r_{l}\right)=y_{i, k}\left(r_{k}\right)<br>$$<br>ç„¶åæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡º$R_e$çš„å¤æ‚åº¦$C(R)=\sum_{l=1}^{L} C_{l}\left(r_{l}\right)=\sum_{l=1}^{L} c_{l} r_{l}$</p><p>äºæ˜¯æœ‰äº†complexityå’Œaccuracyçš„æ˜ å°„ï¼š$f_{C-A}ï¼š \mathbb{R} \rightarrow \mathbb{R}$ï¼Œè¿›ä¸€æ­¥å¾—åˆ°complexityå’Œaccuracyåˆ°rankçš„æ˜ å°„ï¼š$f_{C-R}ï¼š\mathbb{R} \rightarrow \mathbb{R}^L$ã€‚</p><p><strong>ENC-Model/Inf</strong>ï¼šå°†æ‰©å±•ENC-Mapè‡³rankçš„ç»„åˆé—®é¢˜ï¼Œéœ€è¦æœç´¢åˆé€‚çš„rankï¼Œé€šè¿‡1. åˆ©ç”¨å·²çŸ¥å¤æ‚åº¦æ¥é™åˆ¶ 2. æŠŠé•¿å¾—å·®ä¸å¤šçš„çš„å·ç§¯æ ¸çš„rankåˆ†åˆ°ä¸€ç»„ã€‚</p><h3 id="5ï¸âƒ£-ECC-Platform-Independent-Energy-Constrained-Deep-Neural-Network-Compression-via-a-Bilinear-Regression-Model"><a href="#5ï¸âƒ£-ECC-Platform-Independent-Energy-Constrained-Deep-Neural-Network-Compression-via-a-Bilinear-Regression-Model" class="headerlink" title="5ï¸âƒ£ ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model"></a>5ï¸âƒ£ ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model</h3><p>æœ¬æ–‡ç”¨é™åˆ¶èƒ½è€—æ¥è¿›è¡Œæ¨¡å‹å‹ç¼©ï¼Œæå‡ºäº†ç”¨ä¸€ä¸ªåŒçº¿æ€§å›å½’æ¨¡å‹æ¥ä¼°è®¡targetç¡¬ä»¶å¹³å°çš„èƒ½è€—ã€‚</p><p>ç›®æ ‡ç”¨ä¸‹é¢çš„å…¬å¼è¡¨ç¤ºï¼š<br>$$<br>\begin{array}{cl}{\min _{\mathcal{W}, \mathbf{s}}} &amp; {\ell(\mathcal{W})} \ \\{\text { s.t. }} &amp; {\phi\left(\mathbf{w}^{(u)}\right) \leq s^{(u)}, \quad u \in \mathcal{U}}\\ \ {} &amp; {\mathcal{E}(\mathbf{s}) \leq E_{\text { budget }}}\end{array}<br>$$<br>è§£å†³ä¸Šé¢é—®é¢˜éœ€è¦è§£å†³ç¨€ç–ç‡åˆ°èƒ½é‡çš„æ˜ å°„æ¨¡å‹$\mathcal{E}(\mathbf{s})$ã€‚ç”¨data-drivençš„æ–¹æ³•æ¥è®­ç»ƒè¿™ä¸ªè¿‘ä¼¼æ¨¡å‹$\hat{\mathcal{E}}$ï¼š<br>$$<br>\hat{\mathcal{E}}=\underset{f \in \mathcal{F}}{\arg \min } \mathbb{E}_{\mathbf{s}}\left[(f(\mathbf{s})-\mathcal{E}(\mathbf{s}))^{2}\right]<br>$$<br>ç”¨åŒçº¿æ€§æ¨¡å‹æ¥ä¼°è®¡ç½‘ç»œæ•´ä½“èƒ½è€—ï¼š<br>$$<br>\mathcal{F} :=\{f(\mathbf{s})=a_{0}+\sum_{j=1}^{|\mathcal{U}|} a_{j} s_{j} s_{j+1} : a_{0}, a_{1}, \ldots, a_{|\mathcal{U}|} \in \mathbb{R}_{+} \}<br>$$<br>ECCæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼ŒOnlineå’ŒOfflineéƒ¨åˆ†ã€‚åœ¨Offlineéƒ¨åˆ†å»ºç«‹è¿‘ä¼¼èƒ½é‡ä¼°è®¡æ¨¡å‹$\hat{\mathcal{E}}$</p><p><img src="https://i.loli.net/2019/07/28/5d3d42fd271e241820.png" alt=""></p><p>Onlineéƒ¨åˆ†åŸºäºèƒ½é‡æ¨¡å‹è¿›è¡Œå‹ç¼©å’ŒADMMè¿›è¡Œå‹ç¼©ã€‚å°†ç›®æ ‡è½¬ä¸ºminmaxä¼˜åŒ–é—®é¢˜ï¼š<br>$$<br>\min _{\mathcal{W}, \mathbf{s}} \max _{z \geq 0, \mathbf{y} \geq \mathbf{0}} \mathcal{L}(\mathcal{W}, \mathbf{s}, \mathbf{y}, z)<br>$$<br>å¼•å…¥å¯¹å¶å˜é‡$y$å’Œ$z$ç”¨äºé™åˆ¶ç¨€ç–ç‡ï¼Œå¼•å…¥zç”¨äºé™åˆ¶èƒ½é‡ï¼š<br>$$<br>\mathcal{L}(\mathcal{W}, \mathbf{s}, \mathbf{y}, z) \quad :=\ell(\mathcal{W})+\mathcal{L}_{1}(\mathcal{W}, \mathbf{s}, \mathbf{y})+\mathcal{L}_{2}(\mathbf{s}, z)<br>$$<br>å…¶ä¸­$\mathcal{L}_{1}(\mathcal{W}, \mathbf{s}, \mathbf{y}) \quad :=\quad \frac{\rho_{1}}{2} \sum_{u}\left[\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right]_{+}^{2}+\sum_{u} y^{(u)}\left(\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right), \mathcal{L}_{2}(\mathbf{s}, z)$</p><p>$\mathcal{L}_{2}(\mathbf{s}, z) :=\frac{\rho_{2}}{2}\left[\hat{\mathcal{E}}(\mathbf{s})-E_{\mathrm{budget}}\right]_{+}^{2}+z\left(\hat{\mathcal{E}}(\mathbf{s})-E_{\text { budget }}\right)$</p><p>ç®—æ³•é€šè¿‡è¿­ä»£æ›´æ–°å‚æ•°æ¥è¾¾åˆ°æœ€ç»ˆç›®æ ‡</p><ul><li>Update $W$ï¼šç”¨Proximal Adam</li><li><p>Update $s$ï¼š$\mathbf{s}^{t+1}=\mathbf{s}^{t}-\beta\left(\nabla_{\mathbf{s}} \mathcal{L}_{1}\left(\mathcal{W}, \mathbf{s}^{t}, \mathbf{y}\right)+\nabla_{\mathbf{s}} \mathcal{L}_{2}\left(\mathbf{s}^{t}, z\right)\right)$</p></li><li><p>Update å¯¹å¶å˜é‡ï¼š$\begin{aligned} y^{(u)^{t+1}} &amp;=\left[y^{(u)^{t}}+\rho_{1}\left(\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right)\right]_{+} \\ z^{t+1} &amp;=\left[z^{t}+\rho_{2}\left(\hat{\mathcal{E}}(\mathbf{s})-E_{\mathrm{budget}}\right)\right]_{+} \end{aligned}$</p></li></ul><h3 id="NETTAILOR-Tuning-the-architecture-not-just-the-weights"><a href="#NETTAILOR-Tuning-the-architecture-not-just-the-weights" class="headerlink" title="NETTAILOR: Tuning the architecture, not just the weights"></a>NETTAILOR: Tuning the architecture, not just the weights</h3><p>è¿™ç¯‡æ–‡ç« å¾ˆæœ‰æ„æ€ï¼Œä¸æ­¢fintuneç½‘ç»œæƒé‡ï¼Œè¿˜FTç½‘ç»œç»“æ„ã€‚ç›®å‰å¤§éƒ¨åˆ†ç½‘ç»œä½¿ç”¨çš„æ˜¯ç›¸åŒçš„backboneï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ç½‘ç»œç»“æ„æœ¬èº«çš„å½±å“ã€‚å¯èƒ½å°ä¸€äº›çš„ç½‘ç»œåœ¨ç›®æ ‡æ•°æ®é›†ä¸Šå°±è¶³å¤Ÿäº†ã€‚æœ¬æ–‡å°†pre-trainedçš„backboneç½‘ç»œç»“æ„ä¸ºuniversal blocksï¼ŒåŠ ä¸Šä¸€äº›task-specificç½‘ç»œæ¥ç”Ÿæˆæ–°çš„ç½‘ç»œã€‚é€šè¿‡soft-attentionæœºåˆ¶å’Œç½‘ç»œçš„å¤æ‚åº¦é™åˆ¶æ¥å­¦ä¹ æ–°çš„ç½‘ç»œç»“æ„å’Œæƒé‡ã€‚</p><p>ä¸€äº›ç›¸å…³å·¥ä½œåŒ…æ‹¬è¿ç§»å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ï¼ˆå¢å¼ºä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–æ€§ï¼‰ï¼Œè¿ç§»å­¦ä¹ å‡è®¾å›¾åƒæ¥è‡ªä¸åŒçš„åŸŸï¼ŒMTLå‡è®¾æ‰€æœ‰ä»»åŠ¡æ˜¯å¤„äºåŒåŸŸçš„ã€‚Domain adaptationè§£å†³ä¸¤ä¸ªä¸åŒåŸŸæ•°æ®é›†çš„ä»»åŠ¡ã€‚Cascaded classifiers &amp; Adaptive inference graphsèƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ã€‚ä½†æ˜¯é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¦è®­ç»ƒä¸åŒçš„ç½‘ç»œï¼ŒNETTAILORé€šè¿‡é‡ç”¨universal blocksï¼Œåªè®­ç»ƒtaskç›¸å…³çš„blockæ¥è§£å†³multi-domain transfer learning problemsã€‚</p><p>ç®—æ³•ä¸»è¦åˆ†æˆä»¥ä¸‹å››æ­¥ï¼š1. åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šç”¨pre-trainedç½‘ç»œè®­ç»ƒä¸€ä¸ªteacher networkã€‚2. å®šä¹‰åŒ…æ‹¬proxy layersçš„å­¦ç”Ÿç½‘ç»œã€‚3. åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šåªè®­ç»ƒtask-specificå‚æ•°ï¼ŒåŒæ—¶åŠ ä¸Šå¤æ‚åº¦é™åˆ¶ã€‚4. ç²¾ç®€ç½‘ç»œç»“æ„åè¿›è¡Œfinetuneã€‚</p><p><img src="https://github.com/pedro-morgado/nettailor/raw/master/docs/figs/teaser_row.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;### &lt;/p&gt;
&lt;h3 id=&quot;1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Interpretable-an
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>è¿›åŒ–ç­–ç•¥</title>
    <link href="http://yoursite.com/2019/07/20/evolution-strategies/"/>
    <id>http://yoursite.com/2019/07/20/evolution-strategies/</id>
    <published>2019-07-19T23:44:06.000Z</published>
    <updated>2019-07-20T00:25:32.893Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" target="_blank" rel="external">è¿™ç¯‡åšå®¢</a></p><p>è¿›åŒ–ç­–ç•¥æ˜¯ä¸€ç§é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œé˜²æ­¢å‚æ•°é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚è¿›åŒ–ç­–ç•¥å¯ä»¥çœ‹ä½œä¸€ç§æä¾›ä¸€ç³»åˆ—å€™é€‰è§£å†³æ–¹æ¡ˆæ¥è¯„ä¼°ä¸€ä¸ªé—®é¢˜çš„ç®—æ³•ã€‚è¯„ä¼°ç»“æœåŸºäºä¸€ä¸ªç›®æ ‡å‡½æ•°(objective function)ï¼Œä¸€ä¸ªè§£å†³æ–¹æ¡ˆè¿”å›ä¸€ä¸ªé€‚åº”åº¦(fitness value)ï¼ŒåŸºäºå½“å‰è§£å†³æ–¹æ¡ˆçš„é€‚åº”åº¦ï¼Œå†ç”Ÿæˆä¸‹ä¸€é›†åˆçš„å€™é€‰è€…ã€‚æœ€ç®€å•çš„ä¼ªä»£ç å¦‚ä¸‹ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">solver = EvolutionStrategy()</div><div class="line"></div><div class="line">while True:</div><div class="line"></div><div class="line">  # è¯·æ±‚ESç”Ÿæˆå€™é€‰è€…</div><div class="line">  solutions = solver.ask()</div><div class="line"></div><div class="line">  # åˆå§‹åŒ–é€‚åº”åº¦</div><div class="line">  fitness_list = np.zeros(solver.popsize)</div><div class="line"></div><div class="line">  # è¯„ä¼°å€™é€‰è€…ï¼Œç”Ÿæˆä¸å…¶å¯¹åº”çš„é€‚åº”åº¦</div><div class="line">  for i in range(solver.popsize):</div><div class="line">    fitness_list[i] = evaluate(solutions[i])</div><div class="line"></div><div class="line">  # è¿”å›é€‚åº”åº¦ç»“æœç»™ES</div><div class="line">  solver.tell(fitness_list)</div><div class="line"></div><div class="line">  # ä»ESä¸­è·å–æœ€ä¼˜å‚æ•°å’Œæœ€ä¼˜é€‚åº”åº¦</div><div class="line">  best_solution, best_fitness = solver.result()</div><div class="line"></div><div class="line">  if best_fitness &gt; MY_REQUIRED_FITNESS:</div><div class="line">    break</div></pre></td></tr></table></figure><p>ä»¥ç®€å•çš„2Dé—®é¢˜ä¸ºä¾‹ï¼Œå‚æ•°ç”±$\mu=\left(\mu_{x}, \mu_{y}\right)$å’Œ$\sigma=\left(\sigma_{x}, \sigma_{y}\right)$ç»„æˆSimple ESå’ŒSimple GAéƒ½æ˜¯å›ºå®š$\sigma$ä¸å˜ï¼Œé€šè¿‡è¿›åŒ–ç®—æ³•å­¦ä¹ $\mu$ï¼Œç”±æ­¤CMA-ESç®—æ³•è¯ç”Ÿã€‚å®ƒæ˜¯ä¸€ç§ä¸åŸºäºæ¢¯åº¦çš„ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æ‰€æœ‰å‚æ•°ç©ºé—´çš„åæ–¹å·®çŸ©é˜µï¼Œåœ¨æ¯æ¬¡è¿­ä»£æ—¶ä»å¤šå…ƒæ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·è§£å†³æ–¹æ¡ˆã€‚</p><p>ä¸Šé¢æåˆ°çš„ä¸€äº›ç®—æ³•åªä¿ç•™äº†æœ€ä¼˜è§£ï¼Œå¿½ç•¥äº†å…¶ä»–è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤ä¹Ÿå¯èƒ½å¿½ç•¥å¤§éƒ¨åˆ†å¯¹ç”Ÿæˆä¸‹ä¸€ä»£æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç»“åˆRLç®—æ³•ï¼Œæœ‰äººæå‡ºäº†REINFORCE-ESä»¥åŠNESï¼Œéµå¾ªçš„åŸåˆ™æ˜¯ä¸è®ºå¥½åç»¼åˆæ‰€æœ‰å€™é€‰è€…ä»¥ä¼°è®¡æ¢¯åº¦ï¼Œå¾€æ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯<strong>æœ€å¤§åŒ–é‡‡æ ·å€™é€‰è€…çš„é€‚åº”åº¦æœŸæœ›å€¼</strong>ï¼š<br>$$<br>J(\theta)=E_{\theta}[F(z)]=\int F(z) \pi(z, \theta) d z<br>$$<br><a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">NES</a>æä¾›äº†æ¢¯åº¦çš„æ¨å¯¼ï¼Œåˆ©ç”¨log-likelihood trickå’Œè’™ç‰¹å¡æ´›é‡‡æ ·å¯ä»¥å¾—åˆ°ï¼š<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} F\left(z^{i}\right) \nabla_{\theta} \log \pi\left(z^{i}, \theta\right)<br>$$<br><a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a>ä¸ºç‰¹æ®Šçš„æ¡ˆä¾‹ï¼Œå½“$\pi(z, \theta)$æ˜¯ä¸€ä¸ªfactored multi-variate normal distributionï¼Œå³æ¯ä¸ªå‚æ•°æœä»ä¸€ä¸ªä¸€å…ƒæ­£æ€åˆ†å¸ƒ$z_{j} \sim N\left(\mu_{j}, \sigma_{j}\right)$ï¼Œç»™å‡ºäº†ä¸€ä¸ªæ¢¯åº¦çš„å°é—­è§£ï¼š<br>$$<br>\begin{array}{l}{\nabla_{\mu_{j}} \log N\left(z^{i}, \mu, \sigma\right)=\frac{z_{j}^{i}-\mu_{j}}{\sigma_{j}^{2}}} \\ {\nabla_{\sigma_{j}} \log N\left(z^{i}, \mu, \sigma\right)=\frac{\left(z_{j}^{i}-\mu_{j}\right)^{2}-\sigma_{j}^{2}}{\sigma_{j}^{3}}}\end{array}<br>$$<br>è¿™äº›è®ºæ–‡æå‡ºäº†ä¸€äº›tricksï¼Œæ¯”å¦‚PEPGä¸­çš„antithetic samplingï¼ŒNESä¸­åˆ©ç”¨Fisher Informationæ›´æ–°æ¢¯åº¦ç­‰ç­‰ã€‚</p><p>åœ¨OenAIçš„<a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">è®ºæ–‡</a>é‡Œï¼Œå®ƒä»¬å›ºå®š$\sigma$ï¼Œåªæ›´æ–°$\mu$ï¼Œä¸»è¦æ˜¯è§£å†³æ‰§è¡Œå±‚é¢çš„å¹¶è¡Œè¿ç®—é—®é¢˜ã€‚</p><p>é€šå¸¸è¿›åŒ–ç­–ç•¥éƒ½ä¼šé‡‡æ ·ä¸€ä¸ªç§°ä¸ºFitness Shapingçš„trickï¼ŒæŠŠç§ç¾¤é€‚åº”åº¦è½¬åŒ–ä¸ºç§ç¾¤å†…éƒ¨çš„ç›¸å¯¹å€¼ï¼Œå³rankä¸€ä¸‹fitnessï¼Œä¿è¯è¯„ä¼°æŒ‡æ ‡çš„ä¸å˜æ€§ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;è¿™ç¯‡åšå®¢&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;è¿›åŒ–ç­–ç•¥æ˜¯ä¸€ç§é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œé˜²æ­¢å‚æ•°
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.09</title>
    <link href="http://yoursite.com/2019/07/08/weekly-paper-09/"/>
    <id>http://yoursite.com/2019/07/08/weekly-paper-09/</id>
    <published>2019-07-08T01:07:11.000Z</published>
    <updated>2019-07-17T13:39:48.882Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Natural-Evolution-Strategies"><a href="#1ï¸âƒ£-Natural-Evolution-Strategies" class="headerlink" title="1ï¸âƒ£ Natural Evolution Strategies"></a>1ï¸âƒ£ Natural Evolution Strategies</h3><p>NESæ˜¯ä¸€ç§åˆ©ç”¨æœç´¢æ¢¯åº¦(search gradients)æ›´æ–°æœç´¢åˆ†å¸ƒå‚æ•°(parameters of the search distribution)çš„é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œä¸ç»å…¸æ–¹æ³•ï¼ˆEDAsï¼‰åˆ©ç”¨æœ€å¤§ä¼¼ç„¶æ‹Ÿåˆé‡‡æ ·åˆ†å¸ƒçš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºçš„æ›´æ–°ç­–ç•¥æ˜¯æ²¿ç€æ›´é«˜æœŸæœ›é€‚åº”åº¦çš„æ–¹å‘ã€‚</p><p>æœ€å¤§åŒ–æ‰€é‡‡æ ·æ ·æœ¬çš„é€‚åº”åº¦çš„æœŸæœ›å€¼ã€‚æ ¸å¿ƒæ€æƒ³å°±æ˜¯åœ¨æ¯æ¬¡æ–°çš„ç§ç¾¤é‡Œæ›´æ–°meanå’Œstdï¼Œä»æ–°çš„ç»“æœä¸­ç»§ç»­æ›´æ–°</p><h4 id="0-Search-Gradients"><a href="#0-Search-Gradients" class="headerlink" title="0/ Search Gradients"></a>0/ Search Gradients</h4><p>å‡è®¾æˆ‘ä»¬ä»åˆ†å¸ƒ$\pi(\mathbf z | \theta)$ä¸­é‡‡æ ·$\mathbf z$ï¼Œç”¨$f(\mathbf z)$è¡¨ç¤ºè¯¥é‡‡æ ·çš„é€‚åº”åº¦ã€‚åœ¨è¯¥æœç´¢åˆ†å¸ƒä¸‹çš„æœŸæœ›é€‚åº”åº¦ä¸ºï¼š<br>$$<br>J(\theta)=\mathbb{E}_{\theta}[f(\mathbf{z})]=\int f(\mathbf{z}) \pi(\mathbf{z} | \theta) d \mathbf{z}<br>$$<br>åˆ©ç”¨ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/" target="_blank" rel="external">log-likelihood trick</a>ï¼š<br>$$<br>\begin{aligned} \nabla_{\theta} J(\theta) &amp;=\nabla_{\theta} \int f(\mathbf{z}) \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\int f(\mathbf{z}) \nabla_{\theta} \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\int f(\mathbf{z}) \nabla_{\theta} \pi(\mathbf{z} | \theta) \frac{\pi(\mathbf{z} | \theta)}{\pi(\mathbf{z} | \theta)} d \mathbf{z} \\ &amp;=\int\left[f(\mathbf{z}) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)\right] \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\mathbb{E}_{\theta}\left[f(\mathbf{z}) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)\right] \end{aligned}<br>$$<br>åˆ©ç”¨ç§ç¾¤å¤§å°$\lambda$å¯¹æœç´¢æ¢¯åº¦è¿›è¡Œè¿‘ä¼¼ä¼°è®¡ï¼š<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{\lambda} \sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \nabla_{\theta} \log \pi\left(\mathbf{z}_{k} | \theta\right)<br>$$<br>$\nabla_{\theta} J(\theta)$æä¾›äº†æ¢¯åº¦ä¸Šå‡çš„æ–¹å‘ï¼Œæœ€ç›´æ¥çš„æ–¹æ³•å‚æ•°æ›´æ–°æ–¹æ³•ï¼š$\theta \leftarrow \theta+\eta \nabla_{\theta} J(\theta)$</p><blockquote><p>æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æœŸæœ›å€¼ï¼Œå› æ­¤æ˜¯æ¢¯åº¦ä¸Šå‡ã€‚</p></blockquote><p>è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºæ ‡å‡†æœç´¢æ¢¯åº¦ç®—æ³•ï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d229d3f6943c17863.png" alt=""></p><p>ä»¥å¤šå…ƒæ­£æ€åˆ†å¸ƒä¸ºä¾‹ï¼Œ$\theta=\langle\boldsymbol{\mu}, \boldsymbol{\Sigma}\rangle$ä¸ºéœ€è¦å­¦ä¹ çš„åˆ†å¸ƒå‚æ•°ã€‚æˆ‘ä»¬è¿˜éœ€è¦æ»¡è¶³$\mathbf{A}^{\top} \mathbf{A}=\mathbf{\Sigma}$çš„åæ–¹å·®çŸ©é˜µçš„å¹³æ–¹æ ¹çŸ©é˜µ$\mathbf{A} \in \mathbb{R}^{d \times d}$ï¼Œä½¿å¾—$\mathbf{z}=\boldsymbol{\mu}+\mathbf{A}^{\top} \mathbf{s}$å°†æ ‡å‡†æ­£æ€åˆ†å¸ƒ$\mathbf{s} \sim \mathcal{N}(0, \mathbb{I})$è½¬åŒ–ä¸ºç§ç¾¤ä¸ªä½“$\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})$ï¼Œéœ€è¦å¯¹$\boldsymbol{\mu}, \mathbf{\Sigma}$æ±‚å¯¼ï¼Œæ›´æ–°ç®—æ³•ä¸ºï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d229f6ae0f3061548.png" alt=""></p><h4 id="1-Limitations"><a href="#1-Limitations" class="headerlink" title="1/ Limitations"></a>1/ Limitations</h4><p>é’ˆå¯¹æ™®é€šæœç´¢æ¢¯åº¦ç®—æ³•çš„å±€é™ï¼Œæœ¬æ–‡æå‡ºçš„è§£å†³åŠæ³•å¯ä»¥ç”¨ä¸‹è¡¨æ€»ç»“ï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d2299a8b796f92761.png" alt=""></p><p>è¿™äº›é—®é¢˜å’Œæ–¹æ³•è®©æˆ‘ä»¬æ¥ä¸€ä¸ªä¸ªæ”»ç ´ã€‚</p><p><strong>a. Natural gradient</strong></p><p>ä»¤$\lambda=1, d=1$ï¼Œ$\mu \leftarrow \mu+\eta \frac{z-\mu}{\sigma^{2}},\quad \sigma \leftarrow \sigma+\eta \frac{(z-\mu)^{2}-\sigma^{2}}{\sigma^{3}}$</p><p>ç”±äº$\Delta \mu \propto \frac{1}{\sigma}, \Delta \sigma \propto \frac{1}{\sigma}$ï¼Œ$\sigma$åŒæ—¶æ§åˆ¶äº†$\mu$å’Œ$\sigma$çš„æ›´æ–°ï¼Œé€ æˆå‚æ•°çš„æ›´æ–°ä¸æ˜¯å°ºåº¦ä¸å˜(scale-invariant)çš„ï¼šæˆ‘ä»¬å‡å°$\sigma$è®©$\mu$æ¥è¿‘æœ€ä¼˜è§£çš„åŒæ—¶ä¹Ÿå¢å¤§äº†$\sigma$ï¼Œä½¿å…¶åœ¨å‚æ•°æ›´æ–°çš„æ—¶å€™å†æ¬¡è¿œç¦»æœ€ä¼˜è§£ã€‚</p><p>è‡ªç„¶æ¢¯åº¦çš„æå‡ºä¾¿æ˜¯ä¸ºäº†è§£å†³å°ºåº¦ä¸å˜çš„é—®é¢˜ã€‚åŸå§‹æ¢¯åº¦æµ‹é‡çš„æ˜¯å‚æ•°åˆ†å¸ƒçš„æ¬§æ‹‰è·ç¦»ã€‚è‡ªç„¶æ¢¯åº¦åˆ©ç”¨çš„æ˜¯å‚æ•°åˆ†å¸ƒçš„KLæ•£åº¦ã€‚<br>$$<br>\begin{aligned} \max _{\delta \theta} J(\theta+\delta \theta) &amp; \approx J(\theta)+\delta \theta^{\top} \nabla_{\theta} J \\ \text {s.t. } D(\theta+\delta \theta | \theta) &amp;=\varepsilon, \end{aligned}<br>$$<br>$J(\theta)$ä»ç„¶æ˜¯æœŸæœ›é€‚åº”åº¦ï¼Œ$\varepsilon$æ˜¯ä¸€ä¸ªå¾ˆå°çš„å¢é‡ï¼Œé€šè¿‡äºŒé˜¶æ³°å‹’å±•å¼€$\lim \delta \theta \rightarrow 0$ï¼Œ$D(\theta+\delta \theta | \theta)=\frac{1}{2} \delta \theta^{\top} \mathbf{F}(\theta) \delta \theta$ã€‚å…¶ä¸­<br>$$<br>\begin{aligned} \mathbf{F} &amp;=\int \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)^{\top} d \mathbf{z} \\ &amp;=\mathbb{E}\left[\nabla_{\theta} \log \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)^{\top}\right] \end{aligned}<br>$$<br>è¡¨ç¤ºä¸ºè´¹é›ªä¿¡æ¯çŸ©é˜µ(Fisher information matrix)ã€‚è‹¥$ \mathbf{F}$å¯é€†ï¼Œåˆ™è‡ªç„¶æ¢¯åº¦è¡¨ç¤ºä¸ºï¼š<br>$$<br>\widetilde{\nabla}_{\theta} J=\mathbf{F}^{-1} \nabla_{\theta} J(\theta)<br>$$</p><blockquote><p>æ¨å¯¼å¯ä»¥å‚è€ƒ<a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" target="_blank" rel="external">blog</a></p></blockquote><p>ç„¶åæˆ‘ä»¬å°±å¯ä»¥å†™å‡ºæ ‡å‡†è‡ªç„¶æ¼”åŒ–ç­–ç•¥ç®—æ³•ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190708145437180.png" alt="image-20190708145437180"></p><p>ä¸ç®—æ³•1çš„åŒºåˆ«å°±åœ¨äºï¼Œæ›´æ–°çš„æ—¶å€™æ¢¯åº¦ä¹˜ä»¥äº†ä¸€ä¸ªè´¹é›ªä¿¡æ¯çŸ©é˜µçš„é€†ã€‚</p><p><strong>b. Fitness shaping</strong></p><p>ç”¨æ•ˆç”¨å‡½æ•°(utility)æ›¿anä»£é€‚åº”åº¦ï¼š<br>$$<br>\nabla_{\theta} J(\theta)=\sum_{k=1}^{\lambda} u_{k} \nabla_{\theta} \log \pi\left(\mathbf{z}_{k} | \theta\right)<br>$$<br>æœ¬æ–‡å®šä¹‰çš„æ•ˆç”¨å‡½æ•°ä¸ºï¼š<br>$$<br>u_{k}=\frac{\max \left(0, \log \left(\frac{\lambda}{2}+1\right)-\log (k)\right)}{\sum_{j=1}^{\lambda} \max \left(0, \log \left(\frac{\lambda}{2}+1\right)-\log (j)\right)}-\frac{1}{\lambda}<br>$$<br><strong>c. Adapation Sampling</strong></p><p>ç”¨è´¨é‡å‡½æ•°åˆ¤æ–­ï¼Œå½“å‰é‡‡æ ·$\mathbf {zâ€™}$å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºä¹‹å‰çš„é‡‡æ ·$\mathbf {z}$ï¼Œæ‰è¿›è¡Œå‚æ•°æ›´æ–°ã€‚ä¸å•çº¯çš„æœ€å¤§åŒ–é€‚åº”åº¦å‡½æ•°æœ¬èº«ä¸åŒï¼Œé€‚åº”é‡‡æ ·æœ€å¤§åŒ–çš„æ˜¯è¿›æ­¥çš„æ­¥ä¼(pace of progress)ã€‚ä»¥æœ€é‡è¦çš„å‚æ•°å­¦ä¹ ç‡$\eta_{\sigma}$ä¸ºä¾‹ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190708163252563.png" alt="image-20190708163252563"></p><p>é¦–å…ˆåˆ©ç”¨$1.5\eta_{\sigma, t}$å’Œ$\theta_{t-1}$è®¡ç®—å‡ºå‡æƒ³å‚æ•°$\theta â€˜$ï¼Œè®¡ç®—æ¯ä¸ªä¸ªä½“çš„æƒé‡$w_k â€˜$ï¼Œç”¨Mann-Whitney testæ£€éªŒä¸¤ä¸ªè´¨é‡å‡½æ•°ï¼Œå½“å¤§äºé˜ˆå€¼$\rho=\frac{1}{2}-\frac{1}{3(d+1)}$æ—¶ï¼Œå¢åŠ å­¦ä¹ ç‡ï¼Œå¦åˆ™è®©å®ƒé è¿‘åˆå§‹çš„å­¦ä¹ ç‡ã€‚</p><p><strong>d. Exponential parameterization &amp; Natural coordinate system</strong></p><p>NESä¸ç›´æ¥æ›´æ–°åæ–¹å·®$\mathbf \Sigma$ï¼Œè€ƒè™‘ä»¥å®ƒçš„å¹³æ–¹æ ¹$\mathbf A$ä¸ºå‚æ•°çš„æ­£æ€åˆ†å¸ƒï¼Œåˆ©ç”¨è‡ªç„¶æ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚ä¸ºäº†é¿å…ä¼°è®¡FisherçŸ©é˜µï¼ŒxNESä½¿ç”¨äº†å±€éƒ¨åæ ‡ç³»å’ŒæŒ‡æ•°æ˜ å°„ã€‚å±€éƒ¨åæ ‡ç³»ä¸‹çš„è‡ªç„¶æ¢¯åº¦ä¸ºï¼š<br>$$<br>\begin{aligned} \nabla_{\boldsymbol{\delta}} J &amp;=\sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \cdot \mathbf{s}_{k} \\ \nabla_{\mathbf{M}} J &amp;=\sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \cdot\left(\mathbf{s}_{k} \mathbf{s}_{k}^{\top}-\mathbb{I}\right) \end{aligned}<br>$$<br>å…¶ä¸­$\mathbf {s}_k$æ˜¯å±€éƒ¨åæ ‡ç³»ä¸‹ç¬¬$k$ä¸ªæœ€ä¼˜æ ·æœ¬ï¼Œ$\mathbf {z}_k$æ˜¯ç›®æ ‡2åæ ‡ç³»ä¸‹çš„ç›¸åŒæ ·æœ¬ã€‚æŠŠåæ–¹å·®å› å­$\mathbf{A}$åˆ†è§£ä¸ºæ­¥é•¿$\sigma &gt;0$å’Œæ»¡è¶³$\operatorname{det}(\mathbf{B})=1$çš„å½’ä¸€åŒ–åæ–¹å·®å› å­$\mathbf{B}$ã€‚è¿™ç§åˆ†è§£ä½¿å¾—ä¸¤ä¸ªæ­£äº¤æˆåˆ†å¯ä»¥æœ‰å„è‡ªçš„å­¦ä¹ ç‡ï¼ˆ$\eta_{\sigma}$, $\eta_{\mathbf {B}}$ï¼‰ï¼Œå¯¹æ­¥é•¿$\sigma$å’Œ$B$çš„æ›´æ–°åšäº†æŒ‡æ•°æ˜ å°„ã€‚</p><p><img src="https://i.loli.net/2019/07/08/5d230b687c16942920.png" alt=""></p><p>æœ¬æ–‡è¿˜æäº†separable NES(SNES)ï¼Œä½¿ç”¨åˆ†ç¦»çš„æœç´¢åˆ†å¸ƒå‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚æœ¬æ–‡çš„å®ç°æ–¹å¼æ˜¯é™åˆ¶$\mathbf {A}$ä¸ºå¯é€†çš„å¯¹è§’å˜æ¢çŸ©é˜µã€‚</p><p><img src="https://i.loli.net/2019/07/08/5d230b390673572266.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Natural-Evolution-Strategies&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Natural-Evolution-Strategies&quot; class=&quot;headerlink&quot; title=&quot;1ï¸âƒ£ Natural Evolution Strate
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ‰‹æ’•PyTorchçš„Batch Normalization</title>
    <link href="http://yoursite.com/2019/07/02/batch-normalization/"/>
    <id>http://yoursite.com/2019/07/02/batch-normalization/</id>
    <published>2019-07-02T00:35:20.000Z</published>
    <updated>2019-07-05T09:03:38.738Z</updated>
    
    <content type="html"><![CDATA[<p>BNæ˜¯é˜²ç½‘ç»œè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ï¼Œç»†å¾®çš„å·®åˆ«å¯èƒ½å¯¹è¾“å‡ºæ•ˆæœæœ‰å¾ˆå¤§å½±å“ã€‚å› æ­¤éœ€è¦ç†è§£ä¸€ä¸‹PyTorchä¸­BNçš„å…·ä½“å®ç°ã€‚PyTorchçš„æºç ç”¨Cå®ç°çš„torch.batchnormã€‚å¯ä»¥<a href="https://github.com/marvis/pytorch-yolo2/blob/master/layers/batchnorm/src/batchnorm.c" target="_blank" rel="external">yolo2</a>é‡Œæ‰¾åˆ°å…·ä½“å®ç°ã€‚</p><h3 id="è®¡ç®—è¿‡ç¨‹"><a href="#è®¡ç®—è¿‡ç¨‹" class="headerlink" title="è®¡ç®—è¿‡ç¨‹"></a>è®¡ç®—è¿‡ç¨‹</h3><p>$Input : \mathcal{B}=\left\{x_{1}, \cdots, x_{m}\right\}$ è¡¨ç¤ºbatch_sizeä¸º$m$çš„è¾“å…¥æ•°æ®ã€‚</p><p>$Output:  \gamma ,  \beta$   PyTorchä¸­ä¸ºweightså’Œbiasã€‚</p><p><strong>æ›´æ–°è¿‡ç¨‹ï¼š</strong></p><p>$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i}$</p><p>$\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}$</p><p>$\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}$</p><p>$y_{i} \leftarrow \gamma \hat{x}_{i}+\beta \equiv \mathrm{B} \mathrm{N}_{\gamma, \beta}\left(x_{i}\right)$</p><p>é¦–å…ˆé€šè¿‡ä»£ç æµ‹è¯•ä¸€ä¸‹å…·ä½“æ›´æ–°è¿‡ç¨‹çš„å®ç°ã€‚</p><p><strong>æµ‹è¯•è¾“å‡ºï¼š</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">x = torch.range(<span class="number">0</span>, <span class="number">35</span>).reshape(<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># initilization</span></div><div class="line">bn = nn.BatchNorm2d(<span class="number">3</span>)</div><div class="line">bn.weight.data.fill_(<span class="number">1</span>)</div><div class="line">bn.bias.data.zero_()</div><div class="line">mean, new_mean = torch.zeros([<span class="number">3</span>]), torch.zeros([<span class="number">3</span>])</div><div class="line">var, new_var = torch.ones([<span class="number">3</span>]), torch.zeros([<span class="number">3</span>])</div><div class="line"></div><div class="line"><span class="comment"># compute mean</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_mean[i] = x[:, i, :, :].mean()</div><div class="line"></div><div class="line"><span class="comment"># compute variance</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_var[i] = torch.pow((x[:, i, :, :] - new_mean[i]), <span class="number">2</span>).mean()</div><div class="line">  <span class="comment"># ç­‰åŒäº new_var[i] = x[:, i, :, :].var(False) </span></div><div class="line">  <span class="comment"># è®¡ç®—æ–¹å·®æ—¶ä¸ä½¿ç”¨è´å¡å°”æ ¡æ­£</span></div><div class="line">  </div><div class="line">normalized_x = torch.zeros_like(x)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  normalized_x[:, i, :, :] = (x[:, i, :, :] - new_mean[i]) / torch.sqrt(new_var[i] + bn.eps)</div><div class="line">  </div><div class="line">print(bn_x)</div><div class="line">print(normalized_x)</div></pre></td></tr></table></figure><p><strong>æµ‹è¯•runing_meanå’Œrunning_varï¼š</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># moving average</span></div><div class="line">running_mean = bn.momentum * new_mean + (<span class="number">1</span> - bn.momentum) * mean</div><div class="line">running_var = bn.momentum * new_var + (<span class="number">1</span> - bn.momentum) * var</div><div class="line"></div><div class="line">print(bn.running_mean, bn.running_var)</div><div class="line">print(running_mean, running_var)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">1.3500</span>, <span class="number">1.7500</span>, <span class="number">2.1500</span>] [<span class="number">11.5091</span>, <span class="number">11.5091</span>, <span class="number">11.5091</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">1.3500</span>, <span class="number">1.7500</span>, <span class="number">2.1500</span>] [<span class="number">10.6250</span>, <span class="number">10.6250</span>, <span class="number">10.6250</span>]</div></pre></td></tr></table></figure><p>running_meanå¯¹ä¸Šäº†ï¼Œå¯æ˜¯running_varå´ä¸å¯¹ã€‚ä»”ç»†çœ‹äº†ä¸€ä¸‹æºä»£ç åœ¨å‡½æ•°<code>variance_cpu</code>ä¸­æ³¨é‡Šæ‰äº†ä¸€å¥<code>float scale = 1./(batch * spatial - 1)</code>ï¼Œè¿™å¥å°±å¾ˆå…³é”®äº†ã€‚ä¿®æ”¹ä¸€ä¸‹ä¸Šé¢varçš„è®¡ç®—æ–¹å¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scale = x.size(<span class="number">0</span>) * x.size(<span class="number">2</span>) * x.size(<span class="number">3</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_var[i] = x[:, i, :, :].var(<span class="keyword">False</span>) </div><div class="line">  </div><div class="line">new_var = new_var * scale / (scale - <span class="number">1</span>)</div><div class="line">running_var = bn.momentum * new_var + (<span class="number">1</span> - bn.momentum)*var</div><div class="line">print(running_var)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">11.5091</span>, <span class="number">11.5091</span>, <span class="number">11.5091</span>]</div></pre></td></tr></table></figure><p>è¿™å›æ–¹å·®ä¹ŸğŸ‰‘ï¸äº†ã€‚è¯´æ˜å½’ä¸€åŒ–ä¸­çš„æ–¹å·®ä½¿ç”¨çš„æ˜¯æ­£å¸¸è®¡ç®—å‡ºçš„æ–¹å·®ï¼Œè€Œrunning_varçš„æ–¹å·®åœ¨scaleä¸Šåšäº†-1çš„å¤„ç†ã€‚</p><blockquote><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œrunning_meanå’Œrunning_varçš„å˜åŒ–ï¼Œä¸optimizeræ— å…³ï¼Œä¸æ˜¯æ‰§è¡Œstepä»¥åæ‰æ›´æ–°å€¼ï¼Œè€Œæ˜¯æ¯æ¬¡åšå‰å‘å€¼éƒ½ä¼šæ”¹å˜ã€‚</p></blockquote><p><strong>å®Œæ•´ä»£ç </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mean</span><span class="params">(x, running_mean, mom=<span class="number">0.1</span>)</span>:</span></div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    new_mean = torch.zeros([nc])</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        new_mean[i] = x[:, i, :, :].mean()</div><div class="line">    running_mean = mom * new_mean + (<span class="number">1</span> - mom) * running_mean</div><div class="line">    <span class="keyword">return</span> new_mean, running_mean</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_var</span><span class="params">(x, running_var, mom=<span class="number">0.1</span>)</span>:</span></div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    scale = x.size(<span class="number">0</span>) * x.size(<span class="number">2</span>) * x.size(<span class="number">3</span>)</div><div class="line">    new_var = torch.zeros([nc])</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        new_var[i] = x[:, i, :, :].var(<span class="keyword">False</span>)</div><div class="line">    temp_var = new_var * scale / (scale - <span class="number">1</span>)</div><div class="line">    running_var = mom * temp_var + (<span class="number">1</span> - mom) * running_var</div><div class="line">    <span class="keyword">return</span> new_var, running_var</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm_x</span><span class="params">(x, mean, var, eps=<span class="number">1e-5</span>)</span>:</span></div><div class="line">    normalized_x = torch.zeros_like(x)</div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        normalized_x[:, i, :, :] = (x[:, i, :, :] - mean[i]) / torch.sqrt(var[i] + eps)</div><div class="line">    <span class="keyword">return</span> normalized_x</div><div class="line">  </div><div class="line">mean, running_mean = get_mean(x, running_mean)</div><div class="line">var, running_var = get_var(x, running_var)</div><div class="line">x = norm_x(x, mean, var)</div></pre></td></tr></table></figure><p>å°ç»“ä¸€ä¸‹ï¼ŒBNåœ¨è®­ç»ƒå’Œæµ‹è¯•é‡‡å–çš„æ˜¯ä¸¤ç§æ¨¡å¼ï¼Œè®­ç»ƒé˜¶æ®µæ¯ä¸ªbatchç”¨çš„æ˜¯å½“å‰batchç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œæµ‹è¯•é˜¶æ®µæ¯ä¸ªç”¨çš„æ˜¯moving averagesçš„ç»Ÿè®¡å€¼ã€‚åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œå³$\gamma, \beta$ä½¿å¾—æ¯å±‚çš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½å¹³ç¨³ã€‚</p><h3 id="ç›¸å…³å‚æ•°"><a href="#ç›¸å…³å‚æ•°" class="headerlink" title="ç›¸å…³å‚æ•°"></a>ç›¸å…³å‚æ•°</h3><p>çŸ¥é“äº†å…·ä½“æ›´æ–°è¿‡ç¨‹çš„å®ç°åï¼Œæ¥çœ‹ä¸€ä¸‹PyTorchä¸­BatchNormçš„API</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">torch.nn.BatchNorm1d(num_features, </div><div class="line">                     eps=<span class="number">1e-05</span>, </div><div class="line">                     momentum=<span class="number">0.1</span>, </div><div class="line">                     affine=<span class="keyword">True</span>, </div><div class="line">                     track_running_stats=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>å…¶ä¸­<code>affine</code>è¡¨æ˜æ˜¯å¦ç”¨$\gamma$å’Œ$\beta$è¿›è¡Œä»¿å°„ï¼Œå½“<code>affine=False</code>æ—¶ï¼Œ<code>weigthts=None, bias=None</code>ï¼Œ<code>track_running_stats</code>è¡¨æ˜æ˜¯å¦æ›´æ–°ç»Ÿè®¡ç‰¹æ€§ï¼Œå½“<code>track_running_stats=False</code>æ—¶ï¼Œ<code>running_mean=None, running_var=None</code>ï¼Œå³æ¯æ¬¡å½’ä¸€åŒ–çš„æ—¶å€™åªç”¨å½“å‰batchçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸ä¼šå¯¹ä¹‹å‰ç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå¹³æ»‘ã€‚</p><p><strong>å‚è€ƒé“¾æ¥ï¼š</strong></p><ul><li><a href="https://blog.csdn.net/LoseInVain/article/details/86476010" target="_blank" rel="external">Pytorchçš„BatchNormå±‚ä½¿ç”¨ä¸­å®¹æ˜“å‡ºç°çš„é—®é¢˜</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;BNæ˜¯é˜²ç½‘ç»œè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ï¼Œç»†å¾®çš„å·®åˆ«å¯èƒ½å¯¹è¾“å‡ºæ•ˆæœæœ‰å¾ˆå¤§å½±å“ã€‚å› æ­¤éœ€è¦ç†è§£ä¸€ä¸‹PyTorchä¸­BNçš„å…·ä½“å®ç°ã€‚PyTorchçš„æºç ç”¨Cå®ç°çš„torch.batchnormã€‚å¯ä»¥&lt;a href=&quot;https://github.com/marvis/pytorch-
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>å·ç§¯æ ¸çš„ä½ç§©åˆ†è§£</title>
    <link href="http://yoursite.com/2019/07/01/tensor-decompositions/"/>
    <id>http://yoursite.com/2019/07/01/tensor-decompositions/</id>
    <published>2019-07-01T11:25:02.000Z</published>
    <updated>2019-07-28T01:57:23.769Z</updated>
    
    <content type="html"><![CDATA[<p>é€šè¿‡<a href="https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning" target="_blank" rel="external">è¿™ç¯‡blog</a>äº†è§£ä¸€ä¸‹ä½ç§©åˆ†è§£ã€‚</p><p>ä½ç§©åˆ†è§£ä»…ä½œç”¨äºçº¿æ€§ç½‘ç»œå±‚çš„æƒé‡ï¼Œå¯èƒ½å¿½ç•¥ä¸åŒç½‘ç»œå±‚çš„è”ç³»ã€‚</p><blockquote><p>There are works that try to address these issues, and its still an active research area.</p></blockquote><h3 id="å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£"><a href="#å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£" class="headerlink" title="å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£"></a>å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£</h3><p>é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹SVDã€‚å¥‡å¼‚å€¼åˆ†è§£(SVD)æ˜¯å¯¹çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼š<br>$$<br>A_{n \times m}=U_{n \times n} S_{n \times m} V_{m \times m}^{T}<br>$$<br>å…¶ä¸­$S$æ˜¯éè´Ÿå®æ•°å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šå…ƒç´ å³ä¸º$A$çš„<strong>å¥‡å¼‚å€¼</strong>ã€‚é€šå¸¸ä¼šå°†å¥‡å¼‚å€¼ç”±å¤§åˆ°å°æ’åºã€‚$U$å’Œ$V$æ˜¯é…‰çŸ©é˜µï¼Œæ»¡è¶³$U^{T} U=V^{T} V=I$ã€‚å½“æˆ‘ä»¬å–å…¶æœ€å¤§çš„$t$ä¸ªå¥‡å¼‚å€¼ï¼Œå°†å‰©ä¸‹çš„å€¼ç½®0ï¼Œåˆ™èƒ½å¾—åˆ°è¿‘ä¼¼çŸ©é˜µ$\hat{A}=U_{n x t} S_{t x t} V_{m x t}^{T}$ã€‚</p><p>æˆ‘ä»¬å¯¹å…¨è¿æ¥å±‚çš„å…¬å¼$A x+b$è¿›è¡Œåˆ†è§£æœ‰ï¼š$\left(U_{n \times t} S_{t \times t} V_{m \times t}^{T}\right) x+b=U_{n \times t}\left(S_{t \times t} V_{m \times t}^{T} x\right)+b$</p><p>è¿™æ ·å°†ä¸€ä¸ªå¤§çŸ©é˜µè½¬åŒ–ä¸ºä¸¤ä¸ªå°çŸ©é˜µï¼Œå‚æ•°é‡ä» $n \times m$ é™ä¸º $t  (n+m)$</p><h3 id="å·ç§¯å±‚çš„å¼ é‡åˆ†è§£"><a href="#å·ç§¯å±‚çš„å¼ é‡åˆ†è§£" class="headerlink" title="å·ç§¯å±‚çš„å¼ é‡åˆ†è§£"></a>å·ç§¯å±‚çš„å¼ é‡åˆ†è§£</h3><p>ä¸‹é¢ä»‹ç»å¯¹å·ç§¯å±‚è¿›è¡Œå¼ é‡åˆ†è§£æœ€ç»å…¸çš„ä¸¤ç§æ–¹æ³•ï¼šCPåˆ†è§£å’ŒTuckeråˆ†è§£</p><p><strong>CPåˆ†è§£</strong></p><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/1412.6553" target="_blank" rel="external">Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</a></p><p>å¯¹$A$è¿›è¡Œä½ç§©åˆ†è§£æœ‰ï¼š<br>$$<br>A(i, j)=\sum_{n=1}^{R} A_{1}(i, r) A_{2}(j, r), \quad i=\overline{1, n}, \quad j=\overline{1, m}<br>$$<br>å¯¹dç»´çš„$A$è¿›è¡ŒCPåˆ†è§£æœ‰ï¼š<br>$$<br>A\left(i_{1}, \ldots, i_{d}\right)=\sum_{r=1}^{R} A_{1}\left(i_{1}, r\right) \ldots A_{d}\left(i_{d}, r\right)<br>$$<br>å¯¹$d \times d \times S \times T$çš„4Dçš„å·ç§¯æ ¸è¿›è¡ŒCPåˆ†è§£<br>$$<br>K(i, j, s, t)=\sum_{r=1}^{R} K^{x}(i-x+\delta, r) K^{y}(j-y+\delta, r) K^{s}(s, r) K^{t}(t, r)<br>$$<br>åˆ™è¾“å‡ºå¯ä»¥$V$ è¡¨ç¤ºä¸ºï¼š<br>$$<br>V(x, y, t)=\sum_{r=1}^{R} K^{t}(t, r)\left(\sum_{i=x-\delta}^{x+\delta} K^{x}(i-x+\delta, r)\left(\sum_{j=y-\delta}^{y+\delta} K^{y}(j-y+\delta, r)\left(\sum_{s=1}^{S} K^{s}(s, r) U(i, j, s)\right)\right)\right)<br>$$<br>åˆ™1ä¸ªå·ç§¯æ“ä½œå¯ä»¥åˆ†è§£ä¸º4ä¸ªå·ç§¯æ“ä½œï¼š<br>$$<br>\begin{aligned} U^{s}(i, j, r) &amp;=\sum_{s=1}^{S} K^{s}(s, r) U(i, j, s) \\ U^{s y}(i, y, r) &amp;=\sum_{j=1}^{y+\delta} K^{y}(j-y+\delta, r) U^{s}(i, j, r) \\ U^{s y z}(x, y, r) &amp;=\sum_{i=x-\delta}^{x+\delta} K^{x}(i-x+\delta, r) U^{s y}(i, y, r) \\ V(x, y, t) &amp;=\sum_{r=1}^{R} K^{t}(t, r) U^{s y x}(x, y, r) \end{aligned}<br>$$</p><ol><li>ç”¨$1\times1$çš„pointwiseå·ç§¯å°†è¾“å…¥é™è‡³Rçº¬åº¦</li><li>åœ¨å‚ç›´ç»´åº¦åš$d\times1$çš„depthwiseå·ç§¯</li><li><p>åœ¨æ°´å¹³çº¬åº¦åš$d\times1$çš„depthwiseå·ç§¯</p></li><li><p>ç”¨$1\times 1$çš„pointwiseå·ç§¯è·å¾—$T$ç»´çš„è¾“å‡º</p></li></ol><p>å¤æ‚åº¦åˆ†æï¼š</p><ul><li>åŸå§‹å·ç§¯ $STd^2$</li><li>CPåˆ†è§£ $R(S+2d+T)$</li></ul><p><strong>Tuckeråˆ†è§£</strong></p><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/1511.06530" target="_blank" rel="external">Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications</a></p><p>Tuckeråˆ†è§£ä¹Ÿç§°ä¸ºé«˜é˜¶SVDï¼Œ4Då·ç§¯æ ¸è¡¨ç¤ºä¸ºï¼š<br>$$<br>K(i, j, s, t)=\sum_{r_{1}=1}^{R_{1}} \sum_{r_{2}=1}^{R_{2}} \sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} Câ€™_{r_1, r_2, r_3, r_4} K_{r 1}^{x}(i) K_{r_2}^{y}(j) K_{r_3}^{s}(s) K_{r _4}^{t}(t)<br>$$<br>å·ç§¯æ ¸é€šå¸¸æ¯”è¾ƒå°($3 \times 3$)ï¼Œå°±ä¸å†åœ¨ç©ºé—´ç»´åº¦è¿›è¡Œåˆ†è§£<br>$$<br>K(i, j, s, t)=\sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} Câ€™_{i,j, r_3, r_4} K_{r_3}^{s}(s) K_{r _4}^{t}(t)<br>$$<br>å…¶ä¸­$C$ä»£è¡¨$d \times d \times R_3 \times R_4$ çš„æ ¸å¿ƒå¼ é‡ã€‚</p><p>äºæ˜¯å°†1ä¸ªå·ç§¯æ“ä½œåˆ†è§£ä¸º3ä¸ªå·ç§¯æ“ä½œï¼š<br>$$<br>\begin{aligned} \mathcal{Z}_{h, w, r_{3}} &amp;=\sum_{s=1}^{S} U_{s, r_{3}}^{(3)} \mathcal{X}_{h, w, s} \\ \mathcal{Z}_{h^{\prime}, w^{\prime}, r_{4}}^{D} &amp;=\sum_{i=1}^{D} \sum_{j=1}^{D} \sum_{r_{3}=1}^{R_{3}} \mathcal{C}_{i, j, r_{3}, r_{4}} z_{h_{i} w_{j}, r_{3}} \\ y_{h^{\prime}, w^{\prime}, t} &amp;=\sum_{r_{4}=1}^{R_{4}} U_{t, r_{4}}^{(4)} \mathcal{Z}_{h^{\prime}, w^{\prime}, r_{4}}^{\prime} \end{aligned}<br>$$</p><ol><li>ç”¨$1\times1$çš„pointwiseå·ç§¯å°†è¾“å…¥é™è‡³Rçº¬åº¦</li><li>è¿›è¡Œ$d \times d$çš„å·ç§¯</li><li>ç”¨$1\times 1$çš„pointwiseå·ç§¯è·å¾—$T$ç»´çš„è¾“å‡º</li></ol><h3 id="æŒ‘é€‰åˆ†è§£çš„ç§©"><a href="#æŒ‘é€‰åˆ†è§£çš„ç§©" class="headerlink" title="æŒ‘é€‰åˆ†è§£çš„ç§©"></a>æŒ‘é€‰åˆ†è§£çš„ç§©</h3><p>åœ¨åˆ†è§£çš„æ—¶å€™ç§©$R$çš„é€‰æ‹©å¾ˆé‡è¦ï¼ŒTuckeråˆ†è§£ä¸­ç”¨äº†VBMFçš„æ–¹æ³•ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;é€šè¿‡&lt;a href=&quot;https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;è¿™ç¯‡blog&lt;/a&gt;äº†è§£ä¸€ä¸‹ä½ç§©åˆ†è§£
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.08</title>
    <link href="http://yoursite.com/2019/06/29/weekly-paper-08/"/>
    <id>http://yoursite.com/2019/06/29/weekly-paper-08/</id>
    <published>2019-06-29T02:42:16.000Z</published>
    <updated>2019-07-08T01:08:09.009Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure"><a href="#1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure" class="headerlink" title="1ï¸âƒ£  Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure"></a>1ï¸âƒ£  Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure</h3><p>The main idea of this work is to make some filters close to each other in the same cluster during training, and propose Centripetal SGD (C-SGD). </p><p>the update rule of C-SGD is<br>$$<br>\boldsymbol{F}^{(j)} \leftarrow \boldsymbol{F}^{(j)}+\tau \Delta \boldsymbol{F}^{(j)}<br>$$</p><p>$$<br>\begin{aligned} \Delta \boldsymbol{F}^{(j)}=&amp;-\frac{\sum_{k \in H(j)} \frac{\partial L}{\partial \boldsymbol{F}^{(k)}}}{|H(j)|}-\eta \boldsymbol{F}^{(j)} \\ &amp;+\epsilon\left(\frac{\sum_{k \in H(j)} \boldsymbol{F}^{(k)}}{|H(j)|}-\boldsymbol{F}^{(j)}\right) \end{aligned}<br>$$</p><p>For the filters in the same cluster 1ï¼‰objective function are averaged 2ï¼‰weight decay 3ï¼‰gradually eliminate the difference in the initial values.</p><h3 id="2ï¸âƒ£-Variational-Convolutional-Neural-Network-Pruning"><a href="#2ï¸âƒ£-Variational-Convolutional-Neural-Network-Pruning" class="headerlink" title="2ï¸âƒ£ Variational Convolutional Neural Network Pruning"></a>2ï¸âƒ£ Variational Convolutional Neural Network Pruning</h3><p>æœ¬æ–‡ç”¨å˜åˆ†æ¨ç†æ¥è¿›è¡Œå‰ªææ„Ÿè§‰è¿˜è›®æœ‰æ„æ€çš„ã€‚é¦–å…ˆæ–‡ç« åŸºäºNetwork Slimmingåšäº†ä¸€äº›æ”¹è¿›ã€‚å¯¹äºBNçš„$\gamma$ï¼Œå¦‚æœåªç¨€ç–è¿™ä¸ªå€¼ï¼Œä¸è€ƒè™‘$\beta$çš„å½±å“ï¼Œé‚£å®é™…ä¸Šå½’ä¸€åŒ–åçš„è¾“å‡ºä¸ä¼šä¸º0ï¼Œè€Œæ˜¯åŠ ä¸Š$\beta$åçš„å€¼ï¼Œæ‰€ä»¥æ–‡ç« è€ƒè™‘çš„æ”¹è¿›æ–¹å¼æ˜¯ä»¤$\tilde{\beta} = \gamma \cdot \beta$ï¼Œ<br>$$<br>x_{o u t}=\gamma \cdot B N(x)+\tilde{\beta}<br>$$<br>æ–‡ä¸­å°†è¿™ä¸ª$\gamma$ç§°ä¸º<em>channel saliency</em>ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ ç¨€ç–çš„$\gamma$åŒæ—¶æœ€å¤§åŒ–æ¡ä»¶æ¦‚ç‡$p(y | x, \gamma)$ã€‚</p><p>é¦–å…ˆåˆ©ç”¨è´å¶æ–¯å…¬å¼æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š$ p(\gamma | \mathcal{D})= \frac{p(\gamma) p(\mathcal{D} | \gamma)} { p(\mathcal{D})}$</p><p>ç”±äº$p(\mathcal{D})=\int p(\mathcal{D}, \gamma) d \gamma$éš¾ä»¥è®¡ç®—ï¼Œè¿™ä¸ªåéªŒæ¦‚ç‡åˆ†å¸ƒæˆ‘ä»¬å¾ˆéš¾ç›´æ¥æ±‚çš„ã€‚åœ¨å˜åˆ†æ¨ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªå‚æ•°åˆ†å¸ƒ$q_{\phi}(\gamma)$æ¥è¿‘ä¼¼è¿™ä¸ªåéªŒæ¦‚ç‡åˆ†å¸ƒã€‚åˆ©ç”¨KLæ•£åº¦æ‹‰è¿‘ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ï¼š$\min _{\phi} D_{K L}\left(q_{\phi}(\gamma) | p(\gamma | \mathcal{D})\right)$ã€‚ç­‰ä»·äºæœ€å¤§åŒ–ELBOï¼š<br>$$<br>\mathcal{L}(\phi)=L_{\mathcal{D}}(\phi)-D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)<br>$$<br>å…¶ä¸­ï¼Œ$\mathcal{L}_{\mathcal{D}}(\phi)=\sum_{(x, y) \in \mathcal{D}} \mathbb{E}_{q_{\phi}(\gamma)}[\log p(y | x, \gamma)]$</p><p>å¯ä»¥çœ‹åˆ°ç›®æ ‡å‡½æ•°ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯é‡å»ºé¡¹ï¼Œæ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸ºæ­£åˆ™é¡¹ï¼Œåé¢ä¼šå¼•å…¥ä¸€ä¸ªå…ˆéªŒåˆ†å¸ƒå¯¹å‚æ•°è¿›è¡Œæƒ©ç½šï¼Œå³ç¨€ç–$\gamma$ã€‚</p><p>å¯¹äº$\mathcal{L}(\phi)$éœ€è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š</p><ol><li>ç¬¬ä¸€é¡¹ä¸­ç”±äºæœŸæœ›çš„å­˜åœ¨ï¼Œ$\mathcal{L}_{\mathcal{D}}(\phi)$çš„æ¢¯åº¦æ— æ³•ç›´æ¥æ±‚å¾—ã€‚</li><li>ç¬¬äºŒé¡¹å‚æ•°åˆ†å¸ƒ$q_\phi(\gamma)$å’Œå…ˆéªŒåˆ†å¸ƒ$p(\gamma)$çš„é€‰æ‹©ã€‚</li></ol><p>ğŸ”º é—®é¢˜1çš„è§£å†³ï¼š</p><p>â€‹    å¼•å…¥å†å‚åŒ–æŠ€å·§ï¼Œåˆ™$q_{\phi}(\gamma)$å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå¯å¯¼å‡½æ•°$\gamma=f(\phi, \epsilon)$ï¼Œå…¶ä¸­$\epsilon \sim \mathcal{N}(0,1)$<br>$$<br>\mathcal{L}_{\mathcal{D}}(\phi) \simeq \mathcal{L}_{\mathcal{D}}^{\mathcal{A}}(\phi)=\frac{N}{M} \sum_{m=1}^{M} \log p\left(y_{i m} | x_{i m}, \gamma_{i m}=f(\phi, \epsilon)\right)<br>$$<br>â€‹    å…¶ä¸­$M$ä¸ºbatch sizeï¼Œ$N$ä¸ºdataæ•°é‡ã€‚</p><p>â€‹    å°†æ¨¡å‹å‚æ•°$\mathbf{w}$åŠ å…¥ä¼˜åŒ–ç›®æ ‡ä¸­ï¼š<br>$$<br>\mathcal{L}(\phi, \mathbf{w}) \simeq \mathcal{L}_{\mathcal{D}}^{\mathcal{A}}(\phi, \mathbf{w})-D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)<br>$$</p><p>ğŸ”º é—®é¢˜2çš„è§£å†³ï¼š</p><p>â€‹    æœ¬æ–‡é€‰å–é«˜æ–¯åˆ†å¸ƒä½œä¸ºå‚æ•°çš„åˆ†å¸ƒï¼š<br>$$<br>q_{\phi}(\gamma)=\prod_{i=1}^{C} q\left(\gamma_{i}\right), \quad \gamma_{i} \sim \mathcal{N}\left(\mu_{i}, \sigma_{i}\right)<br>$$<br>â€‹    ä¸ºäº†è®©å­¦ä¹ å‡ºçš„å‚æ•°$\phi=(\mu, \sigma)$ä½¿åˆ†å¸ƒ$q_\phi(\gamma)$å°½å¯èƒ½ç¨€ç–ï¼Œæœ¬æ–‡å¼•å…¥çš„å…ˆéªŒåˆ†å¸ƒä¸ºï¼š<br>$$<br>p(\gamma)=\prod_{i=1}^{C} p\left(\gamma_{i}\right), \quad \gamma_{i} \sim \mathcal{N}\left(0, \sigma_{i}^{_}\right)<br>$$<br>â€‹    è¿™æ ·å°±èƒ½è®©$\gamma$å°½å¯èƒ½å‘0å€¼é è¿‘ã€‚<br>$$<br>\begin{aligned} D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right) &amp;=\sum_{i} D_{K L}\left(q_{\phi}\left(\gamma_{i}\right) | p\left(\gamma_{i}\right)\right) \\ &amp;=\sum_{i} \log \frac{\sigma_{i}^{_}}{\sigma_{i}}+\frac{\sigma_{i}^{2}+\mu_{i}^{2}}{2\left(\sigma_{i}^{*}\right)^{2}}-\frac{1}{2} \end{aligned}<br>$$<br>â€‹    è®©ä¸¤ä¸ªåˆ†å¸ƒçš„æ–¹å·®ç›¸åŒï¼Œåˆ™ä¸Šå¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š<br>$$<br>D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)=\sum_{i} k \mu_{i}^{2}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Centripetal-SGD-for-Pruni
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.07</title>
    <link href="http://yoursite.com/2019/06/25/weekly-paper-07/"/>
    <id>http://yoursite.com/2019/06/25/weekly-paper-07/</id>
    <published>2019-06-25T00:55:36.000Z</published>
    <updated>2019-07-08T01:08:06.520Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques"><a href="#1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques" class="headerlink" title="1ï¸âƒ£  Universally Slimmable Networks and Improved Training Techniques"></a>1ï¸âƒ£  Universally Slimmable Networks and Improved Training Techniques</h3><p>Slimmable networkçš„æ‹“å±•å·¥ä½œï¼Œå°†å›ºå®šå®½åº¦çš„ç½‘ç»œæ‰©å±•åˆ°ä»»æ„å®½åº¦ã€‚æå‡ºäº†3ä¸ªæŒ‘æˆ˜ï¼š</p><ol><li>å¦‚ä½•è§£å†³åŒ…å«batch-normalizationçš„ç½‘ç»œï¼Ÿ</li><li>å¦‚ä½•æ›´æœ‰æ•ˆåœ°è®­ç»ƒUS-Nets</li><li>ä¸å•ç‹¬è®­ç»ƒæŸä¸ªå®½åº¦çš„ç½‘ç»œç›¸æ¯”ï¼ŒUS-Netsæ˜¯å¦‚ä½•æå‡æ•´ä½“ç²¾åº¦çš„ï¼Ÿ</li></ol><p>ğŸ”º é—®é¢˜1çš„è§£å†³ï¼š</p><ol><li><p>è®­ç»ƒé˜¶æ®µæ¯æ¬¡å‰å‘æ—¶ï¼Œè®¡ç®—å‡ºè¯¥batchçš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼š<br>$$<br>\hat{x}_{B}=\gamma \frac{x_{B}-E_{B}\left[x_{B}\right]}{\sqrt{\operatorname{Var}_{B}\left[x_{B}\right]+\epsilon}}+\beta<br>$$<br>è®­ç»ƒè¿‡ç¨‹ä¼šå¯¹å‡å€¼å’Œæ–¹å·®åš<strong>moving averages</strong>ï¼š<br>$$<br>\begin{aligned} \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>$$</p><blockquote><p> éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨PyTorchçš„å®ç°ä¸­ï¼Œæ¯æ¬¡è¿›è¡Œç»Ÿè®¡æ—¶$Var_B = \frac{n}{n-1}Var_B$ï¼Œå…¶ä¸­ $n=c \times h \times w$</p></blockquote><p>æµ‹è¯•é˜¶æ®µï¼Œç”¨ç»Ÿè®¡å€¼$\mu=\mu_{T}ï¼Œ \sigma^2=\sigma^2_T$è¿›è¡Œå½’ä¸€åŒ–ï¼š<br>$$<br>\hat{x}_{t e s t}=\gamma^{_} \frac{x_{t e s t}-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta^{_}<br>$$<br>å…¶ä¸­$\gamma^_, \beta^_$æ˜¯bnå­¦å‡ºçš„weightå’Œbiasã€‚è¿›ä¸€æ­¥å¯ä»¥è¡¨ç¤ºä¸ºï¼š<br>$$<br>\hat{x}_{t e s t}=\gamma^{\prime} x_{t e s t}+\beta^{\prime}, \gamma^{\prime}=\frac{\gamma^{_}}{\sqrt{\sigma^{2}+\epsilon}}, \beta^{\prime}=\beta^{_}-\gamma^{\prime} \mu<br>$$</p></li></ol><p>   å¦‚æœå¯¹ä¸åŒå®½åº¦çš„ç½‘ç»œé‡‡ç”¨Shared BNï¼Œç”±äºç‰¹å¾æ˜¯ç›¸åŠ çš„ï¼Œå‰ä¸€å±‚æ˜¯ç”¨ä¸åŒçš„é€šé“æ•°ï¼Œè¾“å‡ºå€¼å°±ä¼šæœ‰æ‰€ä¸åŒï¼Œå‡å€¼å’Œæ–¹å·®ä¹Ÿä¸åŒï¼Œå¯¼è‡´äº†ç»Ÿè®¡å€¼ä¸å‡†ç¡®çš„é—®é¢˜ã€‚Slimmable Networkçš„è§£å†³åŠæ³•æ˜¯å¯¹æ¯ä¸ªå®½åº¦éƒ½è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„BNå±‚ï¼Œä½†å¦‚æœå¯¹æ‰€æœ‰å®½åº¦éƒ½è¿™æ ·åšä»£ä»·å¤ªå¤§äº†ã€‚æœ¬æ–‡çš„è§£å†³åŠæ³•æ˜¯åšexact averagesï¼š<br>   $$<br>   \begin{aligned} m &amp;=(t-1) / t \\ \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>   $$<br>   ç»Ÿè®¡å€¼çš„è®¡ç®—ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œï¼Œè€Œæ˜¯è®­ç»ƒç»“æŸåï¼Œç”¨éšæœºé‡‡æ ·çš„è®­ç»ƒæ•°æ®è¿›è¡Œä¼°è®¡ã€‚</p><p>ğŸ”º é—®é¢˜2çš„è§£å†³ï¼š</p><ol><li>æœ¬æ–‡å‡è®¾æ¨¡å‹çš„è¡¨ç°é™åˆ¶äºå®½åº¦$[0.25 \times, 1.0 \times]$ï¼Œä¼˜åŒ–lower boundå’Œupper boundå°±èƒ½ä¼˜åŒ–æ•´ä¸ªç½‘ç»œã€‚äºæ˜¯æå‡ºäº†<strong>Sandwich Rule</strong>ï¼Œåœ¨æ¯æ¬¡è®­ç»ƒæ—¶éšæœºé‡‡æ ·$n-2$ä¸ªå®½åº¦ï¼ŒåŠ ä¸Šæœ€å°å®½åº¦å’Œæœ€å¤§å®½åº¦ä¸€èµ·è®­ç»ƒã€‚åŒæ—¶è·Ÿè¸ªè¿™ä¸¤ä¸ªæ¨¡å‹çš„éªŒè¯ç²¾åº¦ï¼Œèƒ½å¤§æ¦‚çŸ¥é“US-Netçš„lower boundå’Œupper boundã€‚å¹¶ä¸”ï¼Œè®­ç»ƒæœ€å¤§å®½åº¦çš„æ¨¡å‹å¯ä»¥ç”¨äº<strong>Inplace Distillation</strong>ã€‚æœ€å¤§å®½åº¦çš„æ¨¡å‹ç”¨groud truthåšlossï¼Œè€Œå…¶å®ƒå®½åº¦çš„æ¨¡å‹å¯ä»¥ç”¨æœ€å¤§å®½åº¦æ¨¡å‹é¢„æµ‹å‡ºçš„soft-probabilitiesåšlossã€‚</li></ol><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190703111249984.png" alt="image-20190703111249984"></p><p>æ–‡ç« æœ€ååç€è®¨è®ºäº†å‡ ä¸ªè¯é¢˜ï¼š</p><ol><li>æˆ‘ä»¬èƒ½ä¸èƒ½è®­ç»ƒä¸€ä¸ªéå‡åŒ€çš„US-Netè¿™æ ·æ¯å±‚èƒ½å¤Ÿè°ƒæ•´å®ƒè‡ªå·±çš„å®½åº¦æ¯”ï¼Ÿ</li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Train
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflowä¸­çš„PixelShuffle(depth_to_space)</title>
    <link href="http://yoursite.com/2019/06/23/tf-pixshuffle/"/>
    <id>http://yoursite.com/2019/06/23/tf-pixshuffle/</id>
    <published>2019-06-23T08:31:38.000Z</published>
    <updated>2019-06-28T02:10:01.249Z</updated>
    
    <content type="html"><![CDATA[<p>åœ¨å°è¯•å¯¹PixelShuffleå‰çš„å·ç§¯å±‚åšå‰ªææ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¯¹PixelShuffleçš„å…·ä½“æ“ä½œæœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚</p><p>PixelShuffleé€šè¿‡å°†é€šé“é‡æ’å¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œtfä¸­çš„å‡½æ•°æ˜¯<code>tf.depth_to_sapce</code>ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>Tensor</code>ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯éœ€è¦æ”¾å¤§å€æ•°ã€‚å½“è¾“å…¥<code>X</code>çš„å¤§å°ä¸º<code>[1 2 2 16]</code>ï¼Œæ”¾å¤§å€æ•°ä¸º2ï¼ŒHå’ŒWå„ä¹˜2ï¼ŒCé™¤ä»¥4ï¼ŒPixelShuffleåçš„ç»“æœå°±ä¸º<code>[1 4 4 4]</code>ã€‚</p><p>ğŸ”ºå‘ç‚¹1ï¼šæƒ³å½“ç„¶çš„ä»¥ä¸ºå‚ä¸é‡æ’çš„é€šé“æ˜¯<code>[:, :, :, i:i+4]</code></p><p>æµ‹è¯•ä»£ç ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x = tf.range(<span class="number">64</span>)</div><div class="line">x = tf.reshape(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">16</span>])</div><div class="line">y = tf.depth_to_space(x, <span class="number">2</span>) <span class="comment"># [1, 4, 4, 4]</span></div></pre></td></tr></table></figure><p>ä¸‹é¢çœ‹ä¸€ä¸‹å…·ä½“xå’Œyæ¯ä¸ªé€šé“çš„å€¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>, <span class="number">16</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">48</span>]]]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">1</span>, <span class="number">17</span>],</div><div class="line">[<span class="number">33</span>, <span class="number">49</span>]]]</div><div class="line"> </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">4</span>, <span class="number">16</span>, <span class="number">20</span>],</div><div class="line">[ <span class="number">8</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">28</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">36</span>, <span class="number">48</span>, <span class="number">52</span>],</div><div class="line">[<span class="number">40</span>, <span class="number">44</span>, <span class="number">56</span>, <span class="number">60</span>]]]</div></pre></td></tr></table></figure><p>å¯ä»¥çœ‹å‡º<code>y</code>çš„ç¬¬0ç»´é€šé“åŒ…å«çš„æ˜¯<code>x</code>é€šé“æ•°ä¸º0ã€4ã€8ã€12çš„ç‰¹å¾å›¾ã€‚å¯è§†åŒ–ä¸€ä¸‹å°±æ˜¯è¿™æ ·çš„æ•ˆæœï¼š</p><p><img src="https://i.loli.net/2019/06/27/5d14b6a1d9bbf35069.png" alt=""></p><p>å°†Yçš„ä¸€ä¸ªé€šé“å•ç‹¬å–å‡ºï¼Œçœ‹ä¸€ä¸‹æ¯ä¸ªç‚¹å±äºåŸæ¥Xçš„å“ªä¸ªåæ ‡ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190628095308642.png" alt="image-20190628095308642"></p><p>å¯ä»¥çœ‹åˆ°Yçš„ä¸€ä¸ªé€šé“å®é™…ä¸Šåˆ†æˆ4ä¸ªè±¡é™ï¼Œåœ¨ç©ºé—´ä¸Šç”±<code>(0,0)(0,1)(1,0)(1,1)</code>æ„æˆã€‚åœ¨é€šé“ä¸Šæ¯4ä¸ªé—´éš”æå–å¯¹åº”é€šé“ã€‚è¿™é‡Œçš„é—´éš”å¯¹åº”çš„æ˜¯Pixshuffleåçš„é€šé“æ•°ã€‚</p><p>å‡è®¾åŸå§‹é€šé“æ•°ä¸º<code>c_out</code>ï¼ŒPSåçš„é€šé“æ•°ä¸º<code>ps_out</code>ï¼Œå®é™…ä¸Š<code>y</code>çš„ç¬¬<code>i</code>é€šé“å¯¹åº”çš„æ˜¯<code>x</code>çš„<code>[i, i+ps_out, i+2*ps_out, i+3*ps_out]</code></p><p>è€Œæˆ‘åŸå…ˆç†è§£çš„é€šé“æ’åˆ—æ–¹å¼æ˜¯âŒ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">5</span>],</div><div class="line">[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">6</span>,  <span class="number">7</span>],</div><div class="line">[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">12</span>, <span class="number">13</span>],</div><div class="line">[<span class="number">10</span>, <span class="number">11</span>, <span class="number">14</span>, <span class="number">15</span>]]]</div></pre></td></tr></table></figure><p>ğŸ”ºå‘ç‚¹2ï¼šæå–kä¸ªä¿ç•™çš„é€šé“æ—¶ï¼Œåªéœ€å–ç´¢å¼•çš„å‰kä¸ªå€¼</p><p>å‡è®¾æ”¾å¤§å€ç‡æ˜¯4ï¼Œç”¨L1çš„å‰ªææ–¹å¼ï¼Œéœ€è¦ä¿ç•™çš„é€šé“æ•°ä¸º<code>c_keep</code>ã€‚å½“å¯¹åº”åˆ°å…·ä½“çš„å‰ªæé€šé“çš„æ—¶å€™ï¼Œéœ€è¦æ‰¾åˆ°PixShuffleåå‰ªæ‰é€šé“æ‰€å¯¹åº”çš„åŸå§‹å·ç§¯è¾“å‡ºçš„4ä¸ªé€šé“ã€‚ä»ä¸Šé¢çš„åæ ‡æˆ‘ä»¬å°±å¯ä»¥çœ‹å‡ºï¼Œå‰ªæ‰Yçš„0é€šé“æ—¶ï¼Œéœ€è¦å¯¹åº”å‰ªæ‰Xçš„0ã€4ã€8ã€12é€šé“ã€‚æ¥çœ‹çœ‹å…·ä½“çš„å®ç°ã€‚ä¸»è¦åˆ†ä¸ºå‡ æ­¥ï¼š</p><ol><li>è®¡ç®—Yå¯¹åº”Xçš„é€šé“</li><li>è®¡ç®—Yéœ€è¦ä¿ç•™çš„é€šé“</li><li>å°†Yçš„é€šé“æ˜ å°„å›X</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. è®¡ç®—Yå¯¹åº”Xçš„é€šé“</span></div><div class="line">norm_list, shuffled_idx_list = [], []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(ps_out):</div><div class="line">shuffled_idx = [i+k*ps_out <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">4</span>)]  <span class="comment"># Yé€šé“å¯¹åº”çš„4ä¸ªXé€šé“</span></div><div class="line">shuffled_idx_list.append(shuffled_idx)</div><div class="line">  norm_sum = tf.reduce_sum(tf.gather(norm_value, shuffled_idx)) <span class="comment"># æå–å¯¹åº”ç´¢å¼•çš„é€šé“</span></div><div class="line">  norm_list.append(sess.run(norm_sum))</div><div class="line">  </div><div class="line"><span class="comment"># 2. è®¡ç®—éœ€è¦ä¿ç•™çš„é€šé“</span></div><div class="line">remain_idx = np.sort(np.argsort(norm_list)[::<span class="number">-1</span>][:int(c_keep/<span class="number">4</span>)])</div><div class="line"></div><div class="line">remain_list = []</div><div class="line"><span class="comment"># 3. å°†Yçš„é€šé“æ˜ å°„å›X</span></div><div class="line"><span class="keyword">for</span> remain <span class="keyword">in</span> remain_idx:</div><div class="line">  remain_list.extend(shuffled_idx_list[remain])</div><div class="line">remain_list = np.sort(remain_list)</div></pre></td></tr></table></figure><p>è¿™æ ·<code>remain_list</code>å³åŸå§‹å·ç§¯è¾“å‡ºéœ€è¦å‰ªæ‰çš„é€šé“ç´¢å¼•ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;åœ¨å°è¯•å¯¹PixelShuffleå‰çš„å·ç§¯å±‚åšå‰ªææ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¯¹PixelShuffleçš„å…·ä½“æ“ä½œæœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚&lt;/p&gt;
&lt;p&gt;PixelShuffleé€šè¿‡å°†é€šé“é‡æ’å¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œtfä¸­çš„å‡½æ•°æ˜¯&lt;code&gt;tf.depth_to_sapce&lt;/code&gt;ï¼Œç¬¬ä¸€ä¸ª
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.06</title>
    <link href="http://yoursite.com/2019/06/18/weekly-paper-06/"/>
    <id>http://yoursite.com/2019/06/18/weekly-paper-06/</id>
    <published>2019-06-18T01:49:16.000Z</published>
    <updated>2019-06-21T09:12:49.701Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers"><a href="#1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers" class="headerlink" title="1ï¸âƒ£ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers"></a>1ï¸âƒ£ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers</h3><p>è¿™ç¯‡å’Œ<a href="https://arxiv.org/abs/1812.08928" target="_blank" rel="external">ICLR2019</a>ã€<a href="https://arxiv.org/abs/1903.05134" target="_blank" rel="external">Universally Slimmable Networks</a>æ˜¯åŒä¸€ä¸ªä½œè€…ï¼Œè§£å†³çš„é—®é¢˜éƒ½æ˜¯é€šé“å‰ªæã€‚ä¸‹é¢å…ˆäº†è§£ä¸€ä¸‹æœ¬æ–‡ã€‚</p><p><strong>Why?</strong></p><p>Most channel pruning methods are grouneded on <strong>the importance of trained weights</strong>, so the slimmed layer usually consists channels of discrete index. Most NAS methods have high computational cost and time cost.</p><p><strong>How?</strong></p><p>Extending the work of slimmable networks and propose AutoSlim. The training process is as following:</p><ol><li><p>Train a slimmable model for a few epochs to get a benchmark performance estimator.</p><ul><li><p>Searching space is defined between the upper bound and lower bound of channel numbers. In each training iteration, randomly sample the number of channels in each layer. In each layer remove a group of channels. </p><blockquote><p>in resents, first sample the channel number of residual dentity pathway and then randomly and independenly sample channel number inside each residual block.</p></blockquote></li></ul></li><li><p>Evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop on validation set.</p></li><li><p>Obtain the optimized channel configurations under different resource constraints.</p></li><li><p>Train optimized architectures individually or slimmable network for full training epochs.</p></li></ol><p>The paper is based on the assumption that <strong>the importance of weight is implicitly ranked by its index</strong>, which means that the smaller index of one filter the more important of this filter.</p><h3 id="2ï¸âƒ£-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks"><a href="#2ï¸âƒ£-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks" class="headerlink" title="2ï¸âƒ£ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks"></a>2ï¸âƒ£ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks</h3><p>The method can be easily found in the title, to gradually grow the depth of DNN.</p><p>The  <em>network</em> is  composed of <em>sub-netwok</em>, and <em>sub-network</em> is composed of <em>sub-modules</em>.<br>$$<br>g\left(\mathcal{X}_{0}\right)=l\left(\boldsymbol{f}_{M-1}\left(\boldsymbol{f}_{M-2}\left(\cdots \boldsymbol{f}_{1}\left(\boldsymbol{f}_{0}\left(\mathcal{X}_{0}\right)\right) \cdots\right)\right)\right)<br>$$<br>AutoGrow is based on Network Morphism, but propose to initilize the last Batch Normalization layer in a residual block of <em>AdamInit</em> insted of <em>ZeroInit</em></p><p><strong>AdamInit</strong></p><p>given the new layers $\mathcal{W}$, we have:<br>$$<br>g\left(\mathcal{X}_{0} ; \mathbb{W}\right)=g\left(\mathcal{X}_{0} ; \mathbb{W} \cup \mathcal{W}\right) \forall \mathcal{X}_{0}<br>$$<br>freeze all parameters except the last Batch Normalization layer in $\mathcal{W}$, use Adam optimizer to optimize the last Batch Normalization layer.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-f
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.05</title>
    <link href="http://yoursite.com/2019/06/10/weekly-paper-05/"/>
    <id>http://yoursite.com/2019/06/10/weekly-paper-05/</id>
    <published>2019-06-10T02:29:31.000Z</published>
    <updated>2019-06-18T01:49:25.901Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Dynamic-Capacity-Networks"><a href="#1ï¸âƒ£-Dynamic-Capacity-Networks" class="headerlink" title="1ï¸âƒ£ Dynamic Capacity Networks"></a>1ï¸âƒ£ Dynamic Capacity Networks</h3><p>use two alternative sub-networks: </p><ol><li>coarse layers $f_c$ on the whole input $\mathbf {x}$  </li><li>fine layers $f_f$ at salient regions </li></ol><p><strong>coarse representation vectors</strong><br>$$<br>f_{c}(\mathbf{x})=\left\{\mathbf{c}_{i, j} |(i, j) \in\left[1, s_{1}\right] \times\left[1, s_{2}\right]\right\}<br>$$</p><p>$$<br>h_{c}(\mathbf{x})= \mathbf{o}_c = g\left(f_{c}(\mathbf{x})\right)<br>$$</p><p>$\mathbf{c}_{i, j}=f_{c}\left(\mathbf{x}_{i, j}\right) \in \mathbb{R}^{D}$ </p><p><strong>salient input regions</strong><br>$$<br>H=-\sum_{l=1}^{C} \mathbf{o}_{c}^{(l)} \log \mathbf{o}_{c}^{(l)}<br>$$</p><p>$$<br>M_{i, j}=\left|\nabla_{\mathbf{c}_{i, j}} H\right|_{2}<br>$$</p><p>$C$ is the number of class labels, $\mathbf{M} \in \mathbb{R}^{s_{1} \times s_{2}}$</p><p>select top $k$ input regions $\mathbf{X}^{s}=\left\{\mathbf{x}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}$ based on $\mathbf{M}$</p><p><strong>fine representation vectors</strong><br>$$<br>f_{f}\left(\mathbf{X}^{s}\right)=\left\{\mathbf{f}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}<br>$$<br>refined representation $f_r(\mathbf {x})$ by combining $f_c(\mathbf{x})$ and $f_f(\mathbf{X}^s)$</p><p><strong>loss</strong></p><ol><li><p>Cross Entropy<br>$$<br>J=-\sum_{i=1}^{m} \log p\left(y^{(i)} | \mathbf{x}^{(i)} ; \theta\right)<br>$$</p></li><li><p>encourage similarity between the coarse and fine representations</p></li></ol><p>$$<br>\sum_{\mathbf{x}_{i, j} \in \mathbf{X}^{s}}\left|f_{c}\left(\mathbf{x}_{i, j}\right)-f_{f}\left(\mathbf{x}_{i, j}\right)\right|_{2}^{2}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Dynamic-Capacity-Networks&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Dynamic-Capacity-Networks&quot; class=&quot;headerlink&quot; title=&quot;1ï¸âƒ£ Dynamic Capacity Networks&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Gumbel Softmax</title>
    <link href="http://yoursite.com/2019/05/28/gumbel-softmax/"/>
    <id>http://yoursite.com/2019/05/28/gumbel-softmax/</id>
    <published>2019-05-28T07:29:09.000Z</published>
    <updated>2019-07-16T00:34:11.969Z</updated>
    
    <content type="html"><![CDATA[<p>åœ¨PAGé‡Œå‘ç°äº†Gumbel Sampling Trickï¼ŒæŠŠç¦»æ•£çš„é‡‡æ ·è¿‡ç¨‹ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥ï¼Œäºæ˜¯å¯ä»¥æ”¾è¿›ç¥ç»ç½‘ç»œä¸­è¿›è¡Œæ±‚å¯¼å’Œåå‘ï¼Œè§‰å¾—æ˜¯å¾ˆæœ‰æ„æ€çš„å·¥ä½œï¼Œæƒ³è¦å¤šåŠ æ·±ä¸€äº›äº†è§£ã€‚</p><h3 id="é—®é¢˜å¼•å…¥"><a href="#é—®é¢˜å¼•å…¥" class="headerlink" title="é—®é¢˜å¼•å…¥"></a>é—®é¢˜å¼•å…¥</h3><p>é€šè¿‡<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">åšå®¢</a>å…¥äº†ä¸€ä¸‹å°é—¨ï¼Œç»“åˆ<a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="external">çŸ¥ä¹</a>ï¼Œé¦–å…ˆæ¥ç†è§£ä¸€ä¸‹Gumbel Sampling Trickç”¨æ¥åšä»€ä¹ˆã€‚</p><blockquote><p>å·²çŸ¥ä¸€ä¸ªç¦»æ•£éšæœºå˜é‡Xçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æƒ³å¾—åˆ°ä¸€äº›æœä»è¿™ä¸ªåˆ†å¸ƒçš„ç¦»æ•£çš„xçš„å€¼ã€‚</p></blockquote><p>æ¯”è¾ƒç®€å•çš„æ–¹æ³•æ˜¯ç”¨<code>np.random.choice</code>ã€‚æ¯”å¦‚æˆ‘ä»¬ç°åœ¨æœ‰5ä¸ªå€¼ï¼Œæ¦‚ç‡åˆ†å¸ƒæ˜¯<code>[0.1, 0, 0.3, 0.6, 0]</code>ï¼Œå³ç¬¬4ä¸ªå…ƒç´ æœ€æœ‰å¯èƒ½è¢«é‡‡æ ·åˆ°ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</div><div class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure><p>è¿™æ ·æˆ‘ä»¬æ˜¯è·å–åˆ°äº†å€¼ï¼Œä½†æ˜¯è¿™ä¸ªè¿‡ç¨‹åœ¨ç¥ç»ç½‘ç»œä¸­æ— æ³•æ±‚å¯¼å’Œæ–¹å‘ã€‚äºæ˜¯gumbel-maxå‡ºç°äº†ï¼š</p><blockquote><p>å°†é‡‡æ ·çš„è¿‡ç¨‹å…¬å¼åŒ–ï¼Œå…¬å¼ä¸­çš„å‚æ•°ä¸ºç¦»æ•£éšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p></blockquote><p>$$<br>z_{i}=\left\{\begin{array}{l}{1, i=\operatorname{argmax}_{j}\left(\log \left(p_{j}\right)+g_{j}\right)} \\ {0, \text { otherwise }}\end{array}\right.<br>$$</p><p>å…¶ä¸­$g_{i}$ä»£è¡¨gumbelå™ªå£°ï¼Œ$g_{i}=-\log \left(-\log \left(u_{i}\right)\right), u_{i} \sim U n i f o r m(0,1)$ã€‚è¾“å‡º$z_i$æ˜¯ä¸€ä¸ª$j$ç»´çš„one-hotå‘é‡ã€‚</p><p>ç”±äºargmaxä¸å¯å¯¼ï¼Œç”¨å¯å¯¼çš„softmaxæ›¿ä»£argmax<br>$$<br>\boldsymbol{z}=\operatorname{softmax}((\log (\boldsymbol{p})+\boldsymbol{g}) / \tau)<br>$$<br>å‚æ•°$ \tau$è¶Šå°ï¼Œ$z$è¶Šæ¥è¿‘one-hotå‘é‡ã€‚</p><blockquote><p>æˆ‘ä»¬æŠŠä¸å¯å¯¼çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»xæœ¬èº«è½¬å«åˆ°äº†æ±‚å–xçš„å…¬å¼ä¸­çš„ä¸€é¡¹gä¸Šé¢ï¼Œè€Œgä¸ä¾èµ–äºæ¦‚ç‡åˆ†å¸ƒpã€‚è¿™æ ·ä¸€æ¥ï¼Œxå¯¹pä»ç„¶æ˜¯å¯å¯¼çš„ï¼Œè€Œæˆ‘ä»¬å¾—åˆ°çš„xä»ç„¶æ˜¯ç¦»æ•£å€¼çš„é‡‡æ ·ã€‚è¿™æ ·çš„é‡‡æ ·è¿‡ç¨‹è½¬å«çš„æŠ€å·§å«å†å‚åŒ–æŠ€å·§(reparameterization trick)</p></blockquote><p>é‚£ä¹ˆç½‘ç»œæœ‰å“ªäº›åœ°æ–¹éœ€è¦é‡‡æ ·å‘¢ï¼Ÿæ¥ä¸‹æ¥äº†è§£ä¸€ä¸‹VAEçš„ç›¸å…³åº”ç”¨ã€‚</p><h3 id="ç›¸å…³åº”ç”¨"><a href="#ç›¸å…³åº”ç”¨" class="headerlink" title="ç›¸å…³åº”ç”¨"></a>ç›¸å…³åº”ç”¨</h3><p><strong>å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨VAE</strong></p><p><a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="external">è¿™ç¯‡åšå®¢</a> è§£é‡Šå¾—å¾ˆå¥½ï¼Œè‡ªåŠ¨ç¼–ç å™¨ç”±ç¼–ç å™¨(encoder, E)å’Œè§£ç å™¨(decoder, D)æ„æˆï¼ŒEå¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆéšå‘é‡ï¼Œ Då¯¹éšå‘é‡è¿›è¡Œè§£ç ï¼Œè¾“å‡ºå›¾åƒã€‚</p><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165000500-1207992534.jpg" alt="img"></p><p>ä½†æ˜¯è¿™æ ·æˆ‘ä»¬å¿…é¡»é€šè¿‡å›¾åƒæ¥ç”Ÿæˆéšå‘é‡ï¼Œå±€é™æ€§è¾ƒå¤§ï¼Œå¯ä¸å¯ä»¥éšä¾¿æ¥ä¸€ä¸ªéšå‘é‡ï¼Œè¾“å…¥è¿›Då°±èƒ½ç”Ÿæˆå›¾ç‰‡å‘¢ï¼Ÿäºæ˜¯VAEå°±å‡ºç°äº†ã€‚</p><blockquote><p>é™åˆ¶ç¼–ç å™¨ç”Ÿæˆæœä»å•å…ƒé«˜æ–¯åˆ†å¸ƒçš„éšå‘é‡ã€‚</p></blockquote><p>å› æ­¤å­¦ä¹ ç›®æ ‡å°±å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š1ï¼‰ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒå°½å¯èƒ½æ¥è¿‘ï¼› 2ï¼‰éšå˜é‡æœä»å•å…ƒé«˜æ–¯åˆ†å¸ƒ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">generation_loss = mean(square(generated_image - real_image))  </div><div class="line">latent_loss = KL-Divergence(latent_variable, unit_gaussian)  </div><div class="line">loss = generation_loss + latent_loss</div></pre></td></tr></table></figure><p>ä¸ºäº†ä¼˜åŒ–KLæ•£åº¦ï¼Œéœ€è¦å¼•å…¥ğŸ‘†ğŸ»æåˆ°è¿‡çš„å†å‚åŒ–æŠ€å·§ã€‚</p><blockquote><p>Eä¸ç›´æ¥ç”Ÿæˆéšå‘é‡ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªå‡å€¼å‘é‡å’Œä¸€ä¸ªæ–¹å·®å‘é‡ã€‚å†é€šè¿‡å‡å€¼å’Œæ–¹å·®é‡‡æ ·å‡ºéšå‘é‡ã€‚</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165407236-1369432498.png" alt="img"> </p><p>è‡³æ­¤æˆ‘ä»¬æ˜ç™½äº†<strong>é‡‡æ ·</strong>æ˜¯ä¸ºäº†è®©æ•°æ®å°½å¯èƒ½æœä»æŸä¸€åˆ†å¸ƒï¼Œé€šè¿‡<strong>å†å‚åŒ–æŠ€å·§</strong>æ¥å­¦ä¹ è¿™ä¸ªåˆ†å¸ƒçš„å‚æ•°ã€‚é«˜æ–¯åˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œç›´æ¥å¯æ±‚å¯¼ï¼Œé‚£ä¸€äº›ä¸è¿ç»­çš„ç¦»æ•£åˆ†å¸ƒæ€ä¹ˆåŠå‘€ï¼Ÿè¿™å°±å›åˆ°äº†ä¸€å¼€å§‹çš„é—®é¢˜ã€ŒGumbel Sampling Trickã€ã€‚</p><p><strong>åˆ†ç±»å†å‚åŒ–(Categorical reparameterization)</strong></p><p>ICLR 2017çš„<a href="https://arxiv.org/pdf/1611.01144.pdf" target="_blank" rel="external">è¿™ç¯‡æ–‡ç« </a> å°±åˆ©ç”¨Gumbel-Softmaxåˆ†å¸ƒï¼Œå°†ç¦»æ•£çš„åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·è¿‡ç¨‹è½¬åŒ–ä¸ºäº†å¯æ±‚å¯¼çš„è¿‡ç¨‹ã€‚</p><p><img src="https://i.loli.net/2019/05/30/5cef4476d308a17728.png" alt=""></p><p>ä¸Šå›¾åæ˜ äº†å‚æ•°$ \tau$å¯¹è¿ç»­æ¦‚ç‡åˆ†å¸ƒ(a)å’Œç¦»æ•£çš„one-hotç±»åˆ«åˆ†å¸ƒçš„å½±å“ã€‚å½“$ \tau$å¤ªå°æ—¶ä¼šå¯¼è‡´æ¢¯åº¦çš„æ–¹å·®è¿‡å¤§ï¼Œæ‰€ä»¥æ–‡ç« åœ¨å®éªŒä¸­ç”¨äº†é€€ç«çš„ç­–ç•¥æ¥é€æ¸å‡å°å‚æ•°$ \tau$ã€‚è¿˜å¯ä»¥åˆ©ç”¨ç†µæ­£åˆ™æ¥å­¦ä¹ $\tau$ï¼Œè‡ªåŠ¨è°ƒæ•´Gumbel-Softmaxåˆ†å¸ƒé‡‡æ ·çš„ç½®ä¿¡åº¦ã€‚</p><p>æœ¬æ–‡çš„è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨Straight-Through (ST) Gumbel Estimatorï¼Œå³å‰å‘ç”¨argmaxï¼Œæ¢¯åº¦å›ä¼ æ—¶ç”¨softmaxçš„æ¢¯åº¦ã€‚</p><p><strong>å‚è€ƒé“¾æ¥ï¼š</strong></p><ul><li><p><a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">Gumbel-Softmax Trickå’ŒGumbelåˆ†å¸ƒ</a></p></li><li><p><a href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/" target="_blank" rel="external">The Gumbel-Max Trick for Discrete Distributions</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;åœ¨PAGé‡Œå‘ç°äº†Gumbel Sampling Trickï¼ŒæŠŠç¦»æ•£çš„é‡‡æ ·è¿‡ç¨‹ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥ï¼Œäºæ˜¯å¯ä»¥æ”¾è¿›ç¥ç»ç½‘ç»œä¸­è¿›è¡Œæ±‚å¯¼å’Œåå‘ï¼Œè§‰å¾—æ˜¯å¾ˆæœ‰æ„æ€çš„å·¥ä½œï¼Œæƒ³è¦å¤šåŠ æ·±ä¸€äº›äº†è§£ã€‚&lt;/p&gt;
&lt;h3 id=&quot;é—®é¢˜å¼•å…¥&quot;&gt;&lt;a href=&quot;#é—®é¢˜å¼•å…¥&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>weekly-paper-04</title>
    <link href="http://yoursite.com/2019/05/25/weekly-paper-04/"/>
    <id>http://yoursite.com/2019/05/25/weekly-paper-04/</id>
    <published>2019-05-25T08:30:05.000Z</published>
    <updated>2019-05-27T12:12:52.191Z</updated>
    
    <content type="html"><![CDATA[<p>ä¸ºäº†é”»ç‚¼è‡ªå·±çš„è‹±è¯­å†™ä½œèƒ½åŠ›ï¼Œä»¥åå°½é‡ç”¨è‹±æ–‡åšè¿›è¡Œå½’çº³ï¼ˆâŒ˜+C &amp; âŒ˜+Vï¼‰ï½</p><h3 id="1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression"><a href="#1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression" class="headerlink" title="1ï¸âƒ£ To prune, or not to prune: exploring the efficacy of pruning for model compression"></a>1ï¸âƒ£ To prune, or not to prune: exploring the efficacy of pruning for model compression</h3><p>è¿™ç¯‡æ˜¯TensorFlowè‡ªå·±å‡ºçš„ï¼Œç›´æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èåˆL1å‰ªæã€‚é€šè¿‡å°†æ“ä½œèå…¥TensoFlowçš„training graphï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æƒé‡è¿›è¡Œæ’åºï¼Œç”¨ä¸€ä¸ªmaskå°†æœ€å°çš„weightsç½®0ã€‚ä»inital sparsity values $s_i$å¼€å§‹ï¼Œä»¥$\Delta t$ çš„å‰ªæé¢‘ç‡ï¼Œæœ€ç»ˆè¾¾åˆ°final sparsity value $s_f$<br>$$<br>s_{t}=s_{f}+\left(s_{i}-s_{f}\right)\left(1-\frac{t-t_{0}}{n \Delta t}\right)^{3} \text { for } t \in\left\{t_{0}, \quad t_{0}+\Delta t, \ldots, t_{0}+n \Delta t\right\}<br>$$<br>masksæ¯éš”$\Delta t$æ›´æ–°ä¸€æ¬¡ï¼Œç›´åˆ°è¾¾åˆ°$s_f$åä¸å†æ›´æ–°ã€‚åŒæ—¶æ–‡ç« è¡¨æ˜ï¼Œ$n$çš„é€‰æ‹©ä¸å­¦ä¹ ç‡çš„ä¸‹é™ç­–ç•¥å¯†åˆ‡ç›¸å…³ã€‚</p><h3 id="2ï¸âƒ£-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS"><a href="#2ï¸âƒ£-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS" class="headerlink" title="2ï¸âƒ£ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS"></a>2ï¸âƒ£ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</h3><p>ğŸ“<a href="https://github.com/metalbubble/cnnvisualizer" target="_blank" rel="external">Github Repo</a></p><p><strong>Contributions</strong></p><ul><li>object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet</li><li>the same network can do both object localization and scene recognition in a single forward-pass.</li></ul><p><strong>Experiments</strong></p><ul><li><p>identify the differences in the type of images preferred at the different layers of each network</p></li><li><p>Places-CNN and ImageNet-CNN  prefer similar images in the earlier layers, while the later layers tend to be more specialized to the specific task of scene or object categorization.</p></li><li><p>understand the nature of the representation that the network is learning</p><ul><li><em>simplifying the input images:</em> 1) removing segments from the image to produce the smallest decrease of the correct classification score until the image is incorrectly classified 2) generate the minimal image representations using image set of SUN database. =&gt; use minimal image representations as inputs to show the contribute important information for the network to recognize the scene.</li><li><em>visualize the receptive fields (RFs) of units and their activatoin patterns:</em> use sliding-window to identify which regions of the image led to the high unit activations. =&gt; as the layers go deeper the RF size gradually increases and the activation regions become more semantically meaningful.</li><li><em>understan and quantify the precise semantic learnd by each unit: </em>ask AMT to indentify the common concepts that exists between the top scoring segmentations for each unit.</li></ul></li><li><p>emergence of objects as the internal representation</p><ul><li><p>what object classes emerge? =&gt; use pool5 to show the distribution of objects</p></li><li><p>why do those obejcts emerge? </p><ul><li><p>possibility 1:  the objects correspond to the most frequent ones in the database. (correlation is 0.54)</p></li><li><p>possibility 2:  the objects that allow discriminatin among scene categories. (correlation is 0.84)</p><p>=&gt; the network is automatically identifying the most discriminative object categories to a large extent</p></li></ul></li></ul></li></ul><h3 id="3ï¸âƒ£-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations"><a href="#3ï¸âƒ£-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations" class="headerlink" title="3ï¸âƒ£ Network Dissection: Quantifying Interpretability of Deep Visual Representations"></a>3ï¸âƒ£ Network Dissection: Quantifying Interpretability of Deep Visual Representations</h3><p>ğŸ“<a href="https://github.com/CSAILVision/NetDissect-Lite" target="_blank" rel="external">Github Repo</a></p><p><strong>Questions</strong></p><ul><li>What is a disentangled representation, and how can its factors be quantified and detected?</li><li><p>Do interpretable hidden units reflect a special alignment of feature space, or are interpretations a chimera?</p></li><li><p>What conditions in state-of-the-art training lead to representations with greater or lesser entanglement?</p></li></ul><p><strong>Measurement of interpretability: three-step process of Network Dissection</strong></p><ol><li><p>Identify a broad set of human-labeld visual concepts.</p></li><li><p>Gather hidden variablesâ€™ response to known concepts.</p><ul><li>draw concepts $c$ from the Broden dataset.</li></ul></li><li><p>Quantify alignment of hidden variable â€” concept pairs.</p><ul><li><p>Scoring Unit Interpretability</p><p>input image $x$, activation map $A_{k}(\mathbf{x}) \stackrel{scale up}{\longrightarrow}S_k(x) $ï¼Œindividual unit activations $a_k$</p><p>top quantile level $T_k$ï¼š $P\left(a_{k}&gt;T_{k}\right)=0.005$</p><p>binary segmentationï¼š$M_{k}(\mathbf{x}) \equiv S_{k}(\mathbf{x}) \geq T_{k}$</p><p>input annotaion mask $L_c$ </p><p>scoreï¼šthe accuracy of unit $k$ in detecting concept $c$<br>$$<br>I o U_{k, c}=\frac{\sum\left|M_{k}(\mathbf{x}) \cap L_{c}(\mathbf{x})\right|}{\sum\left|M_{k}(\mathbf{x}) \cup L_{c}(\mathbf{x})\right|}<br>$$</p></li></ul></li></ol><h3 id="4ï¸âƒ£-Pixel-wise-Attentional-Gating-for-Scene-Parsing"><a href="#4ï¸âƒ£-Pixel-wise-Attentional-Gating-for-Scene-Parsing" class="headerlink" title="4ï¸âƒ£ Pixel-wise Attentional Gating for Scene Parsing"></a>4ï¸âƒ£ Pixel-wise Attentional Gating for Scene Parsing</h3><p><strong>Contributions:</strong></p><ul><li>Dynamic computation depth: insert PAG at multiple lyaers of ResNet to control computational parsimony.</li><li>Dynamic spatial pooling: adaptively chooses the proper pooling size for each pixel to aggregate information for inference.</li><li>Experimetns on various pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation.</li></ul><p><img src="https://i.loli.net/2019/05/27/5ceb52642beaa24177.png" alt=""></p><p>binary spatial mask $\mathbf{G}$ on ResBottleneck:<br>$$<br>\begin{array}{ll}{\mathbf{X}=\mathcal{F}^{1}(\mathbf{I})} &amp; {\mathbf{X}=\mathcal{F}^{1}(\mathbf{I}), \mathbf{G}=\mathcal{G}(\mathbf{I})} \\ {\mathbf{Y}=\mathcal{F}^{2}(\mathbf{X})} &amp; {\mathbf{Y}=\mathcal{F}_{\mathbf{G}}^{2}(\mathbf{X})} \\ {\mathbf{Z}=\mathcal{F}^{3}(\mathbf{Y})} &amp; {\mathbf{Z}=\mathcal{F}_{\mathbf{G}}^{3}(\overline{\mathbf{G}} \odot \mathbf{X}+\mathbf{G} \odot \mathbf{Y})} \\ {\mathbf{O}=\mathbf{I}+\mathbf{Z}} &amp; {\mathbf{O}=\mathbf{I}+\mathbf{Z}}\end{array}<br>$$<br><strong>Methods:</strong></p><ul><li><p>Learning attention maps</p><blockquote><p>The key to the proposed PAG is the gating function G that produces a discrete (binary) mask which allows for reduced computation. However, producing the binary mask using hard thresholding is non-differentiable, and thus cannot be simply incorporated in CNN where gradient descent is used for training. To bridge the gap, we exploit the Gumbel-Max trick [19] and its recent continuous relaxation [39, 28].</p></blockquote><p>Gumbel distribution  $m \equiv-\log (-\log (u))$, where $u \sim \mathcal{U}[0,1]$</p><p>$g$ is a discrete random variable with probabilities  $P(g=k) \propto a_{k}$</p><p>$\left\{m_{k}\right\}_{k=1, \dots, K}$ is a sequence of i.i.d Gumbel random variables </p><p>sample from the discrete variable:<br>$$<br>g=\underset{k=1, \ldots, K}{\operatorname{argmax}}\left(\log \alpha_{k}+m_{k}\right)<br>$$<br>Gumbel Sampling Trick (replaces the argmax operation with a softmax): </p></li></ul><p>$$<br>\mathbf{g}=\operatorname{softmax}((\log (\boldsymbol{\alpha})+\mathbf{m}) / \tau)<br>$$</p><p>â€‹        <strong>forwardd pass</strong>: discrete smaples of the argmax </p><p>â€‹        <strong>backward pass</strong>: compute gradient of the softmax relaxation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ä¸ºäº†é”»ç‚¼è‡ªå·±çš„è‹±è¯­å†™ä½œèƒ½åŠ›ï¼Œä»¥åå°½é‡ç”¨è‹±æ–‡åšè¿›è¡Œå½’çº³ï¼ˆâŒ˜+C &amp;amp; âŒ˜+Vï¼‰ï½&lt;/p&gt;
&lt;h3 id=&quot;1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compressi
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.03</title>
    <link href="http://yoursite.com/2019/05/19/weekly-paper-03/"/>
    <id>http://yoursite.com/2019/05/19/weekly-paper-03/</id>
    <published>2019-05-19T13:09:17.000Z</published>
    <updated>2019-05-23T10:42:12.181Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition"><a href="#1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition" class="headerlink" title="1ï¸âƒ£ On Compressing Deep Models by Low Rank and Sparse Decomposition"></a>1ï¸âƒ£ On Compressing Deep Models by Low Rank and Sparse Decomposition</h3><p>æœ¬æ–‡å°†ç½‘ç»œæƒé‡åˆ†è§£æˆä½ç§©å’Œç¨€ç–çš„æˆåˆ†ï¼Œåˆ©ç”¨è´ªå¿ƒåŒè¾¹åˆ†è§£ï¼ˆGreBdecï¼‰ç®—æ³•è¿›è¡Œæ¨¡å‹å‹ç¼©ã€‚</p><p>ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{rank}(L) \leq r} \\ &amp;card(S) \leq c \end{array}<br>$$<br>å‡è®¾$L=UV$ï¼Œå…¶ä¸­$U \in R^{m \times r}, V \in R^{r \times k}$ã€‚æœ¬æ–‡ç”¨ä¸¤ä¸ªå·ç§¯å±‚è¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œ$V$å°†é€šé“æ•°æ˜ å°„åˆ°$r$ï¼Œ$U$ä»£è¡¨$1\times1$å·ç§¯ã€‚ç„¶åæŠŠä½ç§©è¿‘ä¼¼çš„ç»“æœå’Œç¨€ç–çš„ç»“æœç›¸åŠ åˆ©ç”¨maskä¹˜åˆ°åŸfiltersä¸Šï¼Œä¿®æ”¹ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2 n}|Y-(L+S) X|_{F}^{2}} \\ {\text {s.t.}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2} \leq \gamma} \\ &amp; rank(L) \leq r, \\ &amp; card(S) \leq c.\end{array}<br>$$<br>ç­‰åŒäºåˆ©ç”¨è¿­ä»£ä¼˜åŒ–ç­–ç•¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\frac{1}{2 n}|Y-(L+S) X|_{2}^{2}+\frac{\lambda}{2}|W-L-S|_{F}^{2}<br>$$<br>å…¶ä¸­<br>$$<br>\left\{\begin{array}{l}{L_{i}=\text { TruncatedGSVD }\left(B_{i} A^{\dagger}, r\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>æœ¬æ–‡ç”¨SVD-freeçš„GreBdecç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œä»¤$L=UV$<br>$$<br>\begin{array}{cl}{\min _{U, V, S}} &amp; {\frac{1}{2 n}|Y-(U V+S) X|_{F}^{2}+\frac{\lambda}{2}|W-U V-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{card}(S) \leq c}\end{array}<br>$$<br>$U,V,S$é€šè¿‡ä»¥ä¸‹å…¬å¼æ›´æ–°ï¼š<br>$$<br>\left\{\begin{array}{l}{U_{i}=B_{i} V_{i-1}^{\top}\left(V_{i-1} A V_{i-1}^{\top}\right)^{\dagger}} \\ {V_{i}=\left(U_{i}^{\top} U_{i}\right)^{\dagger} U_{i}^{\top}\left(B_{i} A^{\dagger}\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>ç„¶ååˆç»è¿‡ä¸€ç•ªå˜æ¢ä½œè€…åˆ©ç”¨QRåˆ†è§£å¾—åˆ°ä¸€ä¸ªè®©$U,V$æ›´å¿«æ›´æ–°çš„è§„åˆ™ï¼š<br>$$<br>\left\{\begin{array}{l}{U_{i}=Q, Q R\left(B_{i} V^{\top}\right)=Q R} \\ {V_{i}=Q^{\top}\left(B_{i} A^{\dagger}\right)}\end{array}\right.<br>$$<br><img src="https://i.loli.net/2019/05/20/5ce2015dee80b29804.png" alt=""></p><p>###2ï¸âƒ£ Spatial Transformer Networks</p><p>å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç©ºé—´ä¸Šçš„å˜æ¢ï¼Œä»¥å­¦åˆ°å›¾åƒçš„ä¸å˜æ€§(invariance)ã€‚é…åˆ<a href="https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html" target="_blank" rel="external">PyTorch Tutorial</a>é£Ÿç”¨ã€‚</p><p><img src="https://i.loli.net/2019/05/23/5ce663632bbf215753.png" alt=""></p><p>STNä¹Ÿç±»ä¼¼ä¸€ä¸ªæ’ä»¶ï¼Œä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼š</p><ul><li>Localisation netï¼šè¾“å…¥feature map $U \in \mathbb{R}^{H \times W \times C}$ï¼Œè¾“å‡ºå˜æ¢å‚æ•°$\theta=f_{\mathrm{loc}}(U)$ã€‚å…¶ä¸­$\theta$æ˜¯ä¸€ä¸ª6ç»´çš„ä»¿å°„å˜æ¢ã€‚</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer localization-network</span></div><div class="line">self.localization = nn.Sequential(</div><div class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>)</div><div class="line">)</div></pre></td></tr></table></figure><ul><li>Grid generatorï¼šå¯¹å›¾åƒç”¨$A_\theta$è¿›è¡Œ2Dä»¿å°„å˜æ¢ï¼Œå…¶ä¸­$(x_i^t, y_I^t)$ä¸ºtargetåƒç´ ç‚¹åæ ‡ï¼Œ$\left(x_{i}^{s}, y_{i}^{s}\right)$ä¸ºsourceé‡‡æ ·ç‚¹çš„åæ ‡ã€‚</li></ul><p>$$<br>\left( \begin{array}{c}{x_{i}^{s}} \\ {y_{i}^{s}}\end{array}\right)=\mathcal{T}_{\theta}\left(G_{i}\right)=\mathrm{A}_{\theta} \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)=\left[ \begin{array}{ccc}{\theta_{11}} &amp; {\theta_{12}} &amp; {\theta_{13}} \\ {\theta_{21}} &amp; {\theta_{22}} &amp; {\theta_{23}}\end{array}\right] \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Regressor for the 3 * 2 affine matrix</span></div><div class="line">self.fc_loc = nn.Sequential(</div><div class="line">  nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)</div><div class="line">)</div></pre></td></tr></table></figure><p>ä¸ºäº†åœ¨$U$ä¸Šåº”ç”¨ç©ºé—´å˜æ¢è¾“å‡º$V$ï¼Œéœ€è¦ä¸€ä¸ªå¯å¯¼çš„é‡‡æ ·å‡½æ•°ç”Ÿæˆé‡‡æ ·ç‚¹$\mathcal{T}_\theta(G)$ã€‚<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} k\left(x_{i}^{s}-m ; \Phi_{x}\right) k\left(y_{i}^{s}-n ; \Phi_{y}\right) \forall i \in\left[1 \ldots H^{\prime} W^{\prime}\right] \forall c \in[1 \ldots C]<br>$$<br>å…¶ä¸­$k$ä¸ºsampling kernelï¼Œå¯ä»¥å®šä¹‰ä¸ºinteger sampling kernel:<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \delta\left(\left\lfloor x_{i}^{s}+0.5\right\rfloor- m\right) \delta\left(\left\lfloor y_{i}^{s}+0.5\right\rfloor- n\right)<br>$$<br>ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºbilinear sampling kernelï¼š<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|x_{i}^{s}-m\right|\right) \max \left(0,1-\left|y_{i}^{s}-n\right|\right)<br>$$<br>å¯¹è¾“å…¥æ±‚åå¯¼æœ‰ï¼š<br>$$<br>\frac{\partial V_{i}^{c}}{\partial x_{i}^{s}}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|y_{i}^{s}-n\right|\right) \left\{\begin{array}{ll}{0} &amp; {\text { if }\left|m-x_{i}^{s}\right| \geq 1} \\ {1} &amp; {\text { if } m \geq x_{i}^{s}} \\ {-1} &amp; {\text { if } m<x_{i}^{s}}\end{array}\right. $$="" æŠŠ<em="">localisation network, grid generator, samplerç»“åˆèµ·æ¥æ„æˆä¸€ä¸ªSTNæ¨¡å—ï¼š</x_{i}^{s}}\end{array}\right.></p><p><strong>STN</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer network forward function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stn</span><span class="params">(self, x)</span>:</span></div><div class="line">  xs = self.localization(x)</div><div class="line">  xs = xs.view(<span class="number">-1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>)</div><div class="line">  theta = self.fc_loc(xs)</div><div class="line">  theta = theta.view(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line">  grid = F.affine_grid(theta, x.size())</div><div class="line">  x = F.grid_sample(x, grid)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>HEXOä¸»é¢˜cactusä¿®æ”¹</title>
    <link href="http://yoursite.com/2019/05/19/hexo-theme-cactus/"/>
    <id>http://yoursite.com/2019/05/19/hexo-theme-cactus/</id>
    <published>2019-05-19T03:05:14.000Z</published>
    <updated>2019-05-19T08:55:36.409Z</updated>
    
    <content type="html"><![CDATA[<p>cacutsçš„ä¸»é¢˜å¾ˆç®€æ´ï¼Œç”¨å¾—è›®ä¹…ï¼Œçœ‹åˆ°åŸåº“æœ‰æ›´æ–°ï¼Œæ‰€ä»¥forkäº†æ–°çš„ç‰ˆæœ¬å¹¶åœ¨ä¸Šé¢åšä¸€äº›ä¿®æ”¹ï¼Œé¡ºä¾¿è®°å½•ä¸€ä¸‹è¿‡ç¨‹ã€‚</p><h3 id="ä¸»é¢˜é•œåƒ"><a href="#ä¸»é¢˜é•œåƒ" class="headerlink" title="ä¸»é¢˜é•œåƒ"></a>ä¸»é¢˜é•œåƒ</h3><p>é¦–å…ˆæ ¹æ®<a href="https://help.github.com/en/articles/duplicating-a-repository" target="_blank" rel="external">Mirrow a repository</a>é•œåƒä¸€ä¸ªåº“ã€‚åœ¨pushçš„æ—¶å€™è¿˜é‡åˆ°äº†403é—®é¢˜ï¼š</p><blockquote><p>remote: Permission to colorjam/hexo-theme-cactus-mirrored.git denied to xxx</p></blockquote><p>é€šè¿‡åˆ é™¤<strong>Keychain Access</strong>ä¸­å­˜å‚¨çš„github.comçš„Internet passwordå¾—åˆ°è§£å†³ã€‚ç„¶åæŠŠè‡ªå·±çš„åº“å†Cloneè¿›<code>themes</code>ä¸­</p><h3 id="æ ·å¼ç¼–è¾‘"><a href="#æ ·å¼ç¼–è¾‘" class="headerlink" title="æ ·å¼ç¼–è¾‘"></a>æ ·å¼ç¼–è¾‘</h3><ul><li><p>ä¸»é¢˜é¢œè‰²</p><p>åœ¨<code>source/css/_colors</code>ä¸‹æ–°å»ºäº†ä¸€ä¸ª<code>pink.styl</code>ï¼ŒåŒæ—¶ä¿®æ”¹<code>_config.yml</code>ä¸­çš„<code>colorscheme:pink</code>ã€‚</p></li><li><p>logoè®¾ç½®</p><p>æŠŠ<code>source/images/</code>ä¸‹çš„<code>favicon.ico</code>å’Œ<code>logo.png</code>æ¢æˆè‡ªå·±å–œæ¬¢çš„å›¾ç‰‡ã€‚ä¿®æ”¹<code>source/css/_partial/header.styl</code>ä¸­çš„<code>#logo</code> çš„<code>background-size: contain</code></p></li><li><p>ç»†èŠ‚è°ƒæ•´</p><p>åˆ é™¤<code>header.styl</code>ä¸­htmlçš„<code>border-top</code></p><p>é“¾æ¥æ ·å¼ï¼š</p></li></ul><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">a</span></div><div class="line">  color: $color-text</div><div class="line">  <span class="selector-tag">text-decoration</span>: <span class="selector-tag">none</span></div><div class="line"></div><div class="line">  &amp;<span class="selector-pseudo">:hover</span></div><div class="line">  background-image: linear-gradient(transparent, transparent 4px, $color-link 4px, $color-link)</div><div class="line">  <span class="selector-tag">background-position</span>: <span class="selector-tag">bottom</span></div><div class="line">  <span class="selector-tag">background-size</span>: 100% 6<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background-repeat</span>: <span class="selector-tag">repeat-x</span></div></pre></td></tr></table></figure><p>â€‹    è¡Œå†…ä»£ç æ ·å¼ï¼š</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">code</span></div><div class="line">  <span class="selector-tag">padding</span>: 0 5<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background</span>: <span class="selector-id">#f6f8fa</span></div><div class="line">  <span class="selector-tag">border-radius</span>: 2<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">-webkit-border-radius</span>: 2<span class="selector-tag">px</span></div></pre></td></tr></table></figure><h3 id="ä¼šåŠ¨çš„ç²’å­"><a href="#ä¼šåŠ¨çš„ç²’å­" class="headerlink" title="ä¼šåŠ¨çš„ç²’å­"></a>ä¼šåŠ¨çš„ç²’å­</h3><p>åœ¨èƒŒæ™¯åŠ ä¸Š<a href="https://github.com/VincentGarreau/particles.js/" target="_blank" rel="external">ä¼šåŠ¨çš„ç²’å­</a>ï¼Œåœ¨<code>source/lib</code>é‡Œåˆ›å»ºä¸€ä¸ªparticlesæ–‡ä»¶å¤¹ï¼ŒæŠŠ<code>particles.min.js</code>æ”¾è¿›å»ã€‚</p><p>åœ¨<code>layout.ejs</code>ä¸­åŠ å…¥</p><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"particles-js"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure><p>åœ¨<code>scripts.ejs</code>ä¸­æ·»åŠ è„šæœ¬ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;!-- particles --&gt;</div><div class="line">&lt;%- js(&apos;lib/particles/particles.min&apos;) %&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">particlesJS(&apos;particles-js&apos;, &#123;</div><div class="line">        ...</div><div class="line">        &#125;</div><div class="line">      )</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure><p>åœ¨<code>style.css</code>ä¸­æ·»åŠ æ ·å¼ï¼š</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="selector-id">#particles-js</span> &#123;</div><div class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</div><div class="line">  <span class="attribute">position</span>: absolute;</div><div class="line">  <span class="attribute">margin-left</span>: -<span class="number">28%</span>;</div><div class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cacutsçš„ä¸»é¢˜å¾ˆç®€æ´ï¼Œç”¨å¾—è›®ä¹…ï¼Œçœ‹åˆ°åŸåº“æœ‰æ›´æ–°ï¼Œæ‰€ä»¥forkäº†æ–°çš„ç‰ˆæœ¬å¹¶åœ¨ä¸Šé¢åšä¸€äº›ä¿®æ”¹ï¼Œé¡ºä¾¿è®°å½•ä¸€ä¸‹è¿‡ç¨‹ã€‚&lt;/p&gt;
&lt;h3 id=&quot;ä¸»é¢˜é•œåƒ&quot;&gt;&lt;a href=&quot;#ä¸»é¢˜é•œåƒ&quot; class=&quot;headerlink&quot; title=&quot;ä¸»é¢˜é•œåƒ&quot;&gt;&lt;/a&gt;ä¸»é¢˜é•œåƒ&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
</feed>
