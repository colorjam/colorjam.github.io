<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-01T05:53:21.563Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/08/01/Quantization/"/>
    <id>http://yoursite.com/2019/08/01/Quantization/</id>
    <published>2019-08-01T05:50:24.083Z</published>
    <updated>2019-08-01T05:53:21.563Z</updated>
    
    <content type="html"><![CDATA[<h2 id="é‡åŒ–æ–¹æ³•"><a href="#é‡åŒ–æ–¹æ³•" class="headerlink" title="é‡åŒ–æ–¹æ³•"></a>é‡åŒ–æ–¹æ³•</h2><h3 id="Accurate-and-Efficient-2-Bit-Quantized-Neural-Netowrks"><a href="#Accurate-and-Efficient-2-Bit-Quantized-Neural-Netowrks" class="headerlink" title="Accurate and Efficient 2-Bit Quantized Neural Netowrks"></a>Accurate and Efficient 2-Bit Quantized Neural Netowrks</h3><p><strong>PACT</strong></p><p>PAramaterized Clipping acTivationï¼ˆå‚æ•°åŒ–æˆªç•¥å¼æ¿€æ´»ï¼‰ï¼Œå¯¹ReLUçš„è¾“å‡ºè¿›è¡Œæˆªç•¥(clipping)ï¼Œé™åˆ¶è¾“å‡ºèŒƒå›´å†$[0, \alpha]$ã€‚ä¸€ä¸ªæš—æ‹è¿‡åªæœ‰ä¸€ä¸ª$\alpha$å€¼ï¼Œé€šè¿‡SGDå¯¹$\alpha$å€¼è¿›è¡Œæ›´æ–°ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;é‡åŒ–æ–¹æ³•&quot;&gt;&lt;a href=&quot;#é‡åŒ–æ–¹æ³•&quot; class=&quot;headerlink&quot; title=&quot;é‡åŒ–æ–¹æ³•&quot;&gt;&lt;/a&gt;é‡åŒ–æ–¹æ³•&lt;/h2&gt;&lt;h3 id=&quot;Accurate-and-Efficient-2-Bit-Quantized-Neural-Netowrks&quot;&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/08/01/Multi-task%20Learning/"/>
    <id>http://yoursite.com/2019/08/01/Multi-task Learning/</id>
    <published>2019-08-01T03:16:59.168Z</published>
    <updated>2019-08-07T02:46:08.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h1><p>å¤šä»»åŠ¡åŒæ—¶è¿›è¡Œä¸åˆ†ä¸»æ¬¡ï¼Œå¤šä¸ªç›¸å…³çš„ä»»åŠ¡æ”¾åœ¨ä¸€èµ·å­¦ä¹ ï¼Œä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†å…±äº«å’Œå…±åŒå­¦ä¹ ã€‚</p><ul><li><p>Cross-stitch Networks for Multi-task Learning </p><ul><li>æå‡ºä¸€ä¸ªâ€cross-stitchâ€å•å…ƒï¼Œå­¦ä¹ çš„<strong>activation</strong>ä¹‹é—´çš„çº¿æ€§æ˜ å°„ï¼ŒæœŸæœ›æ‰¾åˆ°æœ€ä¼˜çš„share / task-specificç‰¹å¾ç»„åˆã€‚</li><li>æ–¹æ³•ï¼šåŸºäºä¸€ä¸ªAlexNetï¼ˆone-task networkï¼‰ï¼Œç„¶ååœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šåˆ†åˆ«è¿›è¡Œfinetuneè·å¾—ç½‘ç»œAå’ŒBï¼Œå¼•å…¥corss-stitchingã€‚</li><li>æ•°æ®é›†å’Œä»»åŠ¡ï¼š<ul><li>Semantic segmentationï¼ˆSemSegï¼‰and Surface Normal Predictionï¼ˆSNï¼‰on NYU-v2 </li><li>object detection and attribute prediction on PASCAL VOC 2008</li></ul></li><li>å®éªŒï¼š<ul><li>åˆå§‹å€¼$\alpha$çš„å½±å“ï¼Œone-task / ensembleä¸¤ä¸ªç½‘ç»œ / split architecture / MTL-shared</li></ul></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803113347827.png" alt="image-20190803113347827"></p><blockquote><p>åªè€ƒè™‘æƒé‡ï¼Œæ²¡æœ‰è€ƒè™‘ç½‘ç»œç»“æ„çš„å½±å“ã€‚è€Œä¸”å¢åŠ äº†æ¨¡å‹å¤§å°ã€‚</p></blockquote></li><li><p><strong>DAN</strong>: Incremental Learning Through Deep Adaptation (ICLR2018)</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803113821256.png" alt="image-20190803113821256"></p></li><li><ul><li><p>å‡è®¾åŸºäºçš„æ˜¯ä¿æŒç½‘ç»œç»“æ„ä¸å˜ï¼Œåœ¨T1ä»»åŠ¡ä¸Šè®­ç»ƒå¥½çš„ç½‘ç»œNï¼Œé€šè¿‡æ”¹å˜ç½‘ç»œæƒé‡èƒ½å¤Ÿè¿ç§»åˆ°T2ä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼šåŸºäºVGG-Bï¼Œå¼•å…¥controller modulesï¼Œå°è£…äº†åŸå…ˆçš„å·ç§¯å±‚ï¼Œå¯¹åŸå§‹æƒé‡åšäº†ä¸€ä¸‹çº¿æ€§å˜æ¢ï¼Œæ¯ä¸ªä»»åŠ¡æœ‰ä¸€ä¸ªäºŒå€¼å˜é‡$\alpha$ï¼Œæ§åˆ¶é€‰æ‹©åŸå§‹æƒé‡oræ–°çš„æƒé‡ã€‚</p></li><li><p>æ•°æ®é›†å’Œä»»åŠ¡ï¼šCaltech-256, CIFAR-10, Daimler, GTSR, Omniglot, Plankton imagery data, Human Sketch dataset, SVHN</p></li><li><p>å®éªŒï¼š</p><ul><li>control-moduleä¸­Wçš„åˆå§‹åŒ–æ–¹å¼</li><li>base networkçš„é€‰æ‹©</li><li>Visual Decathlon Challengeï¼š</li></ul><blockquote><p>ä¸åŒäºcross-stitchçš„jointly learningï¼Œè€Œæ˜¯one-by-oneï¼Œåœ¨ä¸€ä¸ªç½‘ç»œçš„åŸºç¡€ä¸Šè®­ç»ƒå‡ºå¦å¤–ä¸€ä¸ªã€‚</p></blockquote></li></ul></li></ul><ul><li><p>Learning multiple visual domains with residual adaptersï¼ˆNIPS2017ï¼‰</p></li><li><p>Efficient parametrization of multi-domain deep neural networksï¼ˆCVPR2018ï¼‰</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190805213652297.png" alt="image-20190805213652297"></p></li></ul><ul><li><ul><li>å’ŒğŸ‘†ä¸€ç¯‡æ˜¯åŒä¸€ä¸ªä½œè€…ï¼Œä¸åŒçš„æ˜¯ä¸€ä¸ªä¸Šä¸€ç¯‡ç”¨ä¸²è¡Œçš„adapatersï¼Œè¿™ç¯‡ç”¨å¹¶è¡Œçš„ã€‚</li><li>æ–¹æ³•ï¼šåŸºäºResNetç»“æ„å¼•å…¥residual adaptersï¼Œå¼•å…¥è¾ƒå°‘çš„å‚æ•°å¯¹featureè¿›è¡Œäº†å˜æ¢ã€‚</li><li>å®éªŒï¼š<ul><li>adaptersçš„ä½ç½®ï¼ˆearly / mid / lateï¼‰</li><li>å’Œåœ¨å„è‡ªä»»åŠ¡ä¸Šè·å¾—finetuneç½‘ç»œè¿›è¡Œæ¯”è¾ƒ</li></ul></li></ul></li></ul><ul><li>LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING ï¼ˆICLR2018ï¼‰</li></ul><ul><li><p><strong>Piggyback</strong>: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights</p><p><a href="https://github.com/arunmallya/piggyback" target="_blank" rel="external">https://github.com/arunmallya/piggyback</a></p><ul><li>one-by-one learning</li><li>æ¯ä¸ªå·ç§¯æ ¸å­¦ä¹ ä¸€ä¸ªmaskï¼ˆå‰ªæï¼‰ï¼Œmaskçš„å€¼ä¸º0å’Œ1ï¼ˆé‡åŒ–ï¼‰</li><li>æ•°æ®é›†ï¼šCUBS / Stanford Cars / WikiArt / Sketch</li><li>æ–¹æ³•ï¼šä¸æ”¹å˜pretrainedæ¨¡å‹çš„backboneï¼Œå­¦ä¹ binary maskï¼Œè®©å·ç§¯æ ¸ç¨€ç–ä»¥è¾¾åˆ°é€‚åº”æ–°æ•°æ®é›†çš„ç›®çš„ã€‚</li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190807104608124.png" alt="image-20190807104608124"></p></li><li><p><strong>MTAN</strong>: End-to-End Multi-Task Learning with Attention  (cvpr2019)</p><p><a href="https://github.com/lorenmt/mtan" target="_blank" rel="external">https://github.com/lorenmt/mtan</a></p></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190803164905002.png" alt="image-20190803164905002"></p><ul><li><ul><li>jointly learningï¼Œå‡ ä¸ªä»»åŠ¡æ˜¯åŒæ—¶è®­çš„ã€‚</li><li>æœ‰ä¸€ä¸ªbackboneç½‘ç»œä½œä¸ºtask-sharedç½‘ç»œï¼ˆæœ¬æ–‡é‡‡ç”¨SegNetï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡æœ‰å¯¹åº”çš„attention moduleã€‚</li><li>æå‡ºäº†DWAï¼ŒåŠ¨æ€å¹³è¡¡lossç³»æ•°ã€‚</li></ul></li></ul><ul><li><p>Nerttailor (cvpr2019)</p><p><a href="https://github.com/pedro-morgado/nettailor" target="_blank" rel="external">https://github.com/pedro-morgado/nettailor</a></p><p><img src="https://github.com/pedro-morgado/nettailor/raw/master/docs/figs/teaser_row.png" alt="img"></p><ul><li><p>one-by-one learningï¼Œå…ˆåœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒå®Œè¿ç§»åˆ°å¦ä¸€ä¸ªä»»åŠ¡ä¸Šã€‚</p></li><li><p>universarial networkæ˜¯åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šfine-tuneä¸€ä¸ªpre-trainedç½‘ç»œï¼ŒåŒºåˆ«åœ¨äºä¸ä»…æ”¹å˜æƒé‡ï¼Œè¿˜æ”¹å˜äº†ç½‘ç»œçš„ç»“æ„ã€‚soft Attention+NASã€‚</p></li><li><p>åŠ¨æ€æ”¹å˜ç½‘ç»œç»“æ„ï¼Œbackboneæ˜¯åœ¨ä¸€ä¸ªåŸŸä¸Šè®­å¥½çš„ResNetï¼Œé€šè¿‡æœç´¢è¾…åŠ©å•å…ƒã€‚</p></li></ul></li></ul><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190804144750941.png" alt="image-20190804144750941"></p><ul><li><p>Efficient parametrization of multi-domain deep neural networks</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190805171712077.png" alt="image-20190805171712077"></p><ul><li>ç½‘ç»œç»“æ„ç›¸åŒçš„ç½‘ç»œï¼Œç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¸åŒï¼ˆè¾“å…¥ä¸åŒï¼‰</li><li>é€šè¿‡æƒå€¼å…±äº«çš„æ–¹å¼è¿›è¡Œä¸¤ä¸ªåŸŸæ¨¡å‹çš„å‹ç¼©ã€‚é¦–å…ˆæ€¼äº†GrOWL(Group weighted order lasso)çš„æ–¹æ³•ã€‚</li><li><strong>ä¸‰ä¸ªæ•°æ®é›†ï¼š</strong>SUN-RGBD Datasetï¼ˆRGBå›¾åƒå’Œæ·±åº¦å›¾ï¼‰ï¼ŒUCF-101 Datasetï¼ˆYoutube videosï¼‰ï¼ŒHMDB-51 Datasetï¼ˆvideoï¼‰</li><li><strong>ä¸¤ä¸ªä»»åŠ¡ï¼š</strong>RGB-D Scene Classificationï¼šAlex-Netï¼ŒAction Recognition Tasksï¼šVGG-16</li></ul></li></ul><p>weight sharing?</p><p>åŸºäºç‰¹å¾</p><ul><li>ä¸åŒä»»åŠ¡ç‰¹å¾è½¬æ¢ï¼Œå­¦ä¹ ç‰¹å¾ä¹‹é—´çš„çº¿æ€§ç»„åˆï¼ˆCross-stitchã€Deep Adaptation</li><li><p>ç‰¹å¾é€‰æ‹©ï¼Œç¨€ç–ï¼ˆGroup sparsityï¼‰</p></li><li><p>åˆ†è§£ï¼Œä½ç§©åˆ†è§£</p></li></ul><p>åŸºäºä»»åŠ¡èšç±»</p><ul><li>åŠ æƒæœ€è¿‘é‚»åˆ†ç±»å™¨ã€‚é’ˆå¯¹æ¯ä¸ªä»»åŠ¡ï¼Œé€šè¿‡è°ƒæ•´æƒé‡å®ç°æœ€å°åŒ–ç±»å†…è·ç¦»ï¼Œæœ€å¤§åŒ–ç±»é—´è·ç¦»ã€‚æ¯ä¸ªä»»åŠ¡ä¹‹é—´æ„å»ºè½¬åŒ–çŸ©é˜µAï¼Œå…¶ä¸­$a_{ij}$è¡¨ç¤ºä½¿ç”¨ä»»åŠ¡$T_j$çš„åˆ†ç±»å™¨å¯¹ä»»åŠ¡$T_i$æ ·æœ¬è¿›è¡Œåˆ†ç±»çš„ç¹èŠ±ç²¾åº¦ã€‚åŸºäºçŸ©é˜µAï¼Œå°†$m$ä¸ªä»»åŠ¡èšæˆ$r$ä¸ªç°‡ã€‚ä¸€ä¸ªç°‡é‡Œå„ä¸ªä»»åŠ¡çš„æ ·æœ¬å…±äº«ï¼Œæ¯ä¸ªç°‡è®­ç»ƒå‡ºä¸€ä¸ªå…±åŒçš„åŠ æƒæœ€è¿‘é‚»åˆ†ç±»å™¨ã€‚</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Multi-task-Learning&quot;&gt;&lt;a href=&quot;#Multi-task-Learning&quot; class=&quot;headerlink&quot; title=&quot;Multi-task Learning&quot;&gt;&lt;/a&gt;Multi-task Learning&lt;/h1&gt;&lt;p&gt;å¤šä»»
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.10</title>
    <link href="http://yoursite.com/2019/07/26/weekly-paper-10/"/>
    <id>http://yoursite.com/2019/07/26/weekly-paper-10/</id>
    <published>2019-07-26T15:48:10.000Z</published>
    <updated>2019-07-31T13:30:32.838Z</updated>
    
    <content type="html"><![CDATA[<p>### </p><h3 id="1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks"><a href="#1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks" class="headerlink" title="1ï¸âƒ£ Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks"></a>1ï¸âƒ£ Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks</h3><p><strong>æœ¬æ–‡æ˜¯åŸºäºå¹²æ‰°é¡¹æ–¹æ³•çš„å¯è§£é‡Š</strong>ã€‚å¯è§£é‡Šçš„åŒºåŸŸ$\mathbf{e}^__{C_T}$å¯ä»¥åˆ†ä¸º<strong>æœ€å°ä¿ç•™çš„åŒºåŸŸ</strong>å’Œ_*æœ€å°ç§»é™¤åŒºåŸŸ__ï¼Œå‰è€…æ„å‘³ç€è¿™äº›åŒºåŸŸæ˜¯ä¿è¯æ¨¡å‹åˆ†ç±»æ­£ç¡®çš„éƒ¨åˆ†ï¼Œåè€…æ„å‘³ç€è¿™äº›åŒºåŸŸå¿…é¡»ç§»é™¤ä»¥æ”¹å˜æ¨¡å‹è¾“å‡ºã€‚</p><p>æ·»åŠ å¹²æ‰°é¡¹çš„å›¾åƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š$\mathbf{e}=\mathbf{m} \cdot \mathbf{x}+(1-\mathbf{m}) \cdot \mathbf{r}$ï¼Œé€šè¿‡è®­ç»ƒä½¿maskç¨€ç–ã€‚</p><ul><li><p>ä¿ç•™è§£é‡Šï¼š<br>$$<br>\begin{aligned} \mathbf{e}_{c_{T}}^{_} &amp;=\mathbf{m}_{c_{T}}^{_} \cdot \mathbf{x} \\ \mathbf{m}_{c_{T}}^{*} &amp;=\underset{\mathbf{m}_{c_{T}}}{\arg \min }\left\{\varphi\left(y_{x}^{c_{T}}, y_{e}^{c_{T}}\right)+\lambda \cdot\left|\mathbf{m}_{c_{T}}\right|_{1}\right\} \end{aligned}<br>$$<br>å›¾åƒä¸­çš„eåŒºåŸŸï¼Œä¿è¯æ¨¡å‹çš„åˆ†ç±»æ­£ç¡®ã€‚</p></li><li><p>ç§»é™¤è§£é‡Šï¼š<br>$$<br>\begin{aligned} \mathbf{e}_{c_{T}}^{_} &amp;=\mathbf{m}_{c_{T}}^{_} \cdot \mathbf{x} \\ \mathbf{m}_{c_{T}}^{*} &amp;=\underset{\mathbf{m}_{e_{T}}}{\arg \max }\left\{\varphi\left(y_{x}^{c_{T}}, y_{e}^{c_{T}}\right)+\lambda \cdot\left|\mathbf{m}_{c_{T}}\right|_{1}\right\} \end{aligned}<br>$$<br>å›¾åƒä¸­çš„eåŒºåŸŸï¼Œä½¿å¾—æ¨¡å‹åˆ†ç±»é”™è¯¯ã€‚</p></li></ul><h3 id="2ï¸âƒ£-THE-DEEP-WEIGHT-PRIOR"><a href="#2ï¸âƒ£-THE-DEEP-WEIGHT-PRIOR" class="headerlink" title="2ï¸âƒ£ THE DEEP WEIGHT PRIOR"></a>2ï¸âƒ£ THE DEEP WEIGHT PRIOR</h3><p>ã€ICLR2019ã€‘</p><p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯èƒ½å¤Ÿé€šè¿‡æŸä¸ªæ¦‚ç‡åˆ†å¸ƒç”Ÿæˆç½‘ç»œçš„æƒé‡ã€‚å¯ä»¥çœ‹ä½œæ˜¯å¢å¼ºç½‘ç»œåˆå§‹åŒ–çš„ä¸€ç§æ–¹æ³•ã€‚ä»¥å‰è´å¶æ–¯ç¥ç»ç½‘ç»œéƒ½æ˜¯éœ€è¦å¯¹å‚æ•°çš„å…ˆéªŒåˆ†å¸ƒ$p(W)$è¿›è¡Œå‡è®¾ï¼Œé€šå¸¸æ˜¯log-uniformã€‚<br>$$<br>\mathcal{L}(\theta)=\sum_{i=1}^{N} \mathbb{E}_{q_{\theta}(W)} \log p\left(y_{i} | x_{i}, W\right)-D_{\mathrm{KL}}\left(q_{\theta}(W) | p(W)\right) \rightarrow \max _{\theta}<br>$$<br>VAEæ˜¯é€šè¿‡éšå˜é‡$z_i$ä¼°è®¡åéªŒæ¦‚ç‡åˆ†å¸ƒ$q(z_i|x_i)$çš„æ–¹æ³•ã€‚å…¶ä¸­$x_i$æ˜¯ç”Ÿæˆå›¾åƒã€‚<br>$$<br>\mathcal{L}(\theta, \phi)=\sum_{i=1}^{N} \mathbb{E}_{q_{\theta}\left(z_{i} | x_{i}\right)} \log p_{\phi}\left(x_{i} | z_{i}\right)-D_{\mathrm{KL}}\left(q_{\theta}\left(z_{i} | x_{i}\right) | p\left(z_{i}\right)\right) \rightarrow \max _{\theta, \phi}<br>$$<br>æœ¬æ–‡çš„å‡è®¾æ˜¯åŸºäºé¢„è®­ç»ƒçš„ç½‘ç»œå‚æ•°$\hat{p}_{l}(w)$ï¼Œå‚æ•°çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒä¸ºï¼š<br>$$<br>\hat{p}_{l}(w)=\int p\left(w | z ; \phi_{l}\right) p_{l}(z) d z<br>$$<br>å¼•å…¥auxiliary lower bound KL:<br>$$<br>\begin{array}{l}{D_{\mathrm{KL}}(q(W) | \hat{p}(W))=\sum_{l, i, j} D_{\mathrm{KL}}\left(q\left(w_{i j}^{l} | \theta_{i j}^{l}\right) | \hat{p}_{l}\left(w_{i j}^{l}\right)\right) \leq \sum_{l, i, j}\left(-H\left(q\left(w_{i j}^{l} | \theta_{i j}^{l}\right)\right)+\right.} \\ {+\mathbb{E}_{q\left(w_{i j}^{l} | \theta_{i j}^{l}\right)}\left[D_{\mathrm{KL}}\left(r\left(z | w_{i j}^{l} ; \psi_{l}\right) | p_{l}(z)\right)-\mathbb{E}_{r\left(z | w_{i j}^{l} ; \psi_{l}\right)} \log p\left(w_{i j}^{l} | z ; \phi_{l}\right)\right] )=D_{\mathrm{KL}}^{b o u n d}}\end{array}<br>$$<br>è¿™æ ·å°±å’ŒVAEå¯¹ä¸Šï¼Œç”¨VAEçš„encoderä¼°è®¡å‚æ•°çš„å…ˆéªŒæ¦‚ç‡ï¼ˆæ–‡ç« å‡è®¾éšå˜é‡$z_i$æœä»N(0,1)ï¼‰ï¼Œç„¶åç”¨VAEä¼°è®¡ç½‘ç»œå‚æ•°çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åä»è¯¥åˆ†å¸ƒä¸­ç”Ÿæˆç½‘ç»œçš„å‚æ•°ã€‚</p><p><img src="https://i.loli.net/2019/07/27/5d3c4f29da39269692.png" alt=""></p><h3 id="3ï¸âƒ£-DSC-Dense-Sparse-Convolution-for-Vectorized-Inference-of-Convolutional-Neural-Networks"><a href="#3ï¸âƒ£-DSC-Dense-Sparse-Convolution-for-Vectorized-Inference-of-Convolutional-Neural-Networks" class="headerlink" title="3ï¸âƒ£ DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks"></a>3ï¸âƒ£ DSC: Dense-Sparse Convolution for Vectorized Inference of Convolutional Neural Networks</h3><p>ã€CVPR2019ã€‘</p><p>æœ¬æ–‡æ˜¯ä»å¾ˆç°å®çš„è§’åº¦åšå‹ç¼©ï¼ŒåŸºäºå…·ä½“çš„Winograd convolutionçš„å‹ç¼©æ–¹å¼ã€‚</p><p><strong>è®¡ç®—å•å…ƒå‘é‡åŒ–</strong>ï¼šä»ç°å®è§’åº¦æ¥çœ‹ï¼Œä»å†…å­˜ä¸­è¯»å–8-bitæ•´å½¢å’Œ32-bitæµ®ç‚¹å‹çš„èƒ½è€—ç›¸åŒï¼Œä»i7 CPUè¯»å–æ•°æ®64-bitsæ•°æ®å’ŒAltera Arria 10åº¦å»32-bitæ•°æ®çš„èƒ½è€—ç›¸åŒã€‚åªè¯»å–åŒbitæ•°æ®(align data)å¡«å……å¯„å­˜å™¨åªéœ€è¦ä¸€æ¬¡æ“ä½œï¼ŒåŒæ—¶è¯»å–ä¸åŒbitæ•°æ®(unaligned data)åˆ™éœ€è¦ä¸¤æ¬¡æ“ä½œã€‚é€šå¸¸CPUæ•°æ®æµç¼“å­˜å—çš„å¤§å°æ˜¯64bytes (64*8bits)ï¼Œæ„å‘³ç€64x8-bitæ•´å½¢å’Œ 16x32-bitçš„æ•°æ®å¯ä»¥å¹³è¡Œå¡«å……å¯„å­˜å™¨ã€‚</p><p><strong>WInograd convolution</strong>ï¼šåŸºäºWinogradå·ç§¯æ˜¯ç”¨æ›´å¤šçš„åŠ æ³•æ¥å‡å°‘æƒ©ç½šæ“ä½œï¼Œ2D Winograd Convolution F(2x2, 3x3)çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p><p><img src="https://i.loli.net/2019/07/28/5d3cfdb637a8236799.png" alt=""></p><p><strong>Dense-Sparse Convolution</strong></p><p><img src="https://i.loli.net/2019/07/28/5d3cfe28d812326769.png" alt=""></p><p>é’ˆå¯¹Sparse Convolutionï¼ŒæŠŠå·ç§¯æ ¸ç”¨CSRæ ¼å¼å­˜æ”¾ï¼Œè¿›è¡Œdirect sparse convolutionã€‚</p><p>é’ˆå¯¹Sparse-Dense Convolutionï¼Œä½œè€…å…ˆé€šè¿‡ä¸€ä¸ªthresholdåˆ¤æ–­çš„å·ç§¯æ ¸çš„ç¨€ç–ç¨‹åº¦ï¼Œç„¶åç”¨ä¸‹é¢çš„å…¬å¼è¿›è¡Œè®¡ç®—ï¼š</p><p><img src="https://i.loli.net/2019/07/28/5d3cff84895ca39428.png" alt=""></p><h3 id="4ï¸âƒ£-Efficient-Neural-Network-Compression"><a href="#4ï¸âƒ£-Efficient-Neural-Network-Compression" class="headerlink" title="4ï¸âƒ£ Efficient Neural Network Compression"></a>4ï¸âƒ£ Efficient Neural Network Compression</h3><p>ã€CVPR2019ã€‘</p><p>æœ¬æ–‡çš„å‹ç¼©æ–¹æ³•æ˜¯é’ˆå¯¹å·ç§¯æ ¸è¿›è¡Œä½ç§©åˆ†è§£ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°é’ˆå¯¹æ•´ä¸ªç½‘ç»œçš„æœ€ä¼˜rankä»¥è¿›è¡Œå‹ç¼©ï¼ˆç›¸å½“äºé€šé“ratioï¼‰</p><p><img src="https://github.com/Hyeji-Kim/ENC/raw/master/fig/overall2.png" alt="i"></p><p>ä¸¤ç§Layer-wise Accuracy Metrics</p><ul><li><p>PCA energy-basedï¼š$y_{p, l}\left(r_{l}\right)$<br>$$<br>y_{p, l}\left(r_{l}\right)=\frac{\sigma_{l}^{\prime}\left(r_{l}\right)-\sigma_{l}^{\prime}(1)}{\sigma_{l}^{\prime}\left(r_{l}^{\max }\right)-\sigma_{l}^{\prime}(1)}<br>$$<br>å…¶ä¸­ç¬¬$l$å±‚çš„ç§©æ˜¯$r_l$ï¼Œ$\sigma_l(d)$æ˜¯ç»è¿‡åˆ†è§£çš„ç¬¬$d$ä¸ªå¯¹è§’å€¼ï¼Œ$\sigma_{l}^{\prime}\left(r_{l}\right)=\sum^{r_l}_{d=1}\sigma_l(d)$è¡¨ç¤ºå·ç§¯æ ¸åˆ†è§£åå¯¹åº”ç§©çš„å…ƒç´ ä¹‹å’Œï¼Œè¿›è¡Œå½’ä¸€åŒ–ã€‚</p></li><li><p>Measurement-based Metricï¼š$y_{m, l}\left(r_{l}\right)$</p><p>åªæ”¹å˜ç½‘ç»œå±‚$l$çš„ç§©ï¼Œæ‰€è·å¾—çš„æ•´ä½“ç²¾åº¦ã€‚ç”¨VBMFè¿›è¡Œç§©çš„é‡‡æ ·ã€‚</p></li></ul><p>å‡è®¾æ¯å±‚çš„metricæ˜¯ç‹¬ç«‹çš„ï¼Œè”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨ç¤ºç½‘ç»œæ•´ä½“çš„accuracy metricï¼š<br>$$<br>\mathrm{P}(A ; R)=\prod_{l=1}^{L} \mathrm{P}\left(a_{l} ; r_{l}\right)<br>$$<br>ä¸‰ç§Overall accuracy metricï¼š</p><ul><li>Measurement-basedï¼š$A_{m}(R)=\prod_{l=1}^{L} y_{m, l}\left(r_{l}\right)$</li><li><p>PCA-basedï¼š$A_{p}(R)=\prod_{l=1}^{L} y_{p, l}\left(r_{l}\right)$</p></li><li><p>combied metricï¼š$A_{c}(R)=\left\{A_{p}(R) \times \frac{C(R)}{C_{\text {orig}}}\right\}+A_{m}(R)$</p></li></ul><p><strong>ENC-Map</strong>ï¼šåˆ©ç”¨Accuracy-Complexityçš„æ˜ å°„æ¥é€‰æ‹©æ¯å±‚çš„ranké…ç½®ã€‚æ–‡ç« è®¤ä¸ºè®©ç½‘ç»œæ¯å±‚çš„å…·æœ‰ç›¸åŒçš„ç²¾åº¦æŸå¤±ä¸å…·æœ‰ç›¸åŒå‹ç¼©ç‡ç›¸æ¯”ï¼Œæ˜¯æ›´åˆç†çš„å‹ç¼©ç­–ç•¥ã€‚å› æ­¤å‡è®¾åœ¨VBMFç”Ÿæˆçš„rankä¸‹ï¼Œæ¯å±‚çš„metricéƒ½ç›¸åŒï¼š<br>$$<br>R_{e}=R | y_{i, l}\left(r_{l}\right)=y_{i, k}\left(r_{k}\right)<br>$$<br>ç„¶åæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡º$R_e$çš„å¤æ‚åº¦$C(R)=\sum_{l=1}^{L} C_{l}\left(r_{l}\right)=\sum_{l=1}^{L} c_{l} r_{l}$</p><p>äºæ˜¯æœ‰äº†complexityå’Œaccuracyçš„æ˜ å°„ï¼š$f_{C-A}ï¼š \mathbb{R} \rightarrow \mathbb{R}$ï¼Œè¿›ä¸€æ­¥å¾—åˆ°complexityå’Œaccuracyåˆ°rankçš„æ˜ å°„ï¼š$f_{C-R}ï¼š\mathbb{R} \rightarrow \mathbb{R}^L$ã€‚</p><p><strong>ENC-Model/Inf</strong>ï¼šå°†æ‰©å±•ENC-Mapè‡³rankçš„ç»„åˆé—®é¢˜ï¼Œéœ€è¦æœç´¢åˆé€‚çš„rankï¼Œé€šè¿‡1. åˆ©ç”¨å·²çŸ¥å¤æ‚åº¦æ¥é™åˆ¶ 2. æŠŠé•¿å¾—å·®ä¸å¤šçš„çš„å·ç§¯æ ¸çš„rankåˆ†åˆ°ä¸€ç»„ã€‚</p><h3 id="5ï¸âƒ£-ECC-Platform-Independent-Energy-Constrained-Deep-Neural-Network-Compression-via-a-Bilinear-Regression-Model"><a href="#5ï¸âƒ£-ECC-Platform-Independent-Energy-Constrained-Deep-Neural-Network-Compression-via-a-Bilinear-Regression-Model" class="headerlink" title="5ï¸âƒ£ ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model"></a>5ï¸âƒ£ ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model</h3><p>æœ¬æ–‡ç”¨é™åˆ¶èƒ½è€—æ¥è¿›è¡Œæ¨¡å‹å‹ç¼©ï¼Œæå‡ºäº†ç”¨ä¸€ä¸ªåŒçº¿æ€§å›å½’æ¨¡å‹æ¥ä¼°è®¡targetç¡¬ä»¶å¹³å°çš„èƒ½è€—ã€‚</p><p>ç›®æ ‡ç”¨ä¸‹é¢çš„å…¬å¼è¡¨ç¤ºï¼š<br>$$<br>\begin{array}{cl}{\min _{\mathcal{W}, \mathbf{s}}} &amp; {\ell(\mathcal{W})} \ \\{\text { s.t. }} &amp; {\phi\left(\mathbf{w}^{(u)}\right) \leq s^{(u)}, \quad u \in \mathcal{U}}\\ \ {} &amp; {\mathcal{E}(\mathbf{s}) \leq E_{\text { budget }}}\end{array}<br>$$<br>è§£å†³ä¸Šé¢é—®é¢˜éœ€è¦è§£å†³ç¨€ç–ç‡åˆ°èƒ½é‡çš„æ˜ å°„æ¨¡å‹$\mathcal{E}(\mathbf{s})$ã€‚ç”¨data-drivençš„æ–¹æ³•æ¥è®­ç»ƒè¿™ä¸ªè¿‘ä¼¼æ¨¡å‹$\hat{\mathcal{E}}$ï¼š<br>$$<br>\hat{\mathcal{E}}=\underset{f \in \mathcal{F}}{\arg \min } \mathbb{E}_{\mathbf{s}}\left[(f(\mathbf{s})-\mathcal{E}(\mathbf{s}))^{2}\right]<br>$$<br>ç”¨åŒçº¿æ€§æ¨¡å‹æ¥ä¼°è®¡ç½‘ç»œæ•´ä½“èƒ½è€—ï¼š<br>$$<br>\mathcal{F} :=\{f(\mathbf{s})=a_{0}+\sum_{j=1}^{|\mathcal{U}|} a_{j} s_{j} s_{j+1} : a_{0}, a_{1}, \ldots, a_{|\mathcal{U}|} \in \mathbb{R}_{+} \}<br>$$<br>ECCæ•´ä½“æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼ŒOnlineå’ŒOfflineéƒ¨åˆ†ã€‚åœ¨Offlineéƒ¨åˆ†å»ºç«‹è¿‘ä¼¼èƒ½é‡ä¼°è®¡æ¨¡å‹$\hat{\mathcal{E}}$</p><p><img src="https://i.loli.net/2019/07/28/5d3d42fd271e241820.png" alt=""></p><p>Onlineéƒ¨åˆ†åŸºäºèƒ½é‡æ¨¡å‹è¿›è¡Œå‹ç¼©å’ŒADMMè¿›è¡Œå‹ç¼©ã€‚å°†ç›®æ ‡è½¬ä¸ºminmaxä¼˜åŒ–é—®é¢˜ï¼š<br>$$<br>\min _{\mathcal{W}, \mathbf{s}} \max _{z \geq 0, \mathbf{y} \geq \mathbf{0}} \mathcal{L}(\mathcal{W}, \mathbf{s}, \mathbf{y}, z)<br>$$<br>å¼•å…¥å¯¹å¶å˜é‡$y$å’Œ$z$ç”¨äºé™åˆ¶ç¨€ç–ç‡ï¼Œå¼•å…¥zç”¨äºé™åˆ¶èƒ½é‡ï¼š<br>$$<br>\mathcal{L}(\mathcal{W}, \mathbf{s}, \mathbf{y}, z) \quad :=\ell(\mathcal{W})+\mathcal{L}_{1}(\mathcal{W}, \mathbf{s}, \mathbf{y})+\mathcal{L}_{2}(\mathbf{s}, z)<br>$$<br>å…¶ä¸­$\mathcal{L}_{1}(\mathcal{W}, \mathbf{s}, \mathbf{y}) \quad :=\quad \frac{\rho_{1}}{2} \sum_{u}\left[\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right]_{+}^{2}+\sum_{u} y^{(u)}\left(\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right), \mathcal{L}_{2}(\mathbf{s}, z)$</p><p>$\mathcal{L}_{2}(\mathbf{s}, z) :=\frac{\rho_{2}}{2}\left[\hat{\mathcal{E}}(\mathbf{s})-E_{\mathrm{budget}}\right]_{+}^{2}+z\left(\hat{\mathcal{E}}(\mathbf{s})-E_{\text { budget }}\right)$</p><p>ç®—æ³•é€šè¿‡è¿­ä»£æ›´æ–°å‚æ•°æ¥è¾¾åˆ°æœ€ç»ˆç›®æ ‡</p><ul><li>Update $W$ï¼šç”¨Proximal Adam</li><li><p>Update $s$ï¼š$\mathbf{s}^{t+1}=\mathbf{s}^{t}-\beta\left(\nabla_{\mathbf{s}} \mathcal{L}_{1}\left(\mathcal{W}, \mathbf{s}^{t}, \mathbf{y}\right)+\nabla_{\mathbf{s}} \mathcal{L}_{2}\left(\mathbf{s}^{t}, z\right)\right)$</p></li><li><p>Update å¯¹å¶å˜é‡ï¼š$\begin{aligned} y^{(u)^{t+1}} &amp;=\left[y^{(u)^{t}}+\rho_{1}\left(\phi\left(\mathbf{w}^{(u)}\right)-s^{(u)}\right)\right]_{+} \\ z^{t+1} &amp;=\left[z^{t}+\rho_{2}\left(\hat{\mathcal{E}}(\mathbf{s})-E_{\mathrm{budget}}\right)\right]_{+} \end{aligned}$</p></li></ul><h3 id="NETTAILOR-Tuning-the-architecture-not-just-the-weights"><a href="#NETTAILOR-Tuning-the-architecture-not-just-the-weights" class="headerlink" title="NETTAILOR: Tuning the architecture, not just the weights"></a>NETTAILOR: Tuning the architecture, not just the weights</h3><p>è¿™ç¯‡æ–‡ç« å¾ˆæœ‰æ„æ€ï¼Œä¸æ­¢fintuneç½‘ç»œæƒé‡ï¼Œè¿˜FTç½‘ç»œç»“æ„ã€‚ç›®å‰å¤§éƒ¨åˆ†ç½‘ç»œä½¿ç”¨çš„æ˜¯ç›¸åŒçš„backboneï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ç½‘ç»œç»“æ„æœ¬èº«çš„å½±å“ã€‚å¯èƒ½å°ä¸€äº›çš„ç½‘ç»œåœ¨ç›®æ ‡æ•°æ®é›†ä¸Šå°±è¶³å¤Ÿäº†ã€‚æœ¬æ–‡å°†pre-trainedçš„backboneç½‘ç»œç»“æ„ä¸ºuniversal blocksï¼ŒåŠ ä¸Šä¸€äº›task-specificç½‘ç»œæ¥ç”Ÿæˆæ–°çš„ç½‘ç»œã€‚é€šè¿‡soft-attentionæœºåˆ¶å’Œç½‘ç»œçš„å¤æ‚åº¦é™åˆ¶æ¥å­¦ä¹ æ–°çš„ç½‘ç»œç»“æ„å’Œæƒé‡ã€‚</p><p>ä¸€äº›ç›¸å…³å·¥ä½œåŒ…æ‹¬è¿ç§»å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ï¼ˆå¢å¼ºä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–æ€§ï¼‰ï¼Œè¿ç§»å­¦ä¹ å‡è®¾å›¾åƒæ¥è‡ªä¸åŒçš„åŸŸï¼ŒMTLå‡è®¾æ‰€æœ‰ä»»åŠ¡æ˜¯å¤„äºåŒåŸŸçš„ã€‚Domain adaptationè§£å†³ä¸¤ä¸ªä¸åŒåŸŸæ•°æ®é›†çš„ä»»åŠ¡ã€‚Cascaded classifiers &amp; Adaptive inference graphsèƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ã€‚ä½†æ˜¯é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¦è®­ç»ƒä¸åŒçš„ç½‘ç»œï¼ŒNETTAILORé€šè¿‡é‡ç”¨universal blocksï¼Œåªè®­ç»ƒtaskç›¸å…³çš„blockæ¥è§£å†³multi-domain transfer learning problemsã€‚</p><p>ç®—æ³•ä¸»è¦åˆ†æˆä»¥ä¸‹å››æ­¥ï¼š1. åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šç”¨pre-trainedç½‘ç»œè®­ç»ƒä¸€ä¸ªteacher networkã€‚2. å®šä¹‰åŒ…æ‹¬proxy layersçš„å­¦ç”Ÿç½‘ç»œã€‚3. åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šåªè®­ç»ƒtask-specificå‚æ•°ï¼ŒåŒæ—¶åŠ ä¸Šå¤æ‚åº¦é™åˆ¶ã€‚4. ç²¾ç®€ç½‘ç»œç»“æ„åè¿›è¡Œfinetuneã€‚</p><p><img src="https://github.com/pedro-morgado/nettailor/raw/master/docs/figs/teaser_row.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;### &lt;/p&gt;
&lt;h3 id=&quot;1ï¸âƒ£-Interpretable-and-Fine-Grained-Visual-Explanations-for-Convolutional-Neural-Networks&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Interpretable-an
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>è¿›åŒ–ç­–ç•¥</title>
    <link href="http://yoursite.com/2019/07/20/evolution-strategies/"/>
    <id>http://yoursite.com/2019/07/20/evolution-strategies/</id>
    <published>2019-07-19T23:44:06.000Z</published>
    <updated>2019-07-20T00:25:32.893Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" target="_blank" rel="external">è¿™ç¯‡åšå®¢</a></p><p>è¿›åŒ–ç­–ç•¥æ˜¯ä¸€ç§é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œé˜²æ­¢å‚æ•°é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚è¿›åŒ–ç­–ç•¥å¯ä»¥çœ‹ä½œä¸€ç§æä¾›ä¸€ç³»åˆ—å€™é€‰è§£å†³æ–¹æ¡ˆæ¥è¯„ä¼°ä¸€ä¸ªé—®é¢˜çš„ç®—æ³•ã€‚è¯„ä¼°ç»“æœåŸºäºä¸€ä¸ªç›®æ ‡å‡½æ•°(objective function)ï¼Œä¸€ä¸ªè§£å†³æ–¹æ¡ˆè¿”å›ä¸€ä¸ªé€‚åº”åº¦(fitness value)ï¼ŒåŸºäºå½“å‰è§£å†³æ–¹æ¡ˆçš„é€‚åº”åº¦ï¼Œå†ç”Ÿæˆä¸‹ä¸€é›†åˆçš„å€™é€‰è€…ã€‚æœ€ç®€å•çš„ä¼ªä»£ç å¦‚ä¸‹ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">solver = EvolutionStrategy()</div><div class="line"></div><div class="line">while True:</div><div class="line"></div><div class="line">  # è¯·æ±‚ESç”Ÿæˆå€™é€‰è€…</div><div class="line">  solutions = solver.ask()</div><div class="line"></div><div class="line">  # åˆå§‹åŒ–é€‚åº”åº¦</div><div class="line">  fitness_list = np.zeros(solver.popsize)</div><div class="line"></div><div class="line">  # è¯„ä¼°å€™é€‰è€…ï¼Œç”Ÿæˆä¸å…¶å¯¹åº”çš„é€‚åº”åº¦</div><div class="line">  for i in range(solver.popsize):</div><div class="line">    fitness_list[i] = evaluate(solutions[i])</div><div class="line"></div><div class="line">  # è¿”å›é€‚åº”åº¦ç»“æœç»™ES</div><div class="line">  solver.tell(fitness_list)</div><div class="line"></div><div class="line">  # ä»ESä¸­è·å–æœ€ä¼˜å‚æ•°å’Œæœ€ä¼˜é€‚åº”åº¦</div><div class="line">  best_solution, best_fitness = solver.result()</div><div class="line"></div><div class="line">  if best_fitness &gt; MY_REQUIRED_FITNESS:</div><div class="line">    break</div></pre></td></tr></table></figure><p>ä»¥ç®€å•çš„2Dé—®é¢˜ä¸ºä¾‹ï¼Œå‚æ•°ç”±$\mu=\left(\mu_{x}, \mu_{y}\right)$å’Œ$\sigma=\left(\sigma_{x}, \sigma_{y}\right)$ç»„æˆSimple ESå’ŒSimple GAéƒ½æ˜¯å›ºå®š$\sigma$ä¸å˜ï¼Œé€šè¿‡è¿›åŒ–ç®—æ³•å­¦ä¹ $\mu$ï¼Œç”±æ­¤CMA-ESç®—æ³•è¯ç”Ÿã€‚å®ƒæ˜¯ä¸€ç§ä¸åŸºäºæ¢¯åº¦çš„ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æ‰€æœ‰å‚æ•°ç©ºé—´çš„åæ–¹å·®çŸ©é˜µï¼Œåœ¨æ¯æ¬¡è¿­ä»£æ—¶ä»å¤šå…ƒæ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·è§£å†³æ–¹æ¡ˆã€‚</p><p>ä¸Šé¢æåˆ°çš„ä¸€äº›ç®—æ³•åªä¿ç•™äº†æœ€ä¼˜è§£ï¼Œå¿½ç•¥äº†å…¶ä»–è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤ä¹Ÿå¯èƒ½å¿½ç•¥å¤§éƒ¨åˆ†å¯¹ç”Ÿæˆä¸‹ä¸€ä»£æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç»“åˆRLç®—æ³•ï¼Œæœ‰äººæå‡ºäº†REINFORCE-ESä»¥åŠNESï¼Œéµå¾ªçš„åŸåˆ™æ˜¯ä¸è®ºå¥½åç»¼åˆæ‰€æœ‰å€™é€‰è€…ä»¥ä¼°è®¡æ¢¯åº¦ï¼Œå¾€æ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯<strong>æœ€å¤§åŒ–é‡‡æ ·å€™é€‰è€…çš„é€‚åº”åº¦æœŸæœ›å€¼</strong>ï¼š<br>$$<br>J(\theta)=E_{\theta}[F(z)]=\int F(z) \pi(z, \theta) d z<br>$$<br><a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">NES</a>æä¾›äº†æ¢¯åº¦çš„æ¨å¯¼ï¼Œåˆ©ç”¨log-likelihood trickå’Œè’™ç‰¹å¡æ´›é‡‡æ ·å¯ä»¥å¾—åˆ°ï¼š<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} F\left(z^{i}\right) \nabla_{\theta} \log \pi\left(z^{i}, \theta\right)<br>$$<br><a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a>ä¸ºç‰¹æ®Šçš„æ¡ˆä¾‹ï¼Œå½“$\pi(z, \theta)$æ˜¯ä¸€ä¸ªfactored multi-variate normal distributionï¼Œå³æ¯ä¸ªå‚æ•°æœä»ä¸€ä¸ªä¸€å…ƒæ­£æ€åˆ†å¸ƒ$z_{j} \sim N\left(\mu_{j}, \sigma_{j}\right)$ï¼Œç»™å‡ºäº†ä¸€ä¸ªæ¢¯åº¦çš„å°é—­è§£ï¼š<br>$$<br>\begin{array}{l}{\nabla_{\mu_{j}} \log N\left(z^{i}, \mu, \sigma\right)=\frac{z_{j}^{i}-\mu_{j}}{\sigma_{j}^{2}}} \\ {\nabla_{\sigma_{j}} \log N\left(z^{i}, \mu, \sigma\right)=\frac{\left(z_{j}^{i}-\mu_{j}\right)^{2}-\sigma_{j}^{2}}{\sigma_{j}^{3}}}\end{array}<br>$$<br>è¿™äº›è®ºæ–‡æå‡ºäº†ä¸€äº›tricksï¼Œæ¯”å¦‚PEPGä¸­çš„antithetic samplingï¼ŒNESä¸­åˆ©ç”¨Fisher Informationæ›´æ–°æ¢¯åº¦ç­‰ç­‰ã€‚</p><p>åœ¨OenAIçš„<a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">è®ºæ–‡</a>é‡Œï¼Œå®ƒä»¬å›ºå®š$\sigma$ï¼Œåªæ›´æ–°$\mu$ï¼Œä¸»è¦æ˜¯è§£å†³æ‰§è¡Œå±‚é¢çš„å¹¶è¡Œè¿ç®—é—®é¢˜ã€‚</p><p>é€šå¸¸è¿›åŒ–ç­–ç•¥éƒ½ä¼šé‡‡æ ·ä¸€ä¸ªç§°ä¸ºFitness Shapingçš„trickï¼ŒæŠŠç§ç¾¤é€‚åº”åº¦è½¬åŒ–ä¸ºç§ç¾¤å†…éƒ¨çš„ç›¸å¯¹å€¼ï¼Œå³rankä¸€ä¸‹fitnessï¼Œä¿è¯è¯„ä¼°æŒ‡æ ‡çš„ä¸å˜æ€§ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;è¿™ç¯‡åšå®¢&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;è¿›åŒ–ç­–ç•¥æ˜¯ä¸€ç§é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œé˜²æ­¢å‚æ•°
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.09</title>
    <link href="http://yoursite.com/2019/07/08/weekly-paper-09/"/>
    <id>http://yoursite.com/2019/07/08/weekly-paper-09/</id>
    <published>2019-07-08T01:07:11.000Z</published>
    <updated>2019-07-17T13:39:48.882Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Natural-Evolution-Strategies"><a href="#1ï¸âƒ£-Natural-Evolution-Strategies" class="headerlink" title="1ï¸âƒ£ Natural Evolution Strategies"></a>1ï¸âƒ£ Natural Evolution Strategies</h3><p>NESæ˜¯ä¸€ç§åˆ©ç”¨æœç´¢æ¢¯åº¦(search gradients)æ›´æ–°æœç´¢åˆ†å¸ƒå‚æ•°(parameters of the search distribution)çš„é»‘ç®±ä¼˜åŒ–ç®—æ³•ï¼Œä¸ç»å…¸æ–¹æ³•ï¼ˆEDAsï¼‰åˆ©ç”¨æœ€å¤§ä¼¼ç„¶æ‹Ÿåˆé‡‡æ ·åˆ†å¸ƒçš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºçš„æ›´æ–°ç­–ç•¥æ˜¯æ²¿ç€æ›´é«˜æœŸæœ›é€‚åº”åº¦çš„æ–¹å‘ã€‚</p><p>æœ€å¤§åŒ–æ‰€é‡‡æ ·æ ·æœ¬çš„é€‚åº”åº¦çš„æœŸæœ›å€¼ã€‚æ ¸å¿ƒæ€æƒ³å°±æ˜¯åœ¨æ¯æ¬¡æ–°çš„ç§ç¾¤é‡Œæ›´æ–°meanå’Œstdï¼Œä»æ–°çš„ç»“æœä¸­ç»§ç»­æ›´æ–°</p><h4 id="0-Search-Gradients"><a href="#0-Search-Gradients" class="headerlink" title="0/ Search Gradients"></a>0/ Search Gradients</h4><p>å‡è®¾æˆ‘ä»¬ä»åˆ†å¸ƒ$\pi(\mathbf z | \theta)$ä¸­é‡‡æ ·$\mathbf z$ï¼Œç”¨$f(\mathbf z)$è¡¨ç¤ºè¯¥é‡‡æ ·çš„é€‚åº”åº¦ã€‚åœ¨è¯¥æœç´¢åˆ†å¸ƒä¸‹çš„æœŸæœ›é€‚åº”åº¦ä¸ºï¼š<br>$$<br>J(\theta)=\mathbb{E}_{\theta}[f(\mathbf{z})]=\int f(\mathbf{z}) \pi(\mathbf{z} | \theta) d \mathbf{z}<br>$$<br>åˆ©ç”¨ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/" target="_blank" rel="external">log-likelihood trick</a>ï¼š<br>$$<br>\begin{aligned} \nabla_{\theta} J(\theta) &amp;=\nabla_{\theta} \int f(\mathbf{z}) \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\int f(\mathbf{z}) \nabla_{\theta} \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\int f(\mathbf{z}) \nabla_{\theta} \pi(\mathbf{z} | \theta) \frac{\pi(\mathbf{z} | \theta)}{\pi(\mathbf{z} | \theta)} d \mathbf{z} \\ &amp;=\int\left[f(\mathbf{z}) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)\right] \pi(\mathbf{z} | \theta) d \mathbf{z} \\ &amp;=\mathbb{E}_{\theta}\left[f(\mathbf{z}) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)\right] \end{aligned}<br>$$<br>åˆ©ç”¨ç§ç¾¤å¤§å°$\lambda$å¯¹æœç´¢æ¢¯åº¦è¿›è¡Œè¿‘ä¼¼ä¼°è®¡ï¼š<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{\lambda} \sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \nabla_{\theta} \log \pi\left(\mathbf{z}_{k} | \theta\right)<br>$$<br>$\nabla_{\theta} J(\theta)$æä¾›äº†æ¢¯åº¦ä¸Šå‡çš„æ–¹å‘ï¼Œæœ€ç›´æ¥çš„æ–¹æ³•å‚æ•°æ›´æ–°æ–¹æ³•ï¼š$\theta \leftarrow \theta+\eta \nabla_{\theta} J(\theta)$</p><blockquote><p>æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æœŸæœ›å€¼ï¼Œå› æ­¤æ˜¯æ¢¯åº¦ä¸Šå‡ã€‚</p></blockquote><p>è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºæ ‡å‡†æœç´¢æ¢¯åº¦ç®—æ³•ï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d229d3f6943c17863.png" alt=""></p><p>ä»¥å¤šå…ƒæ­£æ€åˆ†å¸ƒä¸ºä¾‹ï¼Œ$\theta=\langle\boldsymbol{\mu}, \boldsymbol{\Sigma}\rangle$ä¸ºéœ€è¦å­¦ä¹ çš„åˆ†å¸ƒå‚æ•°ã€‚æˆ‘ä»¬è¿˜éœ€è¦æ»¡è¶³$\mathbf{A}^{\top} \mathbf{A}=\mathbf{\Sigma}$çš„åæ–¹å·®çŸ©é˜µçš„å¹³æ–¹æ ¹çŸ©é˜µ$\mathbf{A} \in \mathbb{R}^{d \times d}$ï¼Œä½¿å¾—$\mathbf{z}=\boldsymbol{\mu}+\mathbf{A}^{\top} \mathbf{s}$å°†æ ‡å‡†æ­£æ€åˆ†å¸ƒ$\mathbf{s} \sim \mathcal{N}(0, \mathbb{I})$è½¬åŒ–ä¸ºç§ç¾¤ä¸ªä½“$\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})$ï¼Œéœ€è¦å¯¹$\boldsymbol{\mu}, \mathbf{\Sigma}$æ±‚å¯¼ï¼Œæ›´æ–°ç®—æ³•ä¸ºï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d229f6ae0f3061548.png" alt=""></p><h4 id="1-Limitations"><a href="#1-Limitations" class="headerlink" title="1/ Limitations"></a>1/ Limitations</h4><p>é’ˆå¯¹æ™®é€šæœç´¢æ¢¯åº¦ç®—æ³•çš„å±€é™ï¼Œæœ¬æ–‡æå‡ºçš„è§£å†³åŠæ³•å¯ä»¥ç”¨ä¸‹è¡¨æ€»ç»“ï¼š</p><p><img src="https://i.loli.net/2019/07/08/5d2299a8b796f92761.png" alt=""></p><p>è¿™äº›é—®é¢˜å’Œæ–¹æ³•è®©æˆ‘ä»¬æ¥ä¸€ä¸ªä¸ªæ”»ç ´ã€‚</p><p><strong>a. Natural gradient</strong></p><p>ä»¤$\lambda=1, d=1$ï¼Œ$\mu \leftarrow \mu+\eta \frac{z-\mu}{\sigma^{2}},\quad \sigma \leftarrow \sigma+\eta \frac{(z-\mu)^{2}-\sigma^{2}}{\sigma^{3}}$</p><p>ç”±äº$\Delta \mu \propto \frac{1}{\sigma}, \Delta \sigma \propto \frac{1}{\sigma}$ï¼Œ$\sigma$åŒæ—¶æ§åˆ¶äº†$\mu$å’Œ$\sigma$çš„æ›´æ–°ï¼Œé€ æˆå‚æ•°çš„æ›´æ–°ä¸æ˜¯å°ºåº¦ä¸å˜(scale-invariant)çš„ï¼šæˆ‘ä»¬å‡å°$\sigma$è®©$\mu$æ¥è¿‘æœ€ä¼˜è§£çš„åŒæ—¶ä¹Ÿå¢å¤§äº†$\sigma$ï¼Œä½¿å…¶åœ¨å‚æ•°æ›´æ–°çš„æ—¶å€™å†æ¬¡è¿œç¦»æœ€ä¼˜è§£ã€‚</p><p>è‡ªç„¶æ¢¯åº¦çš„æå‡ºä¾¿æ˜¯ä¸ºäº†è§£å†³å°ºåº¦ä¸å˜çš„é—®é¢˜ã€‚åŸå§‹æ¢¯åº¦æµ‹é‡çš„æ˜¯å‚æ•°åˆ†å¸ƒçš„æ¬§æ‹‰è·ç¦»ã€‚è‡ªç„¶æ¢¯åº¦åˆ©ç”¨çš„æ˜¯å‚æ•°åˆ†å¸ƒçš„KLæ•£åº¦ã€‚<br>$$<br>\begin{aligned} \max _{\delta \theta} J(\theta+\delta \theta) &amp; \approx J(\theta)+\delta \theta^{\top} \nabla_{\theta} J \\ \text {s.t. } D(\theta+\delta \theta | \theta) &amp;=\varepsilon, \end{aligned}<br>$$<br>$J(\theta)$ä»ç„¶æ˜¯æœŸæœ›é€‚åº”åº¦ï¼Œ$\varepsilon$æ˜¯ä¸€ä¸ªå¾ˆå°çš„å¢é‡ï¼Œé€šè¿‡äºŒé˜¶æ³°å‹’å±•å¼€$\lim \delta \theta \rightarrow 0$ï¼Œ$D(\theta+\delta \theta | \theta)=\frac{1}{2} \delta \theta^{\top} \mathbf{F}(\theta) \delta \theta$ã€‚å…¶ä¸­<br>$$<br>\begin{aligned} \mathbf{F} &amp;=\int \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)^{\top} d \mathbf{z} \\ &amp;=\mathbb{E}\left[\nabla_{\theta} \log \pi(\mathbf{z} | \theta) \nabla_{\theta} \log \pi(\mathbf{z} | \theta)^{\top}\right] \end{aligned}<br>$$<br>è¡¨ç¤ºä¸ºè´¹é›ªä¿¡æ¯çŸ©é˜µ(Fisher information matrix)ã€‚è‹¥$ \mathbf{F}$å¯é€†ï¼Œåˆ™è‡ªç„¶æ¢¯åº¦è¡¨ç¤ºä¸ºï¼š<br>$$<br>\widetilde{\nabla}_{\theta} J=\mathbf{F}^{-1} \nabla_{\theta} J(\theta)<br>$$</p><blockquote><p>æ¨å¯¼å¯ä»¥å‚è€ƒ<a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" target="_blank" rel="external">blog</a></p></blockquote><p>ç„¶åæˆ‘ä»¬å°±å¯ä»¥å†™å‡ºæ ‡å‡†è‡ªç„¶æ¼”åŒ–ç­–ç•¥ç®—æ³•ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190708145437180.png" alt="image-20190708145437180"></p><p>ä¸ç®—æ³•1çš„åŒºåˆ«å°±åœ¨äºï¼Œæ›´æ–°çš„æ—¶å€™æ¢¯åº¦ä¹˜ä»¥äº†ä¸€ä¸ªè´¹é›ªä¿¡æ¯çŸ©é˜µçš„é€†ã€‚</p><p><strong>b. Fitness shaping</strong></p><p>ç”¨æ•ˆç”¨å‡½æ•°(utility)æ›¿anä»£é€‚åº”åº¦ï¼š<br>$$<br>\nabla_{\theta} J(\theta)=\sum_{k=1}^{\lambda} u_{k} \nabla_{\theta} \log \pi\left(\mathbf{z}_{k} | \theta\right)<br>$$<br>æœ¬æ–‡å®šä¹‰çš„æ•ˆç”¨å‡½æ•°ä¸ºï¼š<br>$$<br>u_{k}=\frac{\max \left(0, \log \left(\frac{\lambda}{2}+1\right)-\log (k)\right)}{\sum_{j=1}^{\lambda} \max \left(0, \log \left(\frac{\lambda}{2}+1\right)-\log (j)\right)}-\frac{1}{\lambda}<br>$$<br><strong>c. Adapation Sampling</strong></p><p>ç”¨è´¨é‡å‡½æ•°åˆ¤æ–­ï¼Œå½“å‰é‡‡æ ·$\mathbf {zâ€™}$å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºä¹‹å‰çš„é‡‡æ ·$\mathbf {z}$ï¼Œæ‰è¿›è¡Œå‚æ•°æ›´æ–°ã€‚ä¸å•çº¯çš„æœ€å¤§åŒ–é€‚åº”åº¦å‡½æ•°æœ¬èº«ä¸åŒï¼Œé€‚åº”é‡‡æ ·æœ€å¤§åŒ–çš„æ˜¯è¿›æ­¥çš„æ­¥ä¼(pace of progress)ã€‚ä»¥æœ€é‡è¦çš„å‚æ•°å­¦ä¹ ç‡$\eta_{\sigma}$ä¸ºä¾‹ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190708163252563.png" alt="image-20190708163252563"></p><p>é¦–å…ˆåˆ©ç”¨$1.5\eta_{\sigma, t}$å’Œ$\theta_{t-1}$è®¡ç®—å‡ºå‡æƒ³å‚æ•°$\theta â€˜$ï¼Œè®¡ç®—æ¯ä¸ªä¸ªä½“çš„æƒé‡$w_k â€˜$ï¼Œç”¨Mann-Whitney testæ£€éªŒä¸¤ä¸ªè´¨é‡å‡½æ•°ï¼Œå½“å¤§äºé˜ˆå€¼$\rho=\frac{1}{2}-\frac{1}{3(d+1)}$æ—¶ï¼Œå¢åŠ å­¦ä¹ ç‡ï¼Œå¦åˆ™è®©å®ƒé è¿‘åˆå§‹çš„å­¦ä¹ ç‡ã€‚</p><p><strong>d. Exponential parameterization &amp; Natural coordinate system</strong></p><p>NESä¸ç›´æ¥æ›´æ–°åæ–¹å·®$\mathbf \Sigma$ï¼Œè€ƒè™‘ä»¥å®ƒçš„å¹³æ–¹æ ¹$\mathbf A$ä¸ºå‚æ•°çš„æ­£æ€åˆ†å¸ƒï¼Œåˆ©ç”¨è‡ªç„¶æ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚ä¸ºäº†é¿å…ä¼°è®¡FisherçŸ©é˜µï¼ŒxNESä½¿ç”¨äº†å±€éƒ¨åæ ‡ç³»å’ŒæŒ‡æ•°æ˜ å°„ã€‚å±€éƒ¨åæ ‡ç³»ä¸‹çš„è‡ªç„¶æ¢¯åº¦ä¸ºï¼š<br>$$<br>\begin{aligned} \nabla_{\boldsymbol{\delta}} J &amp;=\sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \cdot \mathbf{s}_{k} \\ \nabla_{\mathbf{M}} J &amp;=\sum_{k=1}^{\lambda} f\left(\mathbf{z}_{k}\right) \cdot\left(\mathbf{s}_{k} \mathbf{s}_{k}^{\top}-\mathbb{I}\right) \end{aligned}<br>$$<br>å…¶ä¸­$\mathbf {s}_k$æ˜¯å±€éƒ¨åæ ‡ç³»ä¸‹ç¬¬$k$ä¸ªæœ€ä¼˜æ ·æœ¬ï¼Œ$\mathbf {z}_k$æ˜¯ç›®æ ‡2åæ ‡ç³»ä¸‹çš„ç›¸åŒæ ·æœ¬ã€‚æŠŠåæ–¹å·®å› å­$\mathbf{A}$åˆ†è§£ä¸ºæ­¥é•¿$\sigma &gt;0$å’Œæ»¡è¶³$\operatorname{det}(\mathbf{B})=1$çš„å½’ä¸€åŒ–åæ–¹å·®å› å­$\mathbf{B}$ã€‚è¿™ç§åˆ†è§£ä½¿å¾—ä¸¤ä¸ªæ­£äº¤æˆåˆ†å¯ä»¥æœ‰å„è‡ªçš„å­¦ä¹ ç‡ï¼ˆ$\eta_{\sigma}$, $\eta_{\mathbf {B}}$ï¼‰ï¼Œå¯¹æ­¥é•¿$\sigma$å’Œ$B$çš„æ›´æ–°åšäº†æŒ‡æ•°æ˜ å°„ã€‚</p><p><img src="https://i.loli.net/2019/07/08/5d230b687c16942920.png" alt=""></p><p>æœ¬æ–‡è¿˜æäº†separable NES(SNES)ï¼Œä½¿ç”¨åˆ†ç¦»çš„æœç´¢åˆ†å¸ƒå‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚æœ¬æ–‡çš„å®ç°æ–¹å¼æ˜¯é™åˆ¶$\mathbf {A}$ä¸ºå¯é€†çš„å¯¹è§’å˜æ¢çŸ©é˜µã€‚</p><p><img src="https://i.loli.net/2019/07/08/5d230b390673572266.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Natural-Evolution-Strategies&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Natural-Evolution-Strategies&quot; class=&quot;headerlink&quot; title=&quot;1ï¸âƒ£ Natural Evolution Strate
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ‰‹æ’•PyTorchçš„Batch Normalization</title>
    <link href="http://yoursite.com/2019/07/02/batch-normalization/"/>
    <id>http://yoursite.com/2019/07/02/batch-normalization/</id>
    <published>2019-07-02T00:35:20.000Z</published>
    <updated>2019-07-05T09:03:38.738Z</updated>
    
    <content type="html"><![CDATA[<p>BNæ˜¯é˜²ç½‘ç»œè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ï¼Œç»†å¾®çš„å·®åˆ«å¯èƒ½å¯¹è¾“å‡ºæ•ˆæœæœ‰å¾ˆå¤§å½±å“ã€‚å› æ­¤éœ€è¦ç†è§£ä¸€ä¸‹PyTorchä¸­BNçš„å…·ä½“å®ç°ã€‚PyTorchçš„æºç ç”¨Cå®ç°çš„torch.batchnormã€‚å¯ä»¥<a href="https://github.com/marvis/pytorch-yolo2/blob/master/layers/batchnorm/src/batchnorm.c" target="_blank" rel="external">yolo2</a>é‡Œæ‰¾åˆ°å…·ä½“å®ç°ã€‚</p><h3 id="è®¡ç®—è¿‡ç¨‹"><a href="#è®¡ç®—è¿‡ç¨‹" class="headerlink" title="è®¡ç®—è¿‡ç¨‹"></a>è®¡ç®—è¿‡ç¨‹</h3><p>$Input : \mathcal{B}=\left\{x_{1}, \cdots, x_{m}\right\}$ è¡¨ç¤ºbatch_sizeä¸º$m$çš„è¾“å…¥æ•°æ®ã€‚</p><p>$Output:  \gamma ,  \beta$   PyTorchä¸­ä¸ºweightså’Œbiasã€‚</p><p><strong>æ›´æ–°è¿‡ç¨‹ï¼š</strong></p><p>$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i}$</p><p>$\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}$</p><p>$\hat{x}_{i} \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}$</p><p>$y_{i} \leftarrow \gamma \hat{x}_{i}+\beta \equiv \mathrm{B} \mathrm{N}_{\gamma, \beta}\left(x_{i}\right)$</p><p>é¦–å…ˆé€šè¿‡ä»£ç æµ‹è¯•ä¸€ä¸‹å…·ä½“æ›´æ–°è¿‡ç¨‹çš„å®ç°ã€‚</p><p><strong>æµ‹è¯•è¾“å‡ºï¼š</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">x = torch.range(<span class="number">0</span>, <span class="number">35</span>).reshape(<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># initilization</span></div><div class="line">bn = nn.BatchNorm2d(<span class="number">3</span>)</div><div class="line">bn.weight.data.fill_(<span class="number">1</span>)</div><div class="line">bn.bias.data.zero_()</div><div class="line">mean, new_mean = torch.zeros([<span class="number">3</span>]), torch.zeros([<span class="number">3</span>])</div><div class="line">var, new_var = torch.ones([<span class="number">3</span>]), torch.zeros([<span class="number">3</span>])</div><div class="line"></div><div class="line"><span class="comment"># compute mean</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_mean[i] = x[:, i, :, :].mean()</div><div class="line"></div><div class="line"><span class="comment"># compute variance</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_var[i] = torch.pow((x[:, i, :, :] - new_mean[i]), <span class="number">2</span>).mean()</div><div class="line">  <span class="comment"># ç­‰åŒäº new_var[i] = x[:, i, :, :].var(False) </span></div><div class="line">  <span class="comment"># è®¡ç®—æ–¹å·®æ—¶ä¸ä½¿ç”¨è´å¡å°”æ ¡æ­£</span></div><div class="line">  </div><div class="line">normalized_x = torch.zeros_like(x)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  normalized_x[:, i, :, :] = (x[:, i, :, :] - new_mean[i]) / torch.sqrt(new_var[i] + bn.eps)</div><div class="line">  </div><div class="line">print(bn_x)</div><div class="line">print(normalized_x)</div></pre></td></tr></table></figure><p><strong>æµ‹è¯•runing_meanå’Œrunning_varï¼š</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># moving average</span></div><div class="line">running_mean = bn.momentum * new_mean + (<span class="number">1</span> - bn.momentum) * mean</div><div class="line">running_var = bn.momentum * new_var + (<span class="number">1</span> - bn.momentum) * var</div><div class="line"></div><div class="line">print(bn.running_mean, bn.running_var)</div><div class="line">print(running_mean, running_var)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">1.3500</span>, <span class="number">1.7500</span>, <span class="number">2.1500</span>] [<span class="number">11.5091</span>, <span class="number">11.5091</span>, <span class="number">11.5091</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">1.3500</span>, <span class="number">1.7500</span>, <span class="number">2.1500</span>] [<span class="number">10.6250</span>, <span class="number">10.6250</span>, <span class="number">10.6250</span>]</div></pre></td></tr></table></figure><p>running_meanå¯¹ä¸Šäº†ï¼Œå¯æ˜¯running_varå´ä¸å¯¹ã€‚ä»”ç»†çœ‹äº†ä¸€ä¸‹æºä»£ç åœ¨å‡½æ•°<code>variance_cpu</code>ä¸­æ³¨é‡Šæ‰äº†ä¸€å¥<code>float scale = 1./(batch * spatial - 1)</code>ï¼Œè¿™å¥å°±å¾ˆå…³é”®äº†ã€‚ä¿®æ”¹ä¸€ä¸‹ä¸Šé¢varçš„è®¡ç®—æ–¹å¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scale = x.size(<span class="number">0</span>) * x.size(<span class="number">2</span>) * x.size(<span class="number">3</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">  new_var[i] = x[:, i, :, :].var(<span class="keyword">False</span>) </div><div class="line">  </div><div class="line">new_var = new_var * scale / (scale - <span class="number">1</span>)</div><div class="line">running_var = bn.momentum * new_var + (<span class="number">1</span> - bn.momentum)*var</div><div class="line">print(running_var)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">11.5091</span>, <span class="number">11.5091</span>, <span class="number">11.5091</span>]</div></pre></td></tr></table></figure><p>è¿™å›æ–¹å·®ä¹ŸğŸ‰‘ï¸äº†ã€‚è¯´æ˜å½’ä¸€åŒ–ä¸­çš„æ–¹å·®ä½¿ç”¨çš„æ˜¯æ­£å¸¸è®¡ç®—å‡ºçš„æ–¹å·®ï¼Œè€Œrunning_varçš„æ–¹å·®åœ¨scaleä¸Šåšäº†-1çš„å¤„ç†ã€‚</p><blockquote><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œrunning_meanå’Œrunning_varçš„å˜åŒ–ï¼Œä¸optimizeræ— å…³ï¼Œä¸æ˜¯æ‰§è¡Œstepä»¥åæ‰æ›´æ–°å€¼ï¼Œè€Œæ˜¯æ¯æ¬¡åšå‰å‘å€¼éƒ½ä¼šæ”¹å˜ã€‚</p></blockquote><p><strong>å®Œæ•´ä»£ç </strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mean</span><span class="params">(x, running_mean, mom=<span class="number">0.1</span>)</span>:</span></div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    new_mean = torch.zeros([nc])</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        new_mean[i] = x[:, i, :, :].mean()</div><div class="line">    running_mean = mom * new_mean + (<span class="number">1</span> - mom) * running_mean</div><div class="line">    <span class="keyword">return</span> new_mean, running_mean</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_var</span><span class="params">(x, running_var, mom=<span class="number">0.1</span>)</span>:</span></div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    scale = x.size(<span class="number">0</span>) * x.size(<span class="number">2</span>) * x.size(<span class="number">3</span>)</div><div class="line">    new_var = torch.zeros([nc])</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        new_var[i] = x[:, i, :, :].var(<span class="keyword">False</span>)</div><div class="line">    temp_var = new_var * scale / (scale - <span class="number">1</span>)</div><div class="line">    running_var = mom * temp_var + (<span class="number">1</span> - mom) * running_var</div><div class="line">    <span class="keyword">return</span> new_var, running_var</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm_x</span><span class="params">(x, mean, var, eps=<span class="number">1e-5</span>)</span>:</span></div><div class="line">    normalized_x = torch.zeros_like(x)</div><div class="line">    nc = x.size(<span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nc):</div><div class="line">        normalized_x[:, i, :, :] = (x[:, i, :, :] - mean[i]) / torch.sqrt(var[i] + eps)</div><div class="line">    <span class="keyword">return</span> normalized_x</div><div class="line">  </div><div class="line">mean, running_mean = get_mean(x, running_mean)</div><div class="line">var, running_var = get_var(x, running_var)</div><div class="line">x = norm_x(x, mean, var)</div></pre></td></tr></table></figure><p>å°ç»“ä¸€ä¸‹ï¼ŒBNåœ¨è®­ç»ƒå’Œæµ‹è¯•é‡‡å–çš„æ˜¯ä¸¤ç§æ¨¡å¼ï¼Œè®­ç»ƒé˜¶æ®µæ¯ä¸ªbatchç”¨çš„æ˜¯å½“å‰batchç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œæµ‹è¯•é˜¶æ®µæ¯ä¸ªç”¨çš„æ˜¯moving averagesçš„ç»Ÿè®¡å€¼ã€‚åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œå³$\gamma, \beta$ä½¿å¾—æ¯å±‚çš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½å¹³ç¨³ã€‚</p><h3 id="ç›¸å…³å‚æ•°"><a href="#ç›¸å…³å‚æ•°" class="headerlink" title="ç›¸å…³å‚æ•°"></a>ç›¸å…³å‚æ•°</h3><p>çŸ¥é“äº†å…·ä½“æ›´æ–°è¿‡ç¨‹çš„å®ç°åï¼Œæ¥çœ‹ä¸€ä¸‹PyTorchä¸­BatchNormçš„API</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">torch.nn.BatchNorm1d(num_features, </div><div class="line">                     eps=<span class="number">1e-05</span>, </div><div class="line">                     momentum=<span class="number">0.1</span>, </div><div class="line">                     affine=<span class="keyword">True</span>, </div><div class="line">                     track_running_stats=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>å…¶ä¸­<code>affine</code>è¡¨æ˜æ˜¯å¦ç”¨$\gamma$å’Œ$\beta$è¿›è¡Œä»¿å°„ï¼Œå½“<code>affine=False</code>æ—¶ï¼Œ<code>weigthts=None, bias=None</code>ï¼Œ<code>track_running_stats</code>è¡¨æ˜æ˜¯å¦æ›´æ–°ç»Ÿè®¡ç‰¹æ€§ï¼Œå½“<code>track_running_stats=False</code>æ—¶ï¼Œ<code>running_mean=None, running_var=None</code>ï¼Œå³æ¯æ¬¡å½’ä¸€åŒ–çš„æ—¶å€™åªç”¨å½“å‰batchçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸ä¼šå¯¹ä¹‹å‰ç®—å‡ºçš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œå¹³æ»‘ã€‚</p><p><strong>å‚è€ƒé“¾æ¥ï¼š</strong></p><ul><li><a href="https://blog.csdn.net/LoseInVain/article/details/86476010" target="_blank" rel="external">Pytorchçš„BatchNormå±‚ä½¿ç”¨ä¸­å®¹æ˜“å‡ºç°çš„é—®é¢˜</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;BNæ˜¯é˜²ç½‘ç»œè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ï¼Œç»†å¾®çš„å·®åˆ«å¯èƒ½å¯¹è¾“å‡ºæ•ˆæœæœ‰å¾ˆå¤§å½±å“ã€‚å› æ­¤éœ€è¦ç†è§£ä¸€ä¸‹PyTorchä¸­BNçš„å…·ä½“å®ç°ã€‚PyTorchçš„æºç ç”¨Cå®ç°çš„torch.batchnormã€‚å¯ä»¥&lt;a href=&quot;https://github.com/marvis/pytorch-
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>å·ç§¯æ ¸çš„ä½ç§©åˆ†è§£</title>
    <link href="http://yoursite.com/2019/07/01/tensor-decompositions/"/>
    <id>http://yoursite.com/2019/07/01/tensor-decompositions/</id>
    <published>2019-07-01T11:25:02.000Z</published>
    <updated>2019-07-28T01:57:23.769Z</updated>
    
    <content type="html"><![CDATA[<p>é€šè¿‡<a href="https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning" target="_blank" rel="external">è¿™ç¯‡blog</a>äº†è§£ä¸€ä¸‹ä½ç§©åˆ†è§£ã€‚</p><p>ä½ç§©åˆ†è§£ä»…ä½œç”¨äºçº¿æ€§ç½‘ç»œå±‚çš„æƒé‡ï¼Œå¯èƒ½å¿½ç•¥ä¸åŒç½‘ç»œå±‚çš„è”ç³»ã€‚</p><blockquote><p>There are works that try to address these issues, and its still an active research area.</p></blockquote><h3 id="å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£"><a href="#å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£" class="headerlink" title="å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£"></a>å…¨è¿æ¥å±‚çš„å¼ é‡åˆ†è§£</h3><p>é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹SVDã€‚å¥‡å¼‚å€¼åˆ†è§£(SVD)æ˜¯å¯¹çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼š<br>$$<br>A_{n \times m}=U_{n \times n} S_{n \times m} V_{m \times m}^{T}<br>$$<br>å…¶ä¸­$S$æ˜¯éè´Ÿå®æ•°å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šå…ƒç´ å³ä¸º$A$çš„<strong>å¥‡å¼‚å€¼</strong>ã€‚é€šå¸¸ä¼šå°†å¥‡å¼‚å€¼ç”±å¤§åˆ°å°æ’åºã€‚$U$å’Œ$V$æ˜¯é…‰çŸ©é˜µï¼Œæ»¡è¶³$U^{T} U=V^{T} V=I$ã€‚å½“æˆ‘ä»¬å–å…¶æœ€å¤§çš„$t$ä¸ªå¥‡å¼‚å€¼ï¼Œå°†å‰©ä¸‹çš„å€¼ç½®0ï¼Œåˆ™èƒ½å¾—åˆ°è¿‘ä¼¼çŸ©é˜µ$\hat{A}=U_{n x t} S_{t x t} V_{m x t}^{T}$ã€‚</p><p>æˆ‘ä»¬å¯¹å…¨è¿æ¥å±‚çš„å…¬å¼$A x+b$è¿›è¡Œåˆ†è§£æœ‰ï¼š$\left(U_{n \times t} S_{t \times t} V_{m \times t}^{T}\right) x+b=U_{n \times t}\left(S_{t \times t} V_{m \times t}^{T} x\right)+b$</p><p>è¿™æ ·å°†ä¸€ä¸ªå¤§çŸ©é˜µè½¬åŒ–ä¸ºä¸¤ä¸ªå°çŸ©é˜µï¼Œå‚æ•°é‡ä» $n \times m$ é™ä¸º $t  (n+m)$</p><h3 id="å·ç§¯å±‚çš„å¼ é‡åˆ†è§£"><a href="#å·ç§¯å±‚çš„å¼ é‡åˆ†è§£" class="headerlink" title="å·ç§¯å±‚çš„å¼ é‡åˆ†è§£"></a>å·ç§¯å±‚çš„å¼ é‡åˆ†è§£</h3><p>ä¸‹é¢ä»‹ç»å¯¹å·ç§¯å±‚è¿›è¡Œå¼ é‡åˆ†è§£æœ€ç»å…¸çš„ä¸¤ç§æ–¹æ³•ï¼šCPåˆ†è§£å’ŒTuckeråˆ†è§£</p><p><strong>CPåˆ†è§£</strong></p><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/1412.6553" target="_blank" rel="external">Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</a></p><p>å¯¹$A$è¿›è¡Œä½ç§©åˆ†è§£æœ‰ï¼š<br>$$<br>A(i, j)=\sum_{n=1}^{R} A_{1}(i, r) A_{2}(j, r), \quad i=\overline{1, n}, \quad j=\overline{1, m}<br>$$<br>å¯¹dç»´çš„$A$è¿›è¡ŒCPåˆ†è§£æœ‰ï¼š<br>$$<br>A\left(i_{1}, \ldots, i_{d}\right)=\sum_{r=1}^{R} A_{1}\left(i_{1}, r\right) \ldots A_{d}\left(i_{d}, r\right)<br>$$<br>å¯¹$d \times d \times S \times T$çš„4Dçš„å·ç§¯æ ¸è¿›è¡ŒCPåˆ†è§£<br>$$<br>K(i, j, s, t)=\sum_{r=1}^{R} K^{x}(i-x+\delta, r) K^{y}(j-y+\delta, r) K^{s}(s, r) K^{t}(t, r)<br>$$<br>åˆ™è¾“å‡ºå¯ä»¥$V$ è¡¨ç¤ºä¸ºï¼š<br>$$<br>V(x, y, t)=\sum_{r=1}^{R} K^{t}(t, r)\left(\sum_{i=x-\delta}^{x+\delta} K^{x}(i-x+\delta, r)\left(\sum_{j=y-\delta}^{y+\delta} K^{y}(j-y+\delta, r)\left(\sum_{s=1}^{S} K^{s}(s, r) U(i, j, s)\right)\right)\right)<br>$$<br>åˆ™1ä¸ªå·ç§¯æ“ä½œå¯ä»¥åˆ†è§£ä¸º4ä¸ªå·ç§¯æ“ä½œï¼š<br>$$<br>\begin{aligned} U^{s}(i, j, r) &amp;=\sum_{s=1}^{S} K^{s}(s, r) U(i, j, s) \\ U^{s y}(i, y, r) &amp;=\sum_{j=1}^{y+\delta} K^{y}(j-y+\delta, r) U^{s}(i, j, r) \\ U^{s y z}(x, y, r) &amp;=\sum_{i=x-\delta}^{x+\delta} K^{x}(i-x+\delta, r) U^{s y}(i, y, r) \\ V(x, y, t) &amp;=\sum_{r=1}^{R} K^{t}(t, r) U^{s y x}(x, y, r) \end{aligned}<br>$$</p><ol><li>ç”¨$1\times1$çš„pointwiseå·ç§¯å°†è¾“å…¥é™è‡³Rçº¬åº¦</li><li>åœ¨å‚ç›´ç»´åº¦åš$d\times1$çš„depthwiseå·ç§¯</li><li><p>åœ¨æ°´å¹³çº¬åº¦åš$d\times1$çš„depthwiseå·ç§¯</p></li><li><p>ç”¨$1\times 1$çš„pointwiseå·ç§¯è·å¾—$T$ç»´çš„è¾“å‡º</p></li></ol><p>å¤æ‚åº¦åˆ†æï¼š</p><ul><li>åŸå§‹å·ç§¯ $STd^2$</li><li>CPåˆ†è§£ $R(S+2d+T)$</li></ul><p><strong>Tuckeråˆ†è§£</strong></p><p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/1511.06530" target="_blank" rel="external">Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications</a></p><p>Tuckeråˆ†è§£ä¹Ÿç§°ä¸ºé«˜é˜¶SVDï¼Œ4Då·ç§¯æ ¸è¡¨ç¤ºä¸ºï¼š<br>$$<br>K(i, j, s, t)=\sum_{r_{1}=1}^{R_{1}} \sum_{r_{2}=1}^{R_{2}} \sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} Câ€™_{r_1, r_2, r_3, r_4} K_{r 1}^{x}(i) K_{r_2}^{y}(j) K_{r_3}^{s}(s) K_{r _4}^{t}(t)<br>$$<br>å·ç§¯æ ¸é€šå¸¸æ¯”è¾ƒå°($3 \times 3$)ï¼Œå°±ä¸å†åœ¨ç©ºé—´ç»´åº¦è¿›è¡Œåˆ†è§£<br>$$<br>K(i, j, s, t)=\sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} Câ€™_{i,j, r_3, r_4} K_{r_3}^{s}(s) K_{r _4}^{t}(t)<br>$$<br>å…¶ä¸­$C$ä»£è¡¨$d \times d \times R_3 \times R_4$ çš„æ ¸å¿ƒå¼ é‡ã€‚</p><p>äºæ˜¯å°†1ä¸ªå·ç§¯æ“ä½œåˆ†è§£ä¸º3ä¸ªå·ç§¯æ“ä½œï¼š<br>$$<br>\begin{aligned} \mathcal{Z}_{h, w, r_{3}} &amp;=\sum_{s=1}^{S} U_{s, r_{3}}^{(3)} \mathcal{X}_{h, w, s} \\ \mathcal{Z}_{h^{\prime}, w^{\prime}, r_{4}}^{D} &amp;=\sum_{i=1}^{D} \sum_{j=1}^{D} \sum_{r_{3}=1}^{R_{3}} \mathcal{C}_{i, j, r_{3}, r_{4}} z_{h_{i} w_{j}, r_{3}} \\ y_{h^{\prime}, w^{\prime}, t} &amp;=\sum_{r_{4}=1}^{R_{4}} U_{t, r_{4}}^{(4)} \mathcal{Z}_{h^{\prime}, w^{\prime}, r_{4}}^{\prime} \end{aligned}<br>$$</p><ol><li>ç”¨$1\times1$çš„pointwiseå·ç§¯å°†è¾“å…¥é™è‡³Rçº¬åº¦</li><li>è¿›è¡Œ$d \times d$çš„å·ç§¯</li><li>ç”¨$1\times 1$çš„pointwiseå·ç§¯è·å¾—$T$ç»´çš„è¾“å‡º</li></ol><h3 id="æŒ‘é€‰åˆ†è§£çš„ç§©"><a href="#æŒ‘é€‰åˆ†è§£çš„ç§©" class="headerlink" title="æŒ‘é€‰åˆ†è§£çš„ç§©"></a>æŒ‘é€‰åˆ†è§£çš„ç§©</h3><p>åœ¨åˆ†è§£çš„æ—¶å€™ç§©$R$çš„é€‰æ‹©å¾ˆé‡è¦ï¼ŒTuckeråˆ†è§£ä¸­ç”¨äº†VBMFçš„æ–¹æ³•ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;é€šè¿‡&lt;a href=&quot;https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;è¿™ç¯‡blog&lt;/a&gt;äº†è§£ä¸€ä¸‹ä½ç§©åˆ†è§£
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.08</title>
    <link href="http://yoursite.com/2019/06/29/weekly-paper-08/"/>
    <id>http://yoursite.com/2019/06/29/weekly-paper-08/</id>
    <published>2019-06-29T02:42:16.000Z</published>
    <updated>2019-07-08T01:08:09.009Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure"><a href="#1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure" class="headerlink" title="1ï¸âƒ£  Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure"></a>1ï¸âƒ£  Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure</h3><p>The main idea of this work is to make some filters close to each other in the same cluster during training, and propose Centripetal SGD (C-SGD). </p><p>the update rule of C-SGD is<br>$$<br>\boldsymbol{F}^{(j)} \leftarrow \boldsymbol{F}^{(j)}+\tau \Delta \boldsymbol{F}^{(j)}<br>$$</p><p>$$<br>\begin{aligned} \Delta \boldsymbol{F}^{(j)}=&amp;-\frac{\sum_{k \in H(j)} \frac{\partial L}{\partial \boldsymbol{F}^{(k)}}}{|H(j)|}-\eta \boldsymbol{F}^{(j)} \\ &amp;+\epsilon\left(\frac{\sum_{k \in H(j)} \boldsymbol{F}^{(k)}}{|H(j)|}-\boldsymbol{F}^{(j)}\right) \end{aligned}<br>$$</p><p>For the filters in the same cluster 1ï¼‰objective function are averaged 2ï¼‰weight decay 3ï¼‰gradually eliminate the difference in the initial values.</p><h3 id="2ï¸âƒ£-Variational-Convolutional-Neural-Network-Pruning"><a href="#2ï¸âƒ£-Variational-Convolutional-Neural-Network-Pruning" class="headerlink" title="2ï¸âƒ£ Variational Convolutional Neural Network Pruning"></a>2ï¸âƒ£ Variational Convolutional Neural Network Pruning</h3><p>æœ¬æ–‡ç”¨å˜åˆ†æ¨ç†æ¥è¿›è¡Œå‰ªææ„Ÿè§‰è¿˜è›®æœ‰æ„æ€çš„ã€‚é¦–å…ˆæ–‡ç« åŸºäºNetwork Slimmingåšäº†ä¸€äº›æ”¹è¿›ã€‚å¯¹äºBNçš„$\gamma$ï¼Œå¦‚æœåªç¨€ç–è¿™ä¸ªå€¼ï¼Œä¸è€ƒè™‘$\beta$çš„å½±å“ï¼Œé‚£å®é™…ä¸Šå½’ä¸€åŒ–åçš„è¾“å‡ºä¸ä¼šä¸º0ï¼Œè€Œæ˜¯åŠ ä¸Š$\beta$åçš„å€¼ï¼Œæ‰€ä»¥æ–‡ç« è€ƒè™‘çš„æ”¹è¿›æ–¹å¼æ˜¯ä»¤$\tilde{\beta} = \gamma \cdot \beta$ï¼Œ<br>$$<br>x_{o u t}=\gamma \cdot B N(x)+\tilde{\beta}<br>$$<br>æ–‡ä¸­å°†è¿™ä¸ª$\gamma$ç§°ä¸º<em>channel saliency</em>ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ ç¨€ç–çš„$\gamma$åŒæ—¶æœ€å¤§åŒ–æ¡ä»¶æ¦‚ç‡$p(y | x, \gamma)$ã€‚</p><p>é¦–å…ˆåˆ©ç”¨è´å¶æ–¯å…¬å¼æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š$ p(\gamma | \mathcal{D})= \frac{p(\gamma) p(\mathcal{D} | \gamma)} { p(\mathcal{D})}$</p><p>ç”±äº$p(\mathcal{D})=\int p(\mathcal{D}, \gamma) d \gamma$éš¾ä»¥è®¡ç®—ï¼Œè¿™ä¸ªåéªŒæ¦‚ç‡åˆ†å¸ƒæˆ‘ä»¬å¾ˆéš¾ç›´æ¥æ±‚çš„ã€‚åœ¨å˜åˆ†æ¨ç†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªå‚æ•°åˆ†å¸ƒ$q_{\phi}(\gamma)$æ¥è¿‘ä¼¼è¿™ä¸ªåéªŒæ¦‚ç‡åˆ†å¸ƒã€‚åˆ©ç”¨KLæ•£åº¦æ‹‰è¿‘ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ï¼š$\min _{\phi} D_{K L}\left(q_{\phi}(\gamma) | p(\gamma | \mathcal{D})\right)$ã€‚ç­‰ä»·äºæœ€å¤§åŒ–ELBOï¼š<br>$$<br>\mathcal{L}(\phi)=L_{\mathcal{D}}(\phi)-D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)<br>$$<br>å…¶ä¸­ï¼Œ$\mathcal{L}_{\mathcal{D}}(\phi)=\sum_{(x, y) \in \mathcal{D}} \mathbb{E}_{q_{\phi}(\gamma)}[\log p(y | x, \gamma)]$</p><p>å¯ä»¥çœ‹åˆ°ç›®æ ‡å‡½æ•°ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯é‡å»ºé¡¹ï¼Œæ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸ºæ­£åˆ™é¡¹ï¼Œåé¢ä¼šå¼•å…¥ä¸€ä¸ªå…ˆéªŒåˆ†å¸ƒå¯¹å‚æ•°è¿›è¡Œæƒ©ç½šï¼Œå³ç¨€ç–$\gamma$ã€‚</p><p>å¯¹äº$\mathcal{L}(\phi)$éœ€è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š</p><ol><li>ç¬¬ä¸€é¡¹ä¸­ç”±äºæœŸæœ›çš„å­˜åœ¨ï¼Œ$\mathcal{L}_{\mathcal{D}}(\phi)$çš„æ¢¯åº¦æ— æ³•ç›´æ¥æ±‚å¾—ã€‚</li><li>ç¬¬äºŒé¡¹å‚æ•°åˆ†å¸ƒ$q_\phi(\gamma)$å’Œå…ˆéªŒåˆ†å¸ƒ$p(\gamma)$çš„é€‰æ‹©ã€‚</li></ol><p>ğŸ”º é—®é¢˜1çš„è§£å†³ï¼š</p><p>â€‹    å¼•å…¥å†å‚åŒ–æŠ€å·§ï¼Œåˆ™$q_{\phi}(\gamma)$å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå¯å¯¼å‡½æ•°$\gamma=f(\phi, \epsilon)$ï¼Œå…¶ä¸­$\epsilon \sim \mathcal{N}(0,1)$<br>$$<br>\mathcal{L}_{\mathcal{D}}(\phi) \simeq \mathcal{L}_{\mathcal{D}}^{\mathcal{A}}(\phi)=\frac{N}{M} \sum_{m=1}^{M} \log p\left(y_{i m} | x_{i m}, \gamma_{i m}=f(\phi, \epsilon)\right)<br>$$<br>â€‹    å…¶ä¸­$M$ä¸ºbatch sizeï¼Œ$N$ä¸ºdataæ•°é‡ã€‚</p><p>â€‹    å°†æ¨¡å‹å‚æ•°$\mathbf{w}$åŠ å…¥ä¼˜åŒ–ç›®æ ‡ä¸­ï¼š<br>$$<br>\mathcal{L}(\phi, \mathbf{w}) \simeq \mathcal{L}_{\mathcal{D}}^{\mathcal{A}}(\phi, \mathbf{w})-D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)<br>$$</p><p>ğŸ”º é—®é¢˜2çš„è§£å†³ï¼š</p><p>â€‹    æœ¬æ–‡é€‰å–é«˜æ–¯åˆ†å¸ƒä½œä¸ºå‚æ•°çš„åˆ†å¸ƒï¼š<br>$$<br>q_{\phi}(\gamma)=\prod_{i=1}^{C} q\left(\gamma_{i}\right), \quad \gamma_{i} \sim \mathcal{N}\left(\mu_{i}, \sigma_{i}\right)<br>$$<br>â€‹    ä¸ºäº†è®©å­¦ä¹ å‡ºçš„å‚æ•°$\phi=(\mu, \sigma)$ä½¿åˆ†å¸ƒ$q_\phi(\gamma)$å°½å¯èƒ½ç¨€ç–ï¼Œæœ¬æ–‡å¼•å…¥çš„å…ˆéªŒåˆ†å¸ƒä¸ºï¼š<br>$$<br>p(\gamma)=\prod_{i=1}^{C} p\left(\gamma_{i}\right), \quad \gamma_{i} \sim \mathcal{N}\left(0, \sigma_{i}^{_}\right)<br>$$<br>â€‹    è¿™æ ·å°±èƒ½è®©$\gamma$å°½å¯èƒ½å‘0å€¼é è¿‘ã€‚<br>$$<br>\begin{aligned} D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right) &amp;=\sum_{i} D_{K L}\left(q_{\phi}\left(\gamma_{i}\right) | p\left(\gamma_{i}\right)\right) \\ &amp;=\sum_{i} \log \frac{\sigma_{i}^{_}}{\sigma_{i}}+\frac{\sigma_{i}^{2}+\mu_{i}^{2}}{2\left(\sigma_{i}^{*}\right)^{2}}-\frac{1}{2} \end{aligned}<br>$$<br>â€‹    è®©ä¸¤ä¸ªåˆ†å¸ƒçš„æ–¹å·®ç›¸åŒï¼Œåˆ™ä¸Šå¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š<br>$$<br>D_{K L}\left(q_{\phi}(\gamma) | p(\gamma)\right)=\sum_{i} k \mu_{i}^{2}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Centripetal-SGD-for-Pruning-Very-Deep-Convolutional-Networks-with-Complicated-Structure&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Centripetal-SGD-for-Pruni
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.07</title>
    <link href="http://yoursite.com/2019/06/25/weekly-paper-07/"/>
    <id>http://yoursite.com/2019/06/25/weekly-paper-07/</id>
    <published>2019-06-25T00:55:36.000Z</published>
    <updated>2019-07-08T01:08:06.520Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques"><a href="#1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques" class="headerlink" title="1ï¸âƒ£  Universally Slimmable Networks and Improved Training Techniques"></a>1ï¸âƒ£  Universally Slimmable Networks and Improved Training Techniques</h3><p>Slimmable networkçš„æ‹“å±•å·¥ä½œï¼Œå°†å›ºå®šå®½åº¦çš„ç½‘ç»œæ‰©å±•åˆ°ä»»æ„å®½åº¦ã€‚æå‡ºäº†3ä¸ªæŒ‘æˆ˜ï¼š</p><ol><li>å¦‚ä½•è§£å†³åŒ…å«batch-normalizationçš„ç½‘ç»œï¼Ÿ</li><li>å¦‚ä½•æ›´æœ‰æ•ˆåœ°è®­ç»ƒUS-Nets</li><li>ä¸å•ç‹¬è®­ç»ƒæŸä¸ªå®½åº¦çš„ç½‘ç»œç›¸æ¯”ï¼ŒUS-Netsæ˜¯å¦‚ä½•æå‡æ•´ä½“ç²¾åº¦çš„ï¼Ÿ</li></ol><p>ğŸ”º é—®é¢˜1çš„è§£å†³ï¼š</p><ol><li><p>è®­ç»ƒé˜¶æ®µæ¯æ¬¡å‰å‘æ—¶ï¼Œè®¡ç®—å‡ºè¯¥batchçš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼š<br>$$<br>\hat{x}_{B}=\gamma \frac{x_{B}-E_{B}\left[x_{B}\right]}{\sqrt{\operatorname{Var}_{B}\left[x_{B}\right]+\epsilon}}+\beta<br>$$<br>è®­ç»ƒè¿‡ç¨‹ä¼šå¯¹å‡å€¼å’Œæ–¹å·®åš<strong>moving averages</strong>ï¼š<br>$$<br>\begin{aligned} \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>$$</p><blockquote><p> éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨PyTorchçš„å®ç°ä¸­ï¼Œæ¯æ¬¡è¿›è¡Œç»Ÿè®¡æ—¶$Var_B = \frac{n}{n-1}Var_B$ï¼Œå…¶ä¸­ $n=c \times h \times w$</p></blockquote><p>æµ‹è¯•é˜¶æ®µï¼Œç”¨ç»Ÿè®¡å€¼$\mu=\mu_{T}ï¼Œ \sigma^2=\sigma^2_T$è¿›è¡Œå½’ä¸€åŒ–ï¼š<br>$$<br>\hat{x}_{t e s t}=\gamma^{_} \frac{x_{t e s t}-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\beta^{_}<br>$$<br>å…¶ä¸­$\gamma^_, \beta^_$æ˜¯bnå­¦å‡ºçš„weightå’Œbiasã€‚è¿›ä¸€æ­¥å¯ä»¥è¡¨ç¤ºä¸ºï¼š<br>$$<br>\hat{x}_{t e s t}=\gamma^{\prime} x_{t e s t}+\beta^{\prime}, \gamma^{\prime}=\frac{\gamma^{_}}{\sqrt{\sigma^{2}+\epsilon}}, \beta^{\prime}=\beta^{_}-\gamma^{\prime} \mu<br>$$</p></li></ol><p>   å¦‚æœå¯¹ä¸åŒå®½åº¦çš„ç½‘ç»œé‡‡ç”¨Shared BNï¼Œç”±äºç‰¹å¾æ˜¯ç›¸åŠ çš„ï¼Œå‰ä¸€å±‚æ˜¯ç”¨ä¸åŒçš„é€šé“æ•°ï¼Œè¾“å‡ºå€¼å°±ä¼šæœ‰æ‰€ä¸åŒï¼Œå‡å€¼å’Œæ–¹å·®ä¹Ÿä¸åŒï¼Œå¯¼è‡´äº†ç»Ÿè®¡å€¼ä¸å‡†ç¡®çš„é—®é¢˜ã€‚Slimmable Networkçš„è§£å†³åŠæ³•æ˜¯å¯¹æ¯ä¸ªå®½åº¦éƒ½è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„BNå±‚ï¼Œä½†å¦‚æœå¯¹æ‰€æœ‰å®½åº¦éƒ½è¿™æ ·åšä»£ä»·å¤ªå¤§äº†ã€‚æœ¬æ–‡çš„è§£å†³åŠæ³•æ˜¯åšexact averagesï¼š<br>   $$<br>   \begin{aligned} m &amp;=(t-1) / t \\ \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>   $$<br>   ç»Ÿè®¡å€¼çš„è®¡ç®—ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œï¼Œè€Œæ˜¯è®­ç»ƒç»“æŸåï¼Œç”¨éšæœºé‡‡æ ·çš„è®­ç»ƒæ•°æ®è¿›è¡Œä¼°è®¡ã€‚</p><p>ğŸ”º é—®é¢˜2çš„è§£å†³ï¼š</p><ol><li>æœ¬æ–‡å‡è®¾æ¨¡å‹çš„è¡¨ç°é™åˆ¶äºå®½åº¦$[0.25 \times, 1.0 \times]$ï¼Œä¼˜åŒ–lower boundå’Œupper boundå°±èƒ½ä¼˜åŒ–æ•´ä¸ªç½‘ç»œã€‚äºæ˜¯æå‡ºäº†<strong>Sandwich Rule</strong>ï¼Œåœ¨æ¯æ¬¡è®­ç»ƒæ—¶éšæœºé‡‡æ ·$n-2$ä¸ªå®½åº¦ï¼ŒåŠ ä¸Šæœ€å°å®½åº¦å’Œæœ€å¤§å®½åº¦ä¸€èµ·è®­ç»ƒã€‚åŒæ—¶è·Ÿè¸ªè¿™ä¸¤ä¸ªæ¨¡å‹çš„éªŒè¯ç²¾åº¦ï¼Œèƒ½å¤§æ¦‚çŸ¥é“US-Netçš„lower boundå’Œupper boundã€‚å¹¶ä¸”ï¼Œè®­ç»ƒæœ€å¤§å®½åº¦çš„æ¨¡å‹å¯ä»¥ç”¨äº<strong>Inplace Distillation</strong>ã€‚æœ€å¤§å®½åº¦çš„æ¨¡å‹ç”¨groud truthåšlossï¼Œè€Œå…¶å®ƒå®½åº¦çš„æ¨¡å‹å¯ä»¥ç”¨æœ€å¤§å®½åº¦æ¨¡å‹é¢„æµ‹å‡ºçš„soft-probabilitiesåšlossã€‚</li></ol><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190703111249984.png" alt="image-20190703111249984"></p><p>æ–‡ç« æœ€ååç€è®¨è®ºäº†å‡ ä¸ªè¯é¢˜ï¼š</p><ol><li>æˆ‘ä»¬èƒ½ä¸èƒ½è®­ç»ƒä¸€ä¸ªéå‡åŒ€çš„US-Netè¿™æ ·æ¯å±‚èƒ½å¤Ÿè°ƒæ•´å®ƒè‡ªå·±çš„å®½åº¦æ¯”ï¼Ÿ</li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Training-Techniques&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Universally-Slimmable-Networks-and-Improved-Train
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflowä¸­çš„PixelShuffle(depth_to_space)</title>
    <link href="http://yoursite.com/2019/06/23/tf-pixshuffle/"/>
    <id>http://yoursite.com/2019/06/23/tf-pixshuffle/</id>
    <published>2019-06-23T08:31:38.000Z</published>
    <updated>2019-06-28T02:10:01.249Z</updated>
    
    <content type="html"><![CDATA[<p>åœ¨å°è¯•å¯¹PixelShuffleå‰çš„å·ç§¯å±‚åšå‰ªææ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¯¹PixelShuffleçš„å…·ä½“æ“ä½œæœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚</p><p>PixelShuffleé€šè¿‡å°†é€šé“é‡æ’å¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œtfä¸­çš„å‡½æ•°æ˜¯<code>tf.depth_to_sapce</code>ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯<code>Tensor</code>ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯éœ€è¦æ”¾å¤§å€æ•°ã€‚å½“è¾“å…¥<code>X</code>çš„å¤§å°ä¸º<code>[1 2 2 16]</code>ï¼Œæ”¾å¤§å€æ•°ä¸º2ï¼ŒHå’ŒWå„ä¹˜2ï¼ŒCé™¤ä»¥4ï¼ŒPixelShuffleåçš„ç»“æœå°±ä¸º<code>[1 4 4 4]</code>ã€‚</p><p>ğŸ”ºå‘ç‚¹1ï¼šæƒ³å½“ç„¶çš„ä»¥ä¸ºå‚ä¸é‡æ’çš„é€šé“æ˜¯<code>[:, :, :, i:i+4]</code></p><p>æµ‹è¯•ä»£ç ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x = tf.range(<span class="number">64</span>)</div><div class="line">x = tf.reshape(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">16</span>])</div><div class="line">y = tf.depth_to_space(x, <span class="number">2</span>) <span class="comment"># [1, 4, 4, 4]</span></div></pre></td></tr></table></figure><p>ä¸‹é¢çœ‹ä¸€ä¸‹å…·ä½“xå’Œyæ¯ä¸ªé€šé“çš„å€¼ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>, <span class="number">16</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">48</span>]]]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">1</span>, <span class="number">17</span>],</div><div class="line">[<span class="number">33</span>, <span class="number">49</span>]]]</div><div class="line"> </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">4</span>, <span class="number">16</span>, <span class="number">20</span>],</div><div class="line">[ <span class="number">8</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">28</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">36</span>, <span class="number">48</span>, <span class="number">52</span>],</div><div class="line">[<span class="number">40</span>, <span class="number">44</span>, <span class="number">56</span>, <span class="number">60</span>]]]</div></pre></td></tr></table></figure><p>å¯ä»¥çœ‹å‡º<code>y</code>çš„ç¬¬0ç»´é€šé“åŒ…å«çš„æ˜¯<code>x</code>é€šé“æ•°ä¸º0ã€4ã€8ã€12çš„ç‰¹å¾å›¾ã€‚å¯è§†åŒ–ä¸€ä¸‹å°±æ˜¯è¿™æ ·çš„æ•ˆæœï¼š</p><p><img src="https://i.loli.net/2019/06/27/5d14b6a1d9bbf35069.png" alt=""></p><p>å°†Yçš„ä¸€ä¸ªé€šé“å•ç‹¬å–å‡ºï¼Œçœ‹ä¸€ä¸‹æ¯ä¸ªç‚¹å±äºåŸæ¥Xçš„å“ªä¸ªåæ ‡ï¼š</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190628095308642.png" alt="image-20190628095308642"></p><p>å¯ä»¥çœ‹åˆ°Yçš„ä¸€ä¸ªé€šé“å®é™…ä¸Šåˆ†æˆ4ä¸ªè±¡é™ï¼Œåœ¨ç©ºé—´ä¸Šç”±<code>(0,0)(0,1)(1,0)(1,1)</code>æ„æˆã€‚åœ¨é€šé“ä¸Šæ¯4ä¸ªé—´éš”æå–å¯¹åº”é€šé“ã€‚è¿™é‡Œçš„é—´éš”å¯¹åº”çš„æ˜¯Pixshuffleåçš„é€šé“æ•°ã€‚</p><p>å‡è®¾åŸå§‹é€šé“æ•°ä¸º<code>c_out</code>ï¼ŒPSåçš„é€šé“æ•°ä¸º<code>ps_out</code>ï¼Œå®é™…ä¸Š<code>y</code>çš„ç¬¬<code>i</code>é€šé“å¯¹åº”çš„æ˜¯<code>x</code>çš„<code>[i, i+ps_out, i+2*ps_out, i+3*ps_out]</code></p><p>è€Œæˆ‘åŸå…ˆç†è§£çš„é€šé“æ’åˆ—æ–¹å¼æ˜¯âŒ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">5</span>],</div><div class="line">[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">6</span>,  <span class="number">7</span>],</div><div class="line">[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">12</span>, <span class="number">13</span>],</div><div class="line">[<span class="number">10</span>, <span class="number">11</span>, <span class="number">14</span>, <span class="number">15</span>]]]</div></pre></td></tr></table></figure><p>ğŸ”ºå‘ç‚¹2ï¼šæå–kä¸ªä¿ç•™çš„é€šé“æ—¶ï¼Œåªéœ€å–ç´¢å¼•çš„å‰kä¸ªå€¼</p><p>å‡è®¾æ”¾å¤§å€ç‡æ˜¯4ï¼Œç”¨L1çš„å‰ªææ–¹å¼ï¼Œéœ€è¦ä¿ç•™çš„é€šé“æ•°ä¸º<code>c_keep</code>ã€‚å½“å¯¹åº”åˆ°å…·ä½“çš„å‰ªæé€šé“çš„æ—¶å€™ï¼Œéœ€è¦æ‰¾åˆ°PixShuffleåå‰ªæ‰é€šé“æ‰€å¯¹åº”çš„åŸå§‹å·ç§¯è¾“å‡ºçš„4ä¸ªé€šé“ã€‚ä»ä¸Šé¢çš„åæ ‡æˆ‘ä»¬å°±å¯ä»¥çœ‹å‡ºï¼Œå‰ªæ‰Yçš„0é€šé“æ—¶ï¼Œéœ€è¦å¯¹åº”å‰ªæ‰Xçš„0ã€4ã€8ã€12é€šé“ã€‚æ¥çœ‹çœ‹å…·ä½“çš„å®ç°ã€‚ä¸»è¦åˆ†ä¸ºå‡ æ­¥ï¼š</p><ol><li>è®¡ç®—Yå¯¹åº”Xçš„é€šé“</li><li>è®¡ç®—Yéœ€è¦ä¿ç•™çš„é€šé“</li><li>å°†Yçš„é€šé“æ˜ å°„å›X</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. è®¡ç®—Yå¯¹åº”Xçš„é€šé“</span></div><div class="line">norm_list, shuffled_idx_list = [], []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(ps_out):</div><div class="line">shuffled_idx = [i+k*ps_out <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">4</span>)]  <span class="comment"># Yé€šé“å¯¹åº”çš„4ä¸ªXé€šé“</span></div><div class="line">shuffled_idx_list.append(shuffled_idx)</div><div class="line">  norm_sum = tf.reduce_sum(tf.gather(norm_value, shuffled_idx)) <span class="comment"># æå–å¯¹åº”ç´¢å¼•çš„é€šé“</span></div><div class="line">  norm_list.append(sess.run(norm_sum))</div><div class="line">  </div><div class="line"><span class="comment"># 2. è®¡ç®—éœ€è¦ä¿ç•™çš„é€šé“</span></div><div class="line">remain_idx = np.sort(np.argsort(norm_list)[::<span class="number">-1</span>][:int(c_keep/<span class="number">4</span>)])</div><div class="line"></div><div class="line">remain_list = []</div><div class="line"><span class="comment"># 3. å°†Yçš„é€šé“æ˜ å°„å›X</span></div><div class="line"><span class="keyword">for</span> remain <span class="keyword">in</span> remain_idx:</div><div class="line">  remain_list.extend(shuffled_idx_list[remain])</div><div class="line">remain_list = np.sort(remain_list)</div></pre></td></tr></table></figure><p>è¿™æ ·<code>remain_list</code>å³åŸå§‹å·ç§¯è¾“å‡ºéœ€è¦å‰ªæ‰çš„é€šé“ç´¢å¼•ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;åœ¨å°è¯•å¯¹PixelShuffleå‰çš„å·ç§¯å±‚åšå‰ªææ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¯¹PixelShuffleçš„å…·ä½“æ“ä½œæœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚&lt;/p&gt;
&lt;p&gt;PixelShuffleé€šè¿‡å°†é€šé“é‡æ’å¯¹å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œtfä¸­çš„å‡½æ•°æ˜¯&lt;code&gt;tf.depth_to_sapce&lt;/code&gt;ï¼Œç¬¬ä¸€ä¸ª
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.06</title>
    <link href="http://yoursite.com/2019/06/18/weekly-paper-06/"/>
    <id>http://yoursite.com/2019/06/18/weekly-paper-06/</id>
    <published>2019-06-18T01:49:16.000Z</published>
    <updated>2019-06-21T09:12:49.701Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers"><a href="#1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers" class="headerlink" title="1ï¸âƒ£ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers"></a>1ï¸âƒ£ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers</h3><p>è¿™ç¯‡å’Œ<a href="https://arxiv.org/abs/1812.08928" target="_blank" rel="external">ICLR2019</a>ã€<a href="https://arxiv.org/abs/1903.05134" target="_blank" rel="external">Universally Slimmable Networks</a>æ˜¯åŒä¸€ä¸ªä½œè€…ï¼Œè§£å†³çš„é—®é¢˜éƒ½æ˜¯é€šé“å‰ªæã€‚ä¸‹é¢å…ˆäº†è§£ä¸€ä¸‹æœ¬æ–‡ã€‚</p><p><strong>Why?</strong></p><p>Most channel pruning methods are grouneded on <strong>the importance of trained weights</strong>, so the slimmed layer usually consists channels of discrete index. Most NAS methods have high computational cost and time cost.</p><p><strong>How?</strong></p><p>Extending the work of slimmable networks and propose AutoSlim. The training process is as following:</p><ol><li><p>Train a slimmable model for a few epochs to get a benchmark performance estimator.</p><ul><li><p>Searching space is defined between the upper bound and lower bound of channel numbers. In each training iteration, randomly sample the number of channels in each layer. In each layer remove a group of channels. </p><blockquote><p>in resents, first sample the channel number of residual dentity pathway and then randomly and independenly sample channel number inside each residual block.</p></blockquote></li></ul></li><li><p>Evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop on validation set.</p></li><li><p>Obtain the optimized channel configurations under different resource constraints.</p></li><li><p>Train optimized architectures individually or slimmable network for full training epochs.</p></li></ol><p>The paper is based on the assumption that <strong>the importance of weight is implicitly ranked by its index</strong>, which means that the smaller index of one filter the more important of this filter.</p><h3 id="2ï¸âƒ£-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks"><a href="#2ï¸âƒ£-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks" class="headerlink" title="2ï¸âƒ£ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks"></a>2ï¸âƒ£ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks</h3><p>The method can be easily found in the title, to gradually grow the depth of DNN.</p><p>The  <em>network</em> is  composed of <em>sub-netwok</em>, and <em>sub-network</em> is composed of <em>sub-modules</em>.<br>$$<br>g\left(\mathcal{X}_{0}\right)=l\left(\boldsymbol{f}_{M-1}\left(\boldsymbol{f}_{M-2}\left(\cdots \boldsymbol{f}_{1}\left(\boldsymbol{f}_{0}\left(\mathcal{X}_{0}\right)\right) \cdots\right)\right)\right)<br>$$<br>AutoGrow is based on Network Morphism, but propose to initilize the last Batch Normalization layer in a residual block of <em>AdamInit</em> insted of <em>ZeroInit</em></p><p><strong>AdamInit</strong></p><p>given the new layers $\mathcal{W}$, we have:<br>$$<br>g\left(\mathcal{X}_{0} ; \mathbb{W}\right)=g\left(\mathcal{X}_{0} ; \mathbb{W} \cup \mathcal{W}\right) \forall \mathcal{X}_{0}<br>$$<br>freeze all parameters except the last Batch Normalization layer in $\mathcal{W}$, use Adam optimizer to optimize the last Batch Normalization layer.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-AutoSlim-Towards-One-Shot-Architecture-Search-f
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.05</title>
    <link href="http://yoursite.com/2019/06/10/weekly-paper-05/"/>
    <id>http://yoursite.com/2019/06/10/weekly-paper-05/</id>
    <published>2019-06-10T02:29:31.000Z</published>
    <updated>2019-06-18T01:49:25.901Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-Dynamic-Capacity-Networks"><a href="#1ï¸âƒ£-Dynamic-Capacity-Networks" class="headerlink" title="1ï¸âƒ£ Dynamic Capacity Networks"></a>1ï¸âƒ£ Dynamic Capacity Networks</h3><p>use two alternative sub-networks: </p><ol><li>coarse layers $f_c$ on the whole input $\mathbf {x}$  </li><li>fine layers $f_f$ at salient regions </li></ol><p><strong>coarse representation vectors</strong><br>$$<br>f_{c}(\mathbf{x})=\left\{\mathbf{c}_{i, j} |(i, j) \in\left[1, s_{1}\right] \times\left[1, s_{2}\right]\right\}<br>$$</p><p>$$<br>h_{c}(\mathbf{x})= \mathbf{o}_c = g\left(f_{c}(\mathbf{x})\right)<br>$$</p><p>$\mathbf{c}_{i, j}=f_{c}\left(\mathbf{x}_{i, j}\right) \in \mathbb{R}^{D}$ </p><p><strong>salient input regions</strong><br>$$<br>H=-\sum_{l=1}^{C} \mathbf{o}_{c}^{(l)} \log \mathbf{o}_{c}^{(l)}<br>$$</p><p>$$<br>M_{i, j}=\left|\nabla_{\mathbf{c}_{i, j}} H\right|_{2}<br>$$</p><p>$C$ is the number of class labels, $\mathbf{M} \in \mathbb{R}^{s_{1} \times s_{2}}$</p><p>select top $k$ input regions $\mathbf{X}^{s}=\left\{\mathbf{x}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}$ based on $\mathbf{M}$</p><p><strong>fine representation vectors</strong><br>$$<br>f_{f}\left(\mathbf{X}^{s}\right)=\left\{\mathbf{f}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}<br>$$<br>refined representation $f_r(\mathbf {x})$ by combining $f_c(\mathbf{x})$ and $f_f(\mathbf{X}^s)$</p><p><strong>loss</strong></p><ol><li><p>Cross Entropy<br>$$<br>J=-\sum_{i=1}^{m} \log p\left(y^{(i)} | \mathbf{x}^{(i)} ; \theta\right)<br>$$</p></li><li><p>encourage similarity between the coarse and fine representations</p></li></ol><p>$$<br>\sum_{\mathbf{x}_{i, j} \in \mathbf{X}^{s}}\left|f_{c}\left(\mathbf{x}_{i, j}\right)-f_{f}\left(\mathbf{x}_{i, j}\right)\right|_{2}^{2}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-Dynamic-Capacity-Networks&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-Dynamic-Capacity-Networks&quot; class=&quot;headerlink&quot; title=&quot;1ï¸âƒ£ Dynamic Capacity Networks&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Gumbel Softmax</title>
    <link href="http://yoursite.com/2019/05/28/gumbel-softmax/"/>
    <id>http://yoursite.com/2019/05/28/gumbel-softmax/</id>
    <published>2019-05-28T07:29:09.000Z</published>
    <updated>2019-07-16T00:34:11.969Z</updated>
    
    <content type="html"><![CDATA[<p>åœ¨PAGé‡Œå‘ç°äº†Gumbel Sampling Trickï¼ŒæŠŠç¦»æ•£çš„é‡‡æ ·è¿‡ç¨‹ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥ï¼Œäºæ˜¯å¯ä»¥æ”¾è¿›ç¥ç»ç½‘ç»œä¸­è¿›è¡Œæ±‚å¯¼å’Œåå‘ï¼Œè§‰å¾—æ˜¯å¾ˆæœ‰æ„æ€çš„å·¥ä½œï¼Œæƒ³è¦å¤šåŠ æ·±ä¸€äº›äº†è§£ã€‚</p><h3 id="é—®é¢˜å¼•å…¥"><a href="#é—®é¢˜å¼•å…¥" class="headerlink" title="é—®é¢˜å¼•å…¥"></a>é—®é¢˜å¼•å…¥</h3><p>é€šè¿‡<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">åšå®¢</a>å…¥äº†ä¸€ä¸‹å°é—¨ï¼Œç»“åˆ<a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="external">çŸ¥ä¹</a>ï¼Œé¦–å…ˆæ¥ç†è§£ä¸€ä¸‹Gumbel Sampling Trickç”¨æ¥åšä»€ä¹ˆã€‚</p><blockquote><p>å·²çŸ¥ä¸€ä¸ªç¦»æ•£éšæœºå˜é‡Xçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æƒ³å¾—åˆ°ä¸€äº›æœä»è¿™ä¸ªåˆ†å¸ƒçš„ç¦»æ•£çš„xçš„å€¼ã€‚</p></blockquote><p>æ¯”è¾ƒç®€å•çš„æ–¹æ³•æ˜¯ç”¨<code>np.random.choice</code>ã€‚æ¯”å¦‚æˆ‘ä»¬ç°åœ¨æœ‰5ä¸ªå€¼ï¼Œæ¦‚ç‡åˆ†å¸ƒæ˜¯<code>[0.1, 0, 0.3, 0.6, 0]</code>ï¼Œå³ç¬¬4ä¸ªå…ƒç´ æœ€æœ‰å¯èƒ½è¢«é‡‡æ ·åˆ°ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</div><div class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure><p>è¿™æ ·æˆ‘ä»¬æ˜¯è·å–åˆ°äº†å€¼ï¼Œä½†æ˜¯è¿™ä¸ªè¿‡ç¨‹åœ¨ç¥ç»ç½‘ç»œä¸­æ— æ³•æ±‚å¯¼å’Œæ–¹å‘ã€‚äºæ˜¯gumbel-maxå‡ºç°äº†ï¼š</p><blockquote><p>å°†é‡‡æ ·çš„è¿‡ç¨‹å…¬å¼åŒ–ï¼Œå…¬å¼ä¸­çš„å‚æ•°ä¸ºç¦»æ•£éšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p></blockquote><p>$$<br>z_{i}=\left\{\begin{array}{l}{1, i=\operatorname{argmax}_{j}\left(\log \left(p_{j}\right)+g_{j}\right)} \\ {0, \text { otherwise }}\end{array}\right.<br>$$</p><p>å…¶ä¸­$g_{i}$ä»£è¡¨gumbelå™ªå£°ï¼Œ$g_{i}=-\log \left(-\log \left(u_{i}\right)\right), u_{i} \sim U n i f o r m(0,1)$ã€‚è¾“å‡º$z_i$æ˜¯ä¸€ä¸ª$j$ç»´çš„one-hotå‘é‡ã€‚</p><p>ç”±äºargmaxä¸å¯å¯¼ï¼Œç”¨å¯å¯¼çš„softmaxæ›¿ä»£argmax<br>$$<br>\boldsymbol{z}=\operatorname{softmax}((\log (\boldsymbol{p})+\boldsymbol{g}) / \tau)<br>$$<br>å‚æ•°$ \tau$è¶Šå°ï¼Œ$z$è¶Šæ¥è¿‘one-hotå‘é‡ã€‚</p><blockquote><p>æˆ‘ä»¬æŠŠä¸å¯å¯¼çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»xæœ¬èº«è½¬å«åˆ°äº†æ±‚å–xçš„å…¬å¼ä¸­çš„ä¸€é¡¹gä¸Šé¢ï¼Œè€Œgä¸ä¾èµ–äºæ¦‚ç‡åˆ†å¸ƒpã€‚è¿™æ ·ä¸€æ¥ï¼Œxå¯¹pä»ç„¶æ˜¯å¯å¯¼çš„ï¼Œè€Œæˆ‘ä»¬å¾—åˆ°çš„xä»ç„¶æ˜¯ç¦»æ•£å€¼çš„é‡‡æ ·ã€‚è¿™æ ·çš„é‡‡æ ·è¿‡ç¨‹è½¬å«çš„æŠ€å·§å«å†å‚åŒ–æŠ€å·§(reparameterization trick)</p></blockquote><p>é‚£ä¹ˆç½‘ç»œæœ‰å“ªäº›åœ°æ–¹éœ€è¦é‡‡æ ·å‘¢ï¼Ÿæ¥ä¸‹æ¥äº†è§£ä¸€ä¸‹VAEçš„ç›¸å…³åº”ç”¨ã€‚</p><h3 id="ç›¸å…³åº”ç”¨"><a href="#ç›¸å…³åº”ç”¨" class="headerlink" title="ç›¸å…³åº”ç”¨"></a>ç›¸å…³åº”ç”¨</h3><p><strong>å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨VAE</strong></p><p><a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="external">è¿™ç¯‡åšå®¢</a> è§£é‡Šå¾—å¾ˆå¥½ï¼Œè‡ªåŠ¨ç¼–ç å™¨ç”±ç¼–ç å™¨(encoder, E)å’Œè§£ç å™¨(decoder, D)æ„æˆï¼ŒEå¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆéšå‘é‡ï¼Œ Då¯¹éšå‘é‡è¿›è¡Œè§£ç ï¼Œè¾“å‡ºå›¾åƒã€‚</p><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165000500-1207992534.jpg" alt="img"></p><p>ä½†æ˜¯è¿™æ ·æˆ‘ä»¬å¿…é¡»é€šè¿‡å›¾åƒæ¥ç”Ÿæˆéšå‘é‡ï¼Œå±€é™æ€§è¾ƒå¤§ï¼Œå¯ä¸å¯ä»¥éšä¾¿æ¥ä¸€ä¸ªéšå‘é‡ï¼Œè¾“å…¥è¿›Då°±èƒ½ç”Ÿæˆå›¾ç‰‡å‘¢ï¼Ÿäºæ˜¯VAEå°±å‡ºç°äº†ã€‚</p><blockquote><p>é™åˆ¶ç¼–ç å™¨ç”Ÿæˆæœä»å•å…ƒé«˜æ–¯åˆ†å¸ƒçš„éšå‘é‡ã€‚</p></blockquote><p>å› æ­¤å­¦ä¹ ç›®æ ‡å°±å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š1ï¼‰ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒå°½å¯èƒ½æ¥è¿‘ï¼› 2ï¼‰éšå˜é‡æœä»å•å…ƒé«˜æ–¯åˆ†å¸ƒ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">generation_loss = mean(square(generated_image - real_image))  </div><div class="line">latent_loss = KL-Divergence(latent_variable, unit_gaussian)  </div><div class="line">loss = generation_loss + latent_loss</div></pre></td></tr></table></figure><p>ä¸ºäº†ä¼˜åŒ–KLæ•£åº¦ï¼Œéœ€è¦å¼•å…¥ğŸ‘†ğŸ»æåˆ°è¿‡çš„å†å‚åŒ–æŠ€å·§ã€‚</p><blockquote><p>Eä¸ç›´æ¥ç”Ÿæˆéšå‘é‡ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªå‡å€¼å‘é‡å’Œä¸€ä¸ªæ–¹å·®å‘é‡ã€‚å†é€šè¿‡å‡å€¼å’Œæ–¹å·®é‡‡æ ·å‡ºéšå‘é‡ã€‚</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165407236-1369432498.png" alt="img"> </p><p>è‡³æ­¤æˆ‘ä»¬æ˜ç™½äº†<strong>é‡‡æ ·</strong>æ˜¯ä¸ºäº†è®©æ•°æ®å°½å¯èƒ½æœä»æŸä¸€åˆ†å¸ƒï¼Œé€šè¿‡<strong>å†å‚åŒ–æŠ€å·§</strong>æ¥å­¦ä¹ è¿™ä¸ªåˆ†å¸ƒçš„å‚æ•°ã€‚é«˜æ–¯åˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œç›´æ¥å¯æ±‚å¯¼ï¼Œé‚£ä¸€äº›ä¸è¿ç»­çš„ç¦»æ•£åˆ†å¸ƒæ€ä¹ˆåŠå‘€ï¼Ÿè¿™å°±å›åˆ°äº†ä¸€å¼€å§‹çš„é—®é¢˜ã€ŒGumbel Sampling Trickã€ã€‚</p><p><strong>åˆ†ç±»å†å‚åŒ–(Categorical reparameterization)</strong></p><p>ICLR 2017çš„<a href="https://arxiv.org/pdf/1611.01144.pdf" target="_blank" rel="external">è¿™ç¯‡æ–‡ç« </a> å°±åˆ©ç”¨Gumbel-Softmaxåˆ†å¸ƒï¼Œå°†ç¦»æ•£çš„åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·è¿‡ç¨‹è½¬åŒ–ä¸ºäº†å¯æ±‚å¯¼çš„è¿‡ç¨‹ã€‚</p><p><img src="https://i.loli.net/2019/05/30/5cef4476d308a17728.png" alt=""></p><p>ä¸Šå›¾åæ˜ äº†å‚æ•°$ \tau$å¯¹è¿ç»­æ¦‚ç‡åˆ†å¸ƒ(a)å’Œç¦»æ•£çš„one-hotç±»åˆ«åˆ†å¸ƒçš„å½±å“ã€‚å½“$ \tau$å¤ªå°æ—¶ä¼šå¯¼è‡´æ¢¯åº¦çš„æ–¹å·®è¿‡å¤§ï¼Œæ‰€ä»¥æ–‡ç« åœ¨å®éªŒä¸­ç”¨äº†é€€ç«çš„ç­–ç•¥æ¥é€æ¸å‡å°å‚æ•°$ \tau$ã€‚è¿˜å¯ä»¥åˆ©ç”¨ç†µæ­£åˆ™æ¥å­¦ä¹ $\tau$ï¼Œè‡ªåŠ¨è°ƒæ•´Gumbel-Softmaxåˆ†å¸ƒé‡‡æ ·çš„ç½®ä¿¡åº¦ã€‚</p><p>æœ¬æ–‡çš„è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨Straight-Through (ST) Gumbel Estimatorï¼Œå³å‰å‘ç”¨argmaxï¼Œæ¢¯åº¦å›ä¼ æ—¶ç”¨softmaxçš„æ¢¯åº¦ã€‚</p><p><strong>å‚è€ƒé“¾æ¥ï¼š</strong></p><ul><li><p><a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">Gumbel-Softmax Trickå’ŒGumbelåˆ†å¸ƒ</a></p></li><li><p><a href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/" target="_blank" rel="external">The Gumbel-Max Trick for Discrete Distributions</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;åœ¨PAGé‡Œå‘ç°äº†Gumbel Sampling Trickï¼ŒæŠŠç¦»æ•£çš„é‡‡æ ·è¿‡ç¨‹ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥ï¼Œäºæ˜¯å¯ä»¥æ”¾è¿›ç¥ç»ç½‘ç»œä¸­è¿›è¡Œæ±‚å¯¼å’Œåå‘ï¼Œè§‰å¾—æ˜¯å¾ˆæœ‰æ„æ€çš„å·¥ä½œï¼Œæƒ³è¦å¤šåŠ æ·±ä¸€äº›äº†è§£ã€‚&lt;/p&gt;
&lt;h3 id=&quot;é—®é¢˜å¼•å…¥&quot;&gt;&lt;a href=&quot;#é—®é¢˜å¼•å…¥&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>weekly-paper-04</title>
    <link href="http://yoursite.com/2019/05/25/weekly-paper-04/"/>
    <id>http://yoursite.com/2019/05/25/weekly-paper-04/</id>
    <published>2019-05-25T08:30:05.000Z</published>
    <updated>2019-05-27T12:12:52.191Z</updated>
    
    <content type="html"><![CDATA[<p>ä¸ºäº†é”»ç‚¼è‡ªå·±çš„è‹±è¯­å†™ä½œèƒ½åŠ›ï¼Œä»¥åå°½é‡ç”¨è‹±æ–‡åšè¿›è¡Œå½’çº³ï¼ˆâŒ˜+C &amp; âŒ˜+Vï¼‰ï½</p><h3 id="1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression"><a href="#1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression" class="headerlink" title="1ï¸âƒ£ To prune, or not to prune: exploring the efficacy of pruning for model compression"></a>1ï¸âƒ£ To prune, or not to prune: exploring the efficacy of pruning for model compression</h3><p>è¿™ç¯‡æ˜¯TensorFlowè‡ªå·±å‡ºçš„ï¼Œç›´æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èåˆL1å‰ªæã€‚é€šè¿‡å°†æ“ä½œèå…¥TensoFlowçš„training graphï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æƒé‡è¿›è¡Œæ’åºï¼Œç”¨ä¸€ä¸ªmaskå°†æœ€å°çš„weightsç½®0ã€‚ä»inital sparsity values $s_i$å¼€å§‹ï¼Œä»¥$\Delta t$ çš„å‰ªæé¢‘ç‡ï¼Œæœ€ç»ˆè¾¾åˆ°final sparsity value $s_f$<br>$$<br>s_{t}=s_{f}+\left(s_{i}-s_{f}\right)\left(1-\frac{t-t_{0}}{n \Delta t}\right)^{3} \text { for } t \in\left\{t_{0}, \quad t_{0}+\Delta t, \ldots, t_{0}+n \Delta t\right\}<br>$$<br>masksæ¯éš”$\Delta t$æ›´æ–°ä¸€æ¬¡ï¼Œç›´åˆ°è¾¾åˆ°$s_f$åä¸å†æ›´æ–°ã€‚åŒæ—¶æ–‡ç« è¡¨æ˜ï¼Œ$n$çš„é€‰æ‹©ä¸å­¦ä¹ ç‡çš„ä¸‹é™ç­–ç•¥å¯†åˆ‡ç›¸å…³ã€‚</p><h3 id="2ï¸âƒ£-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS"><a href="#2ï¸âƒ£-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS" class="headerlink" title="2ï¸âƒ£ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS"></a>2ï¸âƒ£ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</h3><p>ğŸ“<a href="https://github.com/metalbubble/cnnvisualizer" target="_blank" rel="external">Github Repo</a></p><p><strong>Contributions</strong></p><ul><li>object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet</li><li>the same network can do both object localization and scene recognition in a single forward-pass.</li></ul><p><strong>Experiments</strong></p><ul><li><p>identify the differences in the type of images preferred at the different layers of each network</p></li><li><p>Places-CNN and ImageNet-CNN  prefer similar images in the earlier layers, while the later layers tend to be more specialized to the specific task of scene or object categorization.</p></li><li><p>understand the nature of the representation that the network is learning</p><ul><li><em>simplifying the input images:</em> 1) removing segments from the image to produce the smallest decrease of the correct classification score until the image is incorrectly classified 2) generate the minimal image representations using image set of SUN database. =&gt; use minimal image representations as inputs to show the contribute important information for the network to recognize the scene.</li><li><em>visualize the receptive fields (RFs) of units and their activatoin patterns:</em> use sliding-window to identify which regions of the image led to the high unit activations. =&gt; as the layers go deeper the RF size gradually increases and the activation regions become more semantically meaningful.</li><li><em>understan and quantify the precise semantic learnd by each unit: </em>ask AMT to indentify the common concepts that exists between the top scoring segmentations for each unit.</li></ul></li><li><p>emergence of objects as the internal representation</p><ul><li><p>what object classes emerge? =&gt; use pool5 to show the distribution of objects</p></li><li><p>why do those obejcts emerge? </p><ul><li><p>possibility 1:  the objects correspond to the most frequent ones in the database. (correlation is 0.54)</p></li><li><p>possibility 2:  the objects that allow discriminatin among scene categories. (correlation is 0.84)</p><p>=&gt; the network is automatically identifying the most discriminative object categories to a large extent</p></li></ul></li></ul></li></ul><h3 id="3ï¸âƒ£-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations"><a href="#3ï¸âƒ£-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations" class="headerlink" title="3ï¸âƒ£ Network Dissection: Quantifying Interpretability of Deep Visual Representations"></a>3ï¸âƒ£ Network Dissection: Quantifying Interpretability of Deep Visual Representations</h3><p>ğŸ“<a href="https://github.com/CSAILVision/NetDissect-Lite" target="_blank" rel="external">Github Repo</a></p><p><strong>Questions</strong></p><ul><li>What is a disentangled representation, and how can its factors be quantified and detected?</li><li><p>Do interpretable hidden units reflect a special alignment of feature space, or are interpretations a chimera?</p></li><li><p>What conditions in state-of-the-art training lead to representations with greater or lesser entanglement?</p></li></ul><p><strong>Measurement of interpretability: three-step process of Network Dissection</strong></p><ol><li><p>Identify a broad set of human-labeld visual concepts.</p></li><li><p>Gather hidden variablesâ€™ response to known concepts.</p><ul><li>draw concepts $c$ from the Broden dataset.</li></ul></li><li><p>Quantify alignment of hidden variable â€” concept pairs.</p><ul><li><p>Scoring Unit Interpretability</p><p>input image $x$, activation map $A_{k}(\mathbf{x}) \stackrel{scale up}{\longrightarrow}S_k(x) $ï¼Œindividual unit activations $a_k$</p><p>top quantile level $T_k$ï¼š $P\left(a_{k}&gt;T_{k}\right)=0.005$</p><p>binary segmentationï¼š$M_{k}(\mathbf{x}) \equiv S_{k}(\mathbf{x}) \geq T_{k}$</p><p>input annotaion mask $L_c$ </p><p>scoreï¼šthe accuracy of unit $k$ in detecting concept $c$<br>$$<br>I o U_{k, c}=\frac{\sum\left|M_{k}(\mathbf{x}) \cap L_{c}(\mathbf{x})\right|}{\sum\left|M_{k}(\mathbf{x}) \cup L_{c}(\mathbf{x})\right|}<br>$$</p></li></ul></li></ol><h3 id="4ï¸âƒ£-Pixel-wise-Attentional-Gating-for-Scene-Parsing"><a href="#4ï¸âƒ£-Pixel-wise-Attentional-Gating-for-Scene-Parsing" class="headerlink" title="4ï¸âƒ£ Pixel-wise Attentional Gating for Scene Parsing"></a>4ï¸âƒ£ Pixel-wise Attentional Gating for Scene Parsing</h3><p><strong>Contributions:</strong></p><ul><li>Dynamic computation depth: insert PAG at multiple lyaers of ResNet to control computational parsimony.</li><li>Dynamic spatial pooling: adaptively chooses the proper pooling size for each pixel to aggregate information for inference.</li><li>Experimetns on various pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation.</li></ul><p><img src="https://i.loli.net/2019/05/27/5ceb52642beaa24177.png" alt=""></p><p>binary spatial mask $\mathbf{G}$ on ResBottleneck:<br>$$<br>\begin{array}{ll}{\mathbf{X}=\mathcal{F}^{1}(\mathbf{I})} &amp; {\mathbf{X}=\mathcal{F}^{1}(\mathbf{I}), \mathbf{G}=\mathcal{G}(\mathbf{I})} \\ {\mathbf{Y}=\mathcal{F}^{2}(\mathbf{X})} &amp; {\mathbf{Y}=\mathcal{F}_{\mathbf{G}}^{2}(\mathbf{X})} \\ {\mathbf{Z}=\mathcal{F}^{3}(\mathbf{Y})} &amp; {\mathbf{Z}=\mathcal{F}_{\mathbf{G}}^{3}(\overline{\mathbf{G}} \odot \mathbf{X}+\mathbf{G} \odot \mathbf{Y})} \\ {\mathbf{O}=\mathbf{I}+\mathbf{Z}} &amp; {\mathbf{O}=\mathbf{I}+\mathbf{Z}}\end{array}<br>$$<br><strong>Methods:</strong></p><ul><li><p>Learning attention maps</p><blockquote><p>The key to the proposed PAG is the gating function G that produces a discrete (binary) mask which allows for reduced computation. However, producing the binary mask using hard thresholding is non-differentiable, and thus cannot be simply incorporated in CNN where gradient descent is used for training. To bridge the gap, we exploit the Gumbel-Max trick [19] and its recent continuous relaxation [39, 28].</p></blockquote><p>Gumbel distribution  $m \equiv-\log (-\log (u))$, where $u \sim \mathcal{U}[0,1]$</p><p>$g$ is a discrete random variable with probabilities  $P(g=k) \propto a_{k}$</p><p>$\left\{m_{k}\right\}_{k=1, \dots, K}$ is a sequence of i.i.d Gumbel random variables </p><p>sample from the discrete variable:<br>$$<br>g=\underset{k=1, \ldots, K}{\operatorname{argmax}}\left(\log \alpha_{k}+m_{k}\right)<br>$$<br>Gumbel Sampling Trick (replaces the argmax operation with a softmax): </p></li></ul><p>$$<br>\mathbf{g}=\operatorname{softmax}((\log (\boldsymbol{\alpha})+\mathbf{m}) / \tau)<br>$$</p><p>â€‹        <strong>forwardd pass</strong>: discrete smaples of the argmax </p><p>â€‹        <strong>backward pass</strong>: compute gradient of the softmax relaxation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ä¸ºäº†é”»ç‚¼è‡ªå·±çš„è‹±è¯­å†™ä½œèƒ½åŠ›ï¼Œä»¥åå°½é‡ç”¨è‹±æ–‡åšè¿›è¡Œå½’çº³ï¼ˆâŒ˜+C &amp;amp; âŒ˜+Vï¼‰ï½&lt;/p&gt;
&lt;h3 id=&quot;1ï¸âƒ£-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compressi
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.03</title>
    <link href="http://yoursite.com/2019/05/19/weekly-paper-03/"/>
    <id>http://yoursite.com/2019/05/19/weekly-paper-03/</id>
    <published>2019-05-19T13:09:17.000Z</published>
    <updated>2019-05-23T10:42:12.181Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition"><a href="#1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition" class="headerlink" title="1ï¸âƒ£ On Compressing Deep Models by Low Rank and Sparse Decomposition"></a>1ï¸âƒ£ On Compressing Deep Models by Low Rank and Sparse Decomposition</h3><p>æœ¬æ–‡å°†ç½‘ç»œæƒé‡åˆ†è§£æˆä½ç§©å’Œç¨€ç–çš„æˆåˆ†ï¼Œåˆ©ç”¨è´ªå¿ƒåŒè¾¹åˆ†è§£ï¼ˆGreBdecï¼‰ç®—æ³•è¿›è¡Œæ¨¡å‹å‹ç¼©ã€‚</p><p>ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{rank}(L) \leq r} \\ &amp;card(S) \leq c \end{array}<br>$$<br>å‡è®¾$L=UV$ï¼Œå…¶ä¸­$U \in R^{m \times r}, V \in R^{r \times k}$ã€‚æœ¬æ–‡ç”¨ä¸¤ä¸ªå·ç§¯å±‚è¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œ$V$å°†é€šé“æ•°æ˜ å°„åˆ°$r$ï¼Œ$U$ä»£è¡¨$1\times1$å·ç§¯ã€‚ç„¶åæŠŠä½ç§©è¿‘ä¼¼çš„ç»“æœå’Œç¨€ç–çš„ç»“æœç›¸åŠ åˆ©ç”¨maskä¹˜åˆ°åŸfiltersä¸Šï¼Œä¿®æ”¹ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2 n}|Y-(L+S) X|_{F}^{2}} \\ {\text {s.t.}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2} \leq \gamma} \\ &amp; rank(L) \leq r, \\ &amp; card(S) \leq c.\end{array}<br>$$<br>ç­‰åŒäºåˆ©ç”¨è¿­ä»£ä¼˜åŒ–ç­–ç•¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼š<br>$$<br>\frac{1}{2 n}|Y-(L+S) X|_{2}^{2}+\frac{\lambda}{2}|W-L-S|_{F}^{2}<br>$$<br>å…¶ä¸­<br>$$<br>\left\{\begin{array}{l}{L_{i}=\text { TruncatedGSVD }\left(B_{i} A^{\dagger}, r\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>æœ¬æ–‡ç”¨SVD-freeçš„GreBdecç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œä»¤$L=UV$<br>$$<br>\begin{array}{cl}{\min _{U, V, S}} &amp; {\frac{1}{2 n}|Y-(U V+S) X|_{F}^{2}+\frac{\lambda}{2}|W-U V-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{card}(S) \leq c}\end{array}<br>$$<br>$U,V,S$é€šè¿‡ä»¥ä¸‹å…¬å¼æ›´æ–°ï¼š<br>$$<br>\left\{\begin{array}{l}{U_{i}=B_{i} V_{i-1}^{\top}\left(V_{i-1} A V_{i-1}^{\top}\right)^{\dagger}} \\ {V_{i}=\left(U_{i}^{\top} U_{i}\right)^{\dagger} U_{i}^{\top}\left(B_{i} A^{\dagger}\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>ç„¶ååˆç»è¿‡ä¸€ç•ªå˜æ¢ä½œè€…åˆ©ç”¨QRåˆ†è§£å¾—åˆ°ä¸€ä¸ªè®©$U,V$æ›´å¿«æ›´æ–°çš„è§„åˆ™ï¼š<br>$$<br>\left\{\begin{array}{l}{U_{i}=Q, Q R\left(B_{i} V^{\top}\right)=Q R} \\ {V_{i}=Q^{\top}\left(B_{i} A^{\dagger}\right)}\end{array}\right.<br>$$<br><img src="https://i.loli.net/2019/05/20/5ce2015dee80b29804.png" alt=""></p><p>###2ï¸âƒ£ Spatial Transformer Networks</p><p>å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç©ºé—´ä¸Šçš„å˜æ¢ï¼Œä»¥å­¦åˆ°å›¾åƒçš„ä¸å˜æ€§(invariance)ã€‚é…åˆ<a href="https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html" target="_blank" rel="external">PyTorch Tutorial</a>é£Ÿç”¨ã€‚</p><p><img src="https://i.loli.net/2019/05/23/5ce663632bbf215753.png" alt=""></p><p>STNä¹Ÿç±»ä¼¼ä¸€ä¸ªæ’ä»¶ï¼Œä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼š</p><ul><li>Localisation netï¼šè¾“å…¥feature map $U \in \mathbb{R}^{H \times W \times C}$ï¼Œè¾“å‡ºå˜æ¢å‚æ•°$\theta=f_{\mathrm{loc}}(U)$ã€‚å…¶ä¸­$\theta$æ˜¯ä¸€ä¸ª6ç»´çš„ä»¿å°„å˜æ¢ã€‚</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer localization-network</span></div><div class="line">self.localization = nn.Sequential(</div><div class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>)</div><div class="line">)</div></pre></td></tr></table></figure><ul><li>Grid generatorï¼šå¯¹å›¾åƒç”¨$A_\theta$è¿›è¡Œ2Dä»¿å°„å˜æ¢ï¼Œå…¶ä¸­$(x_i^t, y_I^t)$ä¸ºtargetåƒç´ ç‚¹åæ ‡ï¼Œ$\left(x_{i}^{s}, y_{i}^{s}\right)$ä¸ºsourceé‡‡æ ·ç‚¹çš„åæ ‡ã€‚</li></ul><p>$$<br>\left( \begin{array}{c}{x_{i}^{s}} \\ {y_{i}^{s}}\end{array}\right)=\mathcal{T}_{\theta}\left(G_{i}\right)=\mathrm{A}_{\theta} \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)=\left[ \begin{array}{ccc}{\theta_{11}} &amp; {\theta_{12}} &amp; {\theta_{13}} \\ {\theta_{21}} &amp; {\theta_{22}} &amp; {\theta_{23}}\end{array}\right] \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Regressor for the 3 * 2 affine matrix</span></div><div class="line">self.fc_loc = nn.Sequential(</div><div class="line">  nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)</div><div class="line">)</div></pre></td></tr></table></figure><p>ä¸ºäº†åœ¨$U$ä¸Šåº”ç”¨ç©ºé—´å˜æ¢è¾“å‡º$V$ï¼Œéœ€è¦ä¸€ä¸ªå¯å¯¼çš„é‡‡æ ·å‡½æ•°ç”Ÿæˆé‡‡æ ·ç‚¹$\mathcal{T}_\theta(G)$ã€‚<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} k\left(x_{i}^{s}-m ; \Phi_{x}\right) k\left(y_{i}^{s}-n ; \Phi_{y}\right) \forall i \in\left[1 \ldots H^{\prime} W^{\prime}\right] \forall c \in[1 \ldots C]<br>$$<br>å…¶ä¸­$k$ä¸ºsampling kernelï¼Œå¯ä»¥å®šä¹‰ä¸ºinteger sampling kernel:<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \delta\left(\left\lfloor x_{i}^{s}+0.5\right\rfloor- m\right) \delta\left(\left\lfloor y_{i}^{s}+0.5\right\rfloor- n\right)<br>$$<br>ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºbilinear sampling kernelï¼š<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|x_{i}^{s}-m\right|\right) \max \left(0,1-\left|y_{i}^{s}-n\right|\right)<br>$$<br>å¯¹è¾“å…¥æ±‚åå¯¼æœ‰ï¼š<br>$$<br>\frac{\partial V_{i}^{c}}{\partial x_{i}^{s}}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|y_{i}^{s}-n\right|\right) \left\{\begin{array}{ll}{0} &amp; {\text { if }\left|m-x_{i}^{s}\right| \geq 1} \\ {1} &amp; {\text { if } m \geq x_{i}^{s}} \\ {-1} &amp; {\text { if } m<x_{i}^{s}}\end{array}\right. $$="" æŠŠ<em="">localisation network, grid generator, samplerç»“åˆèµ·æ¥æ„æˆä¸€ä¸ªSTNæ¨¡å—ï¼š</x_{i}^{s}}\end{array}\right.></p><p><strong>STN</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer network forward function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stn</span><span class="params">(self, x)</span>:</span></div><div class="line">  xs = self.localization(x)</div><div class="line">  xs = xs.view(<span class="number">-1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>)</div><div class="line">  theta = self.fc_loc(xs)</div><div class="line">  theta = theta.view(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line">  grid = F.affine_grid(theta, x.size())</div><div class="line">  x = F.grid_sample(x, grid)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>HEXOä¸»é¢˜cactusä¿®æ”¹</title>
    <link href="http://yoursite.com/2019/05/19/hexo-theme-cactus/"/>
    <id>http://yoursite.com/2019/05/19/hexo-theme-cactus/</id>
    <published>2019-05-19T03:05:14.000Z</published>
    <updated>2019-05-19T08:55:36.409Z</updated>
    
    <content type="html"><![CDATA[<p>cacutsçš„ä¸»é¢˜å¾ˆç®€æ´ï¼Œç”¨å¾—è›®ä¹…ï¼Œçœ‹åˆ°åŸåº“æœ‰æ›´æ–°ï¼Œæ‰€ä»¥forkäº†æ–°çš„ç‰ˆæœ¬å¹¶åœ¨ä¸Šé¢åšä¸€äº›ä¿®æ”¹ï¼Œé¡ºä¾¿è®°å½•ä¸€ä¸‹è¿‡ç¨‹ã€‚</p><h3 id="ä¸»é¢˜é•œåƒ"><a href="#ä¸»é¢˜é•œåƒ" class="headerlink" title="ä¸»é¢˜é•œåƒ"></a>ä¸»é¢˜é•œåƒ</h3><p>é¦–å…ˆæ ¹æ®<a href="https://help.github.com/en/articles/duplicating-a-repository" target="_blank" rel="external">Mirrow a repository</a>é•œåƒä¸€ä¸ªåº“ã€‚åœ¨pushçš„æ—¶å€™è¿˜é‡åˆ°äº†403é—®é¢˜ï¼š</p><blockquote><p>remote: Permission to colorjam/hexo-theme-cactus-mirrored.git denied to xxx</p></blockquote><p>é€šè¿‡åˆ é™¤<strong>Keychain Access</strong>ä¸­å­˜å‚¨çš„github.comçš„Internet passwordå¾—åˆ°è§£å†³ã€‚ç„¶åæŠŠè‡ªå·±çš„åº“å†Cloneè¿›<code>themes</code>ä¸­</p><h3 id="æ ·å¼ç¼–è¾‘"><a href="#æ ·å¼ç¼–è¾‘" class="headerlink" title="æ ·å¼ç¼–è¾‘"></a>æ ·å¼ç¼–è¾‘</h3><ul><li><p>ä¸»é¢˜é¢œè‰²</p><p>åœ¨<code>source/css/_colors</code>ä¸‹æ–°å»ºäº†ä¸€ä¸ª<code>pink.styl</code>ï¼ŒåŒæ—¶ä¿®æ”¹<code>_config.yml</code>ä¸­çš„<code>colorscheme:pink</code>ã€‚</p></li><li><p>logoè®¾ç½®</p><p>æŠŠ<code>source/images/</code>ä¸‹çš„<code>favicon.ico</code>å’Œ<code>logo.png</code>æ¢æˆè‡ªå·±å–œæ¬¢çš„å›¾ç‰‡ã€‚ä¿®æ”¹<code>source/css/_partial/header.styl</code>ä¸­çš„<code>#logo</code> çš„<code>background-size: contain</code></p></li><li><p>ç»†èŠ‚è°ƒæ•´</p><p>åˆ é™¤<code>header.styl</code>ä¸­htmlçš„<code>border-top</code></p><p>é“¾æ¥æ ·å¼ï¼š</p></li></ul><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">a</span></div><div class="line">  color: $color-text</div><div class="line">  <span class="selector-tag">text-decoration</span>: <span class="selector-tag">none</span></div><div class="line"></div><div class="line">  &amp;<span class="selector-pseudo">:hover</span></div><div class="line">  background-image: linear-gradient(transparent, transparent 4px, $color-link 4px, $color-link)</div><div class="line">  <span class="selector-tag">background-position</span>: <span class="selector-tag">bottom</span></div><div class="line">  <span class="selector-tag">background-size</span>: 100% 6<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background-repeat</span>: <span class="selector-tag">repeat-x</span></div></pre></td></tr></table></figure><p>â€‹    è¡Œå†…ä»£ç æ ·å¼ï¼š</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">code</span></div><div class="line">  <span class="selector-tag">padding</span>: 0 5<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background</span>: <span class="selector-id">#f6f8fa</span></div><div class="line">  <span class="selector-tag">border-radius</span>: 2<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">-webkit-border-radius</span>: 2<span class="selector-tag">px</span></div></pre></td></tr></table></figure><h3 id="ä¼šåŠ¨çš„ç²’å­"><a href="#ä¼šåŠ¨çš„ç²’å­" class="headerlink" title="ä¼šåŠ¨çš„ç²’å­"></a>ä¼šåŠ¨çš„ç²’å­</h3><p>åœ¨èƒŒæ™¯åŠ ä¸Š<a href="https://github.com/VincentGarreau/particles.js/" target="_blank" rel="external">ä¼šåŠ¨çš„ç²’å­</a>ï¼Œåœ¨<code>source/lib</code>é‡Œåˆ›å»ºä¸€ä¸ªparticlesæ–‡ä»¶å¤¹ï¼ŒæŠŠ<code>particles.min.js</code>æ”¾è¿›å»ã€‚</p><p>åœ¨<code>layout.ejs</code>ä¸­åŠ å…¥</p><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"particles-js"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure><p>åœ¨<code>scripts.ejs</code>ä¸­æ·»åŠ è„šæœ¬ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;!-- particles --&gt;</div><div class="line">&lt;%- js(&apos;lib/particles/particles.min&apos;) %&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">particlesJS(&apos;particles-js&apos;, &#123;</div><div class="line">        ...</div><div class="line">        &#125;</div><div class="line">      )</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure><p>åœ¨<code>style.css</code>ä¸­æ·»åŠ æ ·å¼ï¼š</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="selector-id">#particles-js</span> &#123;</div><div class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</div><div class="line">  <span class="attribute">position</span>: absolute;</div><div class="line">  <span class="attribute">margin-left</span>: -<span class="number">28%</span>;</div><div class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cacutsçš„ä¸»é¢˜å¾ˆç®€æ´ï¼Œç”¨å¾—è›®ä¹…ï¼Œçœ‹åˆ°åŸåº“æœ‰æ›´æ–°ï¼Œæ‰€ä»¥forkäº†æ–°çš„ç‰ˆæœ¬å¹¶åœ¨ä¸Šé¢åšä¸€äº›ä¿®æ”¹ï¼Œé¡ºä¾¿è®°å½•ä¸€ä¸‹è¿‡ç¨‹ã€‚&lt;/p&gt;
&lt;h3 id=&quot;ä¸»é¢˜é•œåƒ&quot;&gt;&lt;a href=&quot;#ä¸»é¢˜é•œåƒ&quot; class=&quot;headerlink&quot; title=&quot;ä¸»é¢˜é•œåƒ&quot;&gt;&lt;/a&gt;ä¸»é¢˜é•œåƒ&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflowå†…å­˜æ³„æ¼</title>
    <link href="http://yoursite.com/2019/05/18/tf-memory-leak/"/>
    <id>http://yoursite.com/2019/05/18/tf-memory-leak/</id>
    <published>2019-05-18T05:13:31.000Z</published>
    <updated>2019-05-19T06:54:57.626Z</updated>
    
    <content type="html"><![CDATA[<p>ç”¨tfç»å¸¸ä¼šå‡ºç°OOMçš„ç°è±¡ï¼ŒæŸ¥äº†ä¸€ä¸‹å‘ç°äº†ä¸€ç¯‡æ–‡ç« <a href="https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/" target="_blank" rel="external">How To Debug A Memory Leak In Tensorflow</a></p><p>ç”±äºtfå­˜åœ¨å†…å­˜æ³„æ¼é—®é¢˜ï¼Œè®¸å¤šäººä¼šç”¨ <a href="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" target="_blank" rel="external">tcmalloc</a> æ¥æ›¿ä»£ malloc()ã€‚</p><p>ä½†æ˜¯è¿è¡Œç¨‹åºçš„æ—¶å€™ä¼šæŠ¥é”™ï¼š</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ERROR: ld.so: object <span class="string">'/usr/lib/libtcmalloc.so.4'</span> from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.</div></pre></td></tr></table></figure><p>è¸©äº†ä¸€ç³»åˆ—å‘ä»¥åå‘ç°ï¼Œå°†<code>/usr/lib/libtcmalloc.so.4</code>æ”¹ä¸º<code>/usr/local/lib/libtcmalloc.so.4</code>å³å¯ã€‚</p><p><strong>å‚è€ƒé“¾æ¥ï¼š</strong></p><ul><li><a href="https://www.cnblogs.com/Lelouch/p/3365672.html" target="_blank" rel="external">https://www.cnblogs.com/Lelouch/p/3365672.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ç”¨tfç»å¸¸ä¼šå‡ºç°OOMçš„ç°è±¡ï¼ŒæŸ¥äº†ä¸€ä¸‹å‘ç°äº†ä¸€ç¯‡æ–‡ç« &lt;a href=&quot;https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;How
      
    
    </summary>
    
    
      <category term="ç»´ä¿®æŒ‡å—" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>pair-wise-loss</title>
    <link href="http://yoursite.com/2019/05/17/pair-wise-loss/"/>
    <id>http://yoursite.com/2019/05/17/pair-wise-loss/</id>
    <published>2019-05-17T05:44:19.000Z</published>
    <updated>2019-05-17T08:08:40.815Z</updated>
    
    <content type="html"><![CDATA[<p>ç”¨Tensorflowå¤ç°è®ºæ–‡ä¸­çš„pair wise loss<br>$$<br>\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathcal{R}} \sum_{j \in \mathcal{R}}\left(a_{ij}^{s}-a_{ij}^{t}\right){2}<br>$$<br>å…¶ä¸­<br>$$<br>a_{i j}=\mathbf{f}_{i}^{\top} \mathbf{f}_{j} /\left(\left|\mathbf{f}_{i}\right|_{2}\left|\mathbf{f}_{j}\right|_{2}\right)<br>$$<br>$f_i$å’Œ$f_j$åˆ†åˆ«ä»£è¡¨ith / jthåƒç´ ç‚¹çš„cç»´ç‰¹å¾ã€‚å‚è€ƒäº†ä½™å¼¦ç›¸ä¼¼æ€§çš„è®¡ç®—æ–¹æ³•ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(x)</span>:</span></div><div class="line">    x = tf.reshape(x, [x.shape[<span class="number">0</span>], <span class="number">-1</span>, x.shape[<span class="number">-1</span>]])</div><div class="line">    norm = tf.nn.l2_normalize(x, <span class="number">2</span>)</div><div class="line">    a = tf.matmul(norm, norm, adjoint_b = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> a</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_loss</span><span class="params">(x, y)</span>:</span></div><div class="line">    _, h, w, _  = x.shape</div><div class="line">    pa = tf.reduce_sum(tf.pow((similarity(x) - similarity(y)), <span class="number">2</span>)) / tf.pow(tf.cast(h*w, tf.float32),<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> pa</div></pre></td></tr></table></figure><p>å‚è€ƒé“¾æ¥ï¼š</p><ul><li><a href="https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1" target="_blank" rel="external">https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ç”¨Tensorflowå¤ç°è®ºæ–‡ä¸­çš„pair wise loss&lt;br&gt;$$&lt;br&gt;\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathc
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>æ¯å‘¨è®ºæ–‡ Vol.02</title>
    <link href="http://yoursite.com/2019/05/13/weekly-paper-02/"/>
    <id>http://yoursite.com/2019/05/13/weekly-paper-02/</id>
    <published>2019-05-13T02:50:43.000Z</published>
    <updated>2019-05-19T04:29:39.092Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1ï¸âƒ£-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION"><a href="#1ï¸âƒ£-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION" class="headerlink" title="1ï¸âƒ£ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION"></a>1ï¸âƒ£ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION</h3><p>åœ¨ã€ŠRevisiting the Importance of Individual Units in CNNs via Ablationã€‹çš„åŸºç¡€ä¸Šçœ‹äº†è¿™ç¯‡è®ºæ–‡ã€‚</p><p>æœ¬æ–‡æ¢ç©¶çš„æ˜¯æ¿€æ´»å€¼çš„å•æ–¹å‘ä¾èµ–å¯¹ç½‘ç»œæ³›åŒ–æ€§èƒ½çš„å½±å“ï¼Œé€šè¿‡å¯¹unitsè¿›è¡ŒæŠ‘åˆ¶/åŠ å™ªå£°ï¼Œè¡¨ç¤ºç½‘ç»œå¯¹å•åå‘çš„ä¾èµ–èƒ½è¾ƒå¥½çš„é¢„æµ‹å…¶æ³›åŒ–æ€§èƒ½ã€‚æ–‡ç« è®²äº†ä¸€ä¸ªæ•…äº‹ï¼Œä¸€ä¸ªç½‘ç»œåªé€šè¿‡è®°å¿†æ¯å¼ è¾“å…¥å’Œå…¶å¯¹åº”çš„è¾“å‡ºï¼Œæ³›åŒ–æ€§å·®(memorizing network)ï¼Œå¦ä¸€ä¸ªç½‘ç»œèƒ½å¤Ÿæ‰¾åˆ°æ•°æ®ä¸­çš„ç»“æ„æ€§ï¼Œæ³›åŒ–æ€§ä½³(structure-finding network)ã€‚memorizing networkæ‰¾åˆ°çš„æœ€å°æè¿°é•¿åº¦åº”è¯¥å¤§äºstructure-finding networkã€‚å› æ­¤ï¼Œmemorizing networkä¼šä½¿ç”¨æ›´å¤šçš„å•åå‘ï¼Œé‚£ä¹ˆï¼Œå¦‚æœéšæœºæ‰°ä¹±å•ä¸€æ–¹å‘ï¼Œå¯¹memorizing networkçš„å½±å“åº”è¯¥å¤§äºstructure-finding networkã€‚</p><p>é€šè¿‡å¯¹dropoutå’ŒBNå®éªŒï¼ˆä¸¤ä¸ªæ–¹æ³•éƒ½å¢å¼ºäº†ç½‘ç»œçš„æ³›åŒ–æ€§ï¼‰ï¼Œè¡¨æ˜å°½ç®¡dropoutèƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…è®°å¿†éšæœºæ ‡ç­¾ï¼Œä½†ä¸èƒ½é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡åº¦å•æ–¹å‘ä¾èµ–ã€‚åŠ äº†BNçš„ç½‘ç»œè¿›è¡Œç¥ç»å…ƒæŠ‘åˆ¶æ—¶ï¼Œè®­ç»ƒç²¾åº¦ä¼šé™å¾—æ¯”è¾ƒæ…¢ï¼Œè¯´æ˜BNä¹Ÿä¸é¼“åŠ±å•æ–¹å‘ä¾èµ–ã€‚</p><p>æ¥ç€æ–‡ç« éªŒè¯äº†class selectivityä¸ç¥ç»å…ƒé‡è¦æ€§çš„å…³ç³»ã€‚æå‡ºäº†ä¸¤ä¸ªé—®é¢˜ï¼š</p><ol><li><p>BNä¸é¼“åŠ±å•æ–¹å‘ä¾èµ–ï¼Œé‚£ä¹ˆæ˜¯å¦ä¼šå½±å“å•åå‘çš„ç±»åˆ«ä¿¡æ¯åˆ†å¸ƒï¼Ÿ</p><p>æ–‡ç« ä½¿ç”¨class selectivityæ¥è¡¡é‡ç±»åˆ«ä¿¡æ¯åˆ†å¸ƒï¼Œhigh class selectivityè¯´æ˜å…³æ³¨çš„æ˜¯å•ä¸€ç±»åˆ«ï¼Œlow class selectivityè¯´æ˜å…³æ³¨çš„æ˜¯å¤šä¸ªç±»åˆ«ã€‚æ²¡æœ‰BNçš„ç½‘ç»œåè€Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„class selectivityã€‚è¡¨æ˜BNå±‚é¼“åŠ±feature mapå»å­¦ä¹ å¤šç§ç±»åˆ«çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å…³æ³¨å•ä¸€ç±»åˆ«ã€‚</p></li><li><p>æ˜¯å¦èƒ½å¤Ÿåˆ©ç”¨unitçš„class selectivityï¼Œåˆ¤æ–­unitçš„é‡è¦æ€§ï¼Ÿ</p><p>æ–‡ç« å‘ç°class selectivityå’Œç½‘ç»œæµ…å±‚çš„feature mapè´Ÿç›¸å…³ï¼Œä¸ç½‘ç»œæ·±å±‚åˆ™æ— å…³ã€‚ä½œè€…åˆ©ç”¨äº’ä¿¡æ¯ä¹Ÿåšäº†ç›¸åŒçš„å®éªŒã€‚å¾—åˆ°ä¸€è‡´çš„ç»“æœã€‚ä»¥æ­¤è¯´æ˜class selectiviyå¹¶ä¸èƒ½ä»£è¡¨unitçš„é‡è¦æ€§ã€‚</p></li></ol><blockquote><p>ğŸ§ æœ¬æ–‡çš„ç»“è®ºæ˜¯ç´§å‡‘ç½‘ç»œå¯¹å•æ–¹å‘çš„ä¾èµ–æ€§è¾ƒå°‘ï¼Œé‚£ä¹ˆå¦‚ä½•æ‰¾åˆ°ä¸€ä¸ªè¡¡é‡unitæ–¹å‘æ€§çš„å‡½æ•°ï¼Œæ¥è¿›è¡Œç½‘ç»œå‹ç¼©å‘¢ï¼Ÿ</p></blockquote><h3 id="2ï¸âƒ£-MaskConnect-Connectivity-Learning-by-Gradient-Descent"><a href="#2ï¸âƒ£-MaskConnect-Connectivity-Learning-by-Gradient-Descent" class="headerlink" title="2ï¸âƒ£  MaskConnect: Connectivity Learning by Gradient Descent"></a>2ï¸âƒ£  MaskConnect: Connectivity Learning by Gradient Descent</h3><p>ç”¨æ¢¯åº¦ä¸‹é™è‡ªåŠ¨å­¦ä¹ è¿æ¥ã€‚å’Œç½‘ç»œæƒé‡ä¸€èµ·å­¦ä¹ <em>connectivity masks</em>ï¼Œæ¥å†³å®šç½‘ç»œblockä¹‹é—´çš„è¿æ¥ã€‚</p><p>ç¬¬$j$ä¸ªblockçš„è¾“å…¥å¯ä»¥ç”±å‰é¢æ‰€æœ‰è¾“å‡ºç›¸åŠ è€Œæˆï¼Œç”¨äºŒå€¼çš„$m$è¡¨ç¤ºæ˜¯å¦è¿æ¥ï¼š<br>$$<br>\mathbf{x}_{j}=\sum_{k=1}^{j-1} m_{j, k} \cdot \mathbf{y}_{k}<br>$$<br>æœ¬æ–‡è¡¨ç¤ºæ¯ä¸ªblockåªå’Œ$K$ä¸ªè¿æ¥æ•ˆæœæœ€å¥½ï¼š<br>$$<br>m_{j, k} \in\{0,1\} \forall j, k, \quad and \quad \sum_{k=1}^{j-1} m_{j, k}=K \forall j<br>$$<br>ğŸ”º è®­ç»ƒè¿‡ç¨‹ï¼š</p><p><strong>Forward Propagation</strong>. é™åˆ¶å®å€¼çš„maskçš„å’Œä¸º1ï¼Œå³$\sum_{k=1}^{j-1} \tilde{m}_{j, k}=1$ï¼Œä»£è¡¨ä¸€ä¸ªå¤šé¡¹å¼åˆ†å¸ƒï¼Œä»ä¸­é‡‡æ ·Kä¸ªæ ·æœ¬$a_{1}, a_{2}, \ldots, a_{K} \in\{1, \ldots,(j-1)\}$ï¼Œæ¿€æ´»å¯¹åº”çš„mask $m_{j, a_{k}} \leftarrow 1$ã€‚</p><p><strong>Backward Propagation.</strong> ç¬¬$k$ä¸ªblockè¾“å‡ºçš„æ¢¯åº¦é€šè¿‡äºŒå€¼$m_{j,k}$å’Œ$x_j$çš„æ¢¯åº¦è·å¾—ã€‚</p><p><strong>Mask Update.</strong> é€šè¿‡clipå®å€¼maskï¼Œé™åˆ¶å®ƒä»¬åœ¨[0,1]çš„èŒƒå›´ã€‚</p><p>ğŸ”º è®­ç»ƒç»“æŸï¼š</p><p>ï¼ˆ1ï¼‰ä¸ºæ¯ä¸ª$m_j$æ¿€æ´»$\tilde{m}_{j}$ä¸­top-Kçš„è¿æ¥ï¼Œ</p><p>ï¼ˆ2ï¼‰å›ºå®šäºŒå€¼maskï¼Œftç½‘ç»œæƒé‡$\theta$</p><blockquote><p>ğŸ§ æœ¬æ–‡ç®—æ˜¯NASçš„åˆ†æ”¯å§ï¼Œæœç´¢çš„åªæ˜¯ç½‘ç»œå—ä¹‹é—´çš„è¿æ¥ã€‚æ¯ä¸ªblockæœ‰ä¸€ä¸ªå¤šé¡¹å¼åˆ†å¸ƒï¼Œä»£è¡¨å®ƒä¸ä¹‹å‰æ‰€æœ‰blockè¿æ¥çš„æ¦‚ç‡ï¼Œä»è¿™ä¸ªåˆ†å¸ƒä¸­é‡‡æ ·æ¿€æ´»çš„è¿æ¥ã€‚ç»“åˆæˆ‘æƒ³åšçš„ä¸œè¥¿ï¼Œ<strong>æ ¹æ®ä¸åŒçš„è¾“å…¥å›¾ç‰‡é€‰æ‹©ä¸åŒçš„block</strong>ï¼Œæ¯ä¸ªblockçš„è¾“å‡ºä¸ºä¸€ä¸ªnum_classesçš„åˆ†å¸ƒï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨æŸä¸ªç±»æ¿€æ´»è¿™ä¸ªblockæ¦‚ç‡ï¼Œåˆ©ç”¨è¿™ä¸ªæ¦‚ç‡è¿›è¡ŒäºŒé¡¹å¼åˆ†å¸ƒçš„é‡‡æ ·ã€‚</p></blockquote><h3 id="3ï¸âƒ£-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION"><a href="#3ï¸âƒ£-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION" class="headerlink" title="3ï¸âƒ£ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION"></a>3ï¸âƒ£ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION</h3><p>æœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªå‹ç¼©æ–¹æ³•ï¼š1.<em> quantized distillation</em>ï¼šåˆ©ç”¨è’¸é¦è®­ç»ƒæƒé‡æ˜¯é‡åŒ–çš„å°ç½‘ç»œã€‚ 2. <em>differentiable quantization</em>ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–é‡åŒ–ç‚¹çš„ä½ç½®ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1ï¸âƒ£-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION&quot;&gt;&lt;a href=&quot;#1ï¸âƒ£-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATI
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="æ¯å‘¨è®ºæ–‡" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Revisiting the Importance of Individual Units in CNNs via Ablation</title>
    <link href="http://yoursite.com/2019/05/12/Revisiting%20the%20Importance%20of%20Individual%20Units%20in%20CNNs%20via%20Ablation/"/>
    <id>http://yoursite.com/2019/05/12/Revisiting the Importance of Individual Units in CNNs via Ablation/</id>
    <published>2019-05-12T05:51:59.000Z</published>
    <updated>2019-05-19T09:48:53.400Z</updated>
    
    <content type="html"><![CDATA[<p>ä¹‹å‰çš„ä¸€äº›å·¥ä½œé€šè¿‡å¯è§†åŒ–æ¯ä¸ªç¥ç»å…ƒçš„æ–¹å¼æ¥ç†è§£ç¥ç»ç½‘ç»œï¼Œå®ƒä»¬é€‰æ‹©çš„æ˜¯<em>high selectivity</em>çš„ç¥ç»å…ƒï¼Œå‘ç°ç½‘ç»œæµ…å±‚è¯†åˆ«çš„æ˜¯å…·ä½“çš„å›¾æ¡ˆï¼ˆe.g çº¹ç†ã€å›¾åƒï¼‰ï¼Œç½‘ç»œæ·±å±‚è¯†åˆ«çš„æ˜¯è¯­ä¹‰ä¿¡æ¯ï¼ˆe.g ç‹—å¤´ã€è½¦è½®ï¼‰ï¼Œè®ºæ–‡[11]ä¼¼ä¹æ‰“è„¸äº†è¿™ç§æ–¹å¼ï¼Œè¡¨æ˜å¯¹äºä»£è¡¨æ•´ä½“åˆ†ç±»ç²¾åº¦ï¼Œ<em>class selectivity</em>å±æ€§ä¸èƒ½ç”¨æ¥é¢„æµ‹ç¥ç»å…ƒçš„é‡è¦æ€§ã€‚</p><p>æœ¬æ–‡è¡¨æ˜è¿™ä¸¤ç§æ–¹å¼éƒ½æ˜¯åˆç†çš„ã€‚ç”¨<em>class selectivity</em>æˆ–å…¶ä»–å±æ€§æ¥é¢„æµ‹ç¥ç»å…ƒçš„é‡è¦æ€§ä»æ•´ä½“åˆ†ç±»ç²¾åº¦ï¼ˆç½‘ç»œçš„æ³›åŒ–æ€§ï¼‰ä¸Šæ¥çœ‹ç¡®å®ä¸å¥½ï¼Œä½†æ˜¯èƒ½ä½œä¸ºå…·ä½“ç±»åˆ«çš„åˆ¤æ–­ä¾æ®ã€‚</p><p><strong>æŠ‘åˆ¶ç¥ç»å…ƒçš„æ–¹å¼</strong>ï¼šå°†å…¶weightå’Œbiasè®¾ç½®æˆ0ã€‚</p><p><strong>ä¸¤ç§ç²¾åº¦ä¸‹é™ç±»å‹ï¼š</strong>overall accuracy drop &amp; max class accuracy drop</p><p><strong>åˆ¤æ–­ç¥ç»å…ƒé‡è¦æ€§çš„å±æ€§ï¼š</strong></p><ul><li><p>L1 Normï¼š<br>$$<br>\operatorname{norm}_{1}(i)=\left|w_{i}\right|_{1}=\sum_{j}\left|\left(w_{i}\right)_{j}\right|<br>$$</p></li><li><p>Class Correlationï¼š<br>$$<br>\operatorname{corr}(i, k)=\frac{E\left[\left(x_{i}-\overline{x}_{i}\right)\left(p_{k}-\overline{p}_{k}\right)\right]}{\sigma_{x_{i}} \sigma_{p_{k}}}<br>$$</p></li><li><p>Class Selectivityï¼š</p></li></ul><p>$$<br>\operatorname{select}(i, k)=\frac{\overline{x}_{i}^{k}-\overline{x}_{i}^{-k}}{\overline{x}_{i}^{k}+\overline{x}_{i}^{-k}}<br>$$</p><p>â€‹        å…¶ä¸­$\overline{x}_{i}^{k}$è¡¨ç¤ºç¥ç»å…ƒ$i$å±äºkthç±»åˆ«çš„å¹³å‡æ¿€æ´»å€¼ï¼Œ$\overline{x}_{i}^{-k}$è¡¨ç¤ºç¥ç»å…ƒ$i$å±äºnon-kthç±»åˆ«çš„å¹³å‡æ¿€æ´»å€¼çš„å‡å€¼ã€‚è¿™ä¸ªå€¼çš„èŒƒå›´æ˜¯[0, 1]ï¼Œ0è¡¨ç¤ºä¸€ä¸ªç¥ç»å…ƒçš„å¹³å‡æ¿€æ´»å€¼ä¸å…¶ä»–ç±»åˆ«éƒ½ç›¸åŒï¼Œ1è¡¨ç¤ºä¸€ä¸ªç¥ç»å…ƒåªå¯¹æŸä¸ªç±»åˆ«çš„è¾“å…¥æœ‰ååº”ã€‚</p><ul><li>Concept Alighmentï¼šIoU between unit activation and gt concepts</li><li>Unit Visualization</li></ul><p>ğŸ”º <strong>å®éªŒä¸€ï¼š</strong>éªŒè¯æŠ‘åˆ¶å•ä¸ªç¥ç»å…ƒ/ä¸€ç»„ç¥ç»å…ƒå¯¹ä¸¤ç§ç²¾åº¦ä¸‹é™ç±»å‹çš„å½±å“ã€‚</p><ul><li>å®éªŒæ–¹å¼ï¼š<ul><li>æŠ‘åˆ¶æŸä¸ªç¥ç»å…ƒï¼Œæ¨ªè½´è¡¨ç¤ºCLassï¼Œçºµè½´è¡¨ç¤ºClass Accuracy Dropã€‚</li><li>é’ˆå¯¹ç‰¹å®šçš„ç½‘ç»œå±‚ï¼Œæ ¹æ®Mac Class Accuracy Dropè¿›è¡Œæ’åºï¼Œç»˜åˆ¶ä¸‰æ¡æ›²çº¿ï¼ˆOveral Accuracy Drop / Max Class Accuracy Drop / Min Class Accuracy Dropï¼‰ã€‚</li><li>åˆ©ç”¨greedyçš„æ–¹å¼è¿­ä»£åœ°ç§»é™¤é™ä½ç‰¹å®šç±»å‡†ç¡®ç‡æœ€å¤šçš„ç¥ç»å…ƒã€‚ç»˜åˆ¶äº†ç‰¹å®šç±»åˆ«ç²¾åº¦ä¸‹é™çš„æ›²çº¿ï¼Œå’Œæ‰€æœ‰ç±»åˆ«å¹³å‡ç²¾åº¦ä¸‹é™çš„æ›²çº¿ã€‚åŒæ—¶ç”¨randomä½œä¸ºbaselineã€‚</li></ul></li><li>ç»“è®ºï¼š<ul><li>æŠ‘åˆ¶å•ä¸ªç¥ç»å…ƒå¯¹æŸäº›ç±»åˆ«çš„åˆ†ç±»ç²¾åº¦å½±å“å¾ˆå¤§ï¼Œä½†å¯¹æ€»ä½“çš„ç²¾åº¦å½±å“ä¸å¤§ï¼Œå¹¶ä¸”èƒ½é€šè¿‡å¯è§†åŒ–çš„å½¢å¼çœ‹å‡ºè¿™äº›æŠ‘åˆ¶çš„ç¥ç»å…ƒç¡®å®å±•ç°å‡ºäº†ç›¸åº”ç±»åˆ«çš„ç‰¹ç‚¹ã€‚</li><li>greedyåœ°æŠ‘åˆ¶ä¸€ç»„ç¥ç»å…ƒï¼Œèƒ½ä½¿è¿™ä¸ªç±»åˆ«çš„ç²¾åº¦å¤§å¤§é™ä½ï¼Œä½†æ˜¯randomçš„æ–¹å¼å½±å“ä¸å¤§ã€‚</li></ul></li></ul><p>ğŸ”º <strong>å®éªŒäºŒï¼š</strong>éªŒè¯ä¸åŒå±æ€§ä¸ç²¾åº¦ä¸‹é™ä¹‹é—´çš„å…³ç³»ã€‚</p><ul><li>å®éªŒæ–¹å¼ï¼š<ul><li>ç”¨æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°å’ŒPå€¼ç»Ÿè®¡äº†ä¸åŒå±æ€§å€¼ä¸ç²¾åº¦ä¸‹é™ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li><li>ç”¨ä¸åŒå±æ€§åˆ¤æ–­å‡ºçš„æœ€é‡è¦çš„é‚£ä¸ªç¥ç»å…ƒæ¥é¢„æµ‹åˆ†ç±»</li></ul></li><li>ç»“è®ºï¼š<ul><li>å¯¹äºæ•´ä½“ç²¾åº¦ä¸‹é™ï¼šclass selectivityï¼Œclass correlationå’Œconcept alighmentè¡¨ç¤ºå‡ºæ­£ç›¸å…³ï¼ŒL1æ˜¯è´Ÿç›¸å…³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“æŠ‘åˆ¶class selectivityå€¼å¾ˆå¤§çš„ç¥ç»å…ƒï¼Œå¯¹æ•´ä½“ç½‘ç»œçš„ç²¾åº¦ä¸‹é™å½±å“è¾ƒå°ï¼Œä¸è®ºæ–‡[11]ä¸­ç»“è®ºä¸€è‡´ã€‚</li><li>å¯¹äºæœ€å¤§ç±»åˆ«ç²¾åº¦ä¸‹é™ï¼šæ¯ä¸ªå±æ€§åŸºæœ¬éƒ½è¡¨ç°å‡ºè´Ÿç›¸å…³ã€‚è¯´æ˜æŠ‘åˆ¶è¿™äº›å±æ€§å€¼å¤§çš„ç¥ç»å…ƒï¼Œå¯¹ç‰¹å®šç±»åˆ«ç²¾åº¦å½±å“å¾ˆå¤§ã€‚</li><li>Concept Alignmentä¼¼ä¹æœ€èƒ½ä»£è¡¨ç¥ç»å…ƒçš„é‡è¦æ€§</li></ul></li></ul><p>ğŸ”º <strong>å®éªŒä¸‰ï¼š</strong>éªŒè¯é€‰æ‹©çš„ç¥ç»å…ƒä¸å…¶æ–¹å‘ç›¸å…³ï¼Œè€Œä¸æ˜¯éšæœºæ–¹å‘ã€‚</p><p>ğŸ”º <strong>å®éªŒå››ï¼š</strong>éªŒè¯BNå’ŒDropoutçš„å½±å“ã€‚</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ä¹‹å‰çš„ä¸€äº›å·¥ä½œé€šè¿‡å¯è§†åŒ–æ¯ä¸ªç¥ç»å…ƒçš„æ–¹å¼æ¥ç†è§£ç¥ç»ç½‘ç»œï¼Œå®ƒä»¬é€‰æ‹©çš„æ˜¯&lt;em&gt;high selectivity&lt;/em&gt;çš„ç¥ç»å…ƒï¼Œå‘ç°ç½‘ç»œæµ…å±‚è¯†åˆ«çš„æ˜¯å…·ä½“çš„å›¾æ¡ˆï¼ˆe.g çº¹ç†ã€å›¾åƒï¼‰ï¼Œç½‘ç»œæ·±å±‚è¯†åˆ«çš„æ˜¯è¯­ä¹‰ä¿¡æ¯ï¼ˆe.g ç‹—å¤´ã€è½¦è½®ï¼‰ï¼Œè®ºæ–‡[11]ä¼¼ä¹æ‰“è„¸äº†è¿™ç§æ–¹å¼ï¼Œè¡¨æ˜å¯¹äºä»£è¡¨
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
</feed>
