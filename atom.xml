<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-27T11:48:18.266Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>weekly-paper-07</title>
    <link href="http://yoursite.com/2019/06/25/weekly-paper-07/"/>
    <id>http://yoursite.com/2019/06/25/weekly-paper-07/</id>
    <published>2019-06-25T00:55:36.000Z</published>
    <updated>2019-06-27T11:48:18.266Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-Universally-Slimmable-Networks-and-Improved-Training-Techniques"><a href="#1️⃣-Universally-Slimmable-Networks-and-Improved-Training-Techniques" class="headerlink" title="1️⃣  Universally Slimmable Networks and Improved Training Techniques"></a>1️⃣  Universally Slimmable Networks and Improved Training Techniques</h3><p>This work extending slimmable networks to execute at arbitrary width, the contributions can list as</p><ol><li><p>Post-Statistics of Batch Normalization</p><p>Instead of using moving averages to test model, calculating BN statistics of all widths after training (exact averages).</p><ul><li>moving averages：</li></ul><p>$$<br>\begin{aligned} \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>$$</p><ul><li>exact averages：</li></ul><p>$$<br>\begin{aligned} m &amp;=(t-1) / t \\ \mu_{t} &amp;=m \mu_{t-1}+(1-m) E_{B}\left[x_{B}\right] \\ \sigma_{t}^{2} &amp;=m \sigma_{t-1}^{2}+(1-m) \operatorname{Var}_{B}\left[x_{B}\right] \end{aligned}<br>$$</p></li></ol><ol><li><p>Sandwich Rule</p><p>This paper assumes that the performances at all widths are bounded by performance of the model at smallest width $0.25 \times$ and largest width $1.0 \times$ , so during training just sample n widths between [min-width, max-width]</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-Universally-Slimmable-Networks-and-Improved-Training-Techniques&quot;&gt;&lt;a href=&quot;#1️⃣-Universally-Slimmable-Networks-and-Improved-Train
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的PixelShuffle(depth_to_space)</title>
    <link href="http://yoursite.com/2019/06/23/tf-pixshuffle/"/>
    <id>http://yoursite.com/2019/06/23/tf-pixshuffle/</id>
    <published>2019-06-23T08:31:38.000Z</published>
    <updated>2019-06-28T02:10:01.249Z</updated>
    
    <content type="html"><![CDATA[<p>在尝试对PixelShuffle前的卷积层做剪枝时遇到了一些问题，对PixelShuffle的具体操作有了进一步的了解。</p><p>PixelShuffle通过将通道重排对图像进行上采样，tf中的函数是<code>tf.depth_to_sapce</code>，第一个参数是<code>Tensor</code>，第二个参数是需要放大倍数。当输入<code>X</code>的大小为<code>[1 2 2 16]</code>，放大倍数为2，H和W各乘2，C除以4，PixelShuffle后的结果就为<code>[1 4 4 4]</code>。</p><p>🔺坑点1：想当然的以为参与重排的通道是<code>[:, :, :, i:i+4]</code></p><p>测试代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x = tf.range(<span class="number">64</span>)</div><div class="line">x = tf.reshape(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">16</span>])</div><div class="line">y = tf.depth_to_space(x, <span class="number">2</span>) <span class="comment"># [1, 4, 4, 4]</span></div></pre></td></tr></table></figure><p>下面看一下具体x和y每个通道的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>, <span class="number">16</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">48</span>]]]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x[:, :, :, <span class="number">1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">1</span>, <span class="number">17</span>],</div><div class="line">[<span class="number">33</span>, <span class="number">49</span>]]]</div><div class="line"> </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">4</span>, <span class="number">16</span>, <span class="number">20</span>],</div><div class="line">[ <span class="number">8</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">28</span>],</div><div class="line">[<span class="number">32</span>, <span class="number">36</span>, <span class="number">48</span>, <span class="number">52</span>],</div><div class="line">[<span class="number">40</span>, <span class="number">44</span>, <span class="number">56</span>, <span class="number">60</span>]]]</div></pre></td></tr></table></figure><p>可以看出<code>y</code>的第0维通道包含的是<code>x</code>通道数为0、4、8、12的特征图。可视化一下就是这样的效果：</p><p><img src="https://i.loli.net/2019/06/27/5d14b6a1d9bbf35069.png" alt=""></p><p>将Y的一个通道单独取出，看一下每个点属于原来X的哪个坐标：</p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190628095308642.png" alt="image-20190628095308642"></p><p>可以看到Y的一个通道实际上分成4个象限，在空间上由<code>(0,0)(0,1)(1,0)(1,1)</code>构成。在通道上每4个间隔提取对应通道。这里的间隔对应的是Pixshuffle后的通道数。</p><p>假设原始通道数为<code>c_out</code>，PS后的通道数为<code>ps_out</code>，实际上<code>y</code>的第<code>i</code>通道对应的是<code>x</code>的<code>[i, i+ps_out, i+2*ps_out, i+3*ps_out]</code></p><p>而我原先理解的通道排列方式是❌</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>y[:, :, :, <span class="number">0</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">5</span>],</div><div class="line">[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">6</span>,  <span class="number">7</span>],</div><div class="line">[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">12</span>, <span class="number">13</span>],</div><div class="line">[<span class="number">10</span>, <span class="number">11</span>, <span class="number">14</span>, <span class="number">15</span>]]]</div></pre></td></tr></table></figure><p>🔺坑点2：提取k个保留的通道时，只需取索引的前k个值</p><p>假设放大倍率是4，用L1的剪枝方式，需要保留的通道数为<code>c_keep</code>。当对应到具体的剪枝通道的时候，需要找到PixShuffle后剪掉通道所对应的原始卷积输出的4个通道。从上面的坐标我们就可以看出，剪掉Y的0通道时，需要对应剪掉X的0、4、8、12通道。来看看具体的实现。主要分为几步：</p><ol><li>计算Y对应X的通道</li><li>计算Y需要保留的通道</li><li>将Y的通道映射回X</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. 计算Y对应X的通道</span></div><div class="line">norm_list, shuffled_idx_list = [], []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(ps_out):</div><div class="line">shuffled_idx = [i+k*ps_out <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">4</span>)]  <span class="comment"># Y通道对应的4个X通道</span></div><div class="line">shuffled_idx_list.append(shuffled_idx)</div><div class="line">  norm_sum = tf.reduce_sum(tf.gather(norm_value, shuffled_idx)) <span class="comment"># 提取对应索引的通道</span></div><div class="line">  norm_list.append(sess.run(norm_sum))</div><div class="line">  </div><div class="line"><span class="comment"># 2. 计算需要保留的通道</span></div><div class="line">remain_idx = np.sort(np.argsort(norm_list)[::<span class="number">-1</span>][:int(c_keep/<span class="number">4</span>)])</div><div class="line"></div><div class="line">remain_list = []</div><div class="line"><span class="comment"># 3. 将Y的通道映射回X</span></div><div class="line"><span class="keyword">for</span> remain <span class="keyword">in</span> remain_idx:</div><div class="line">  remain_list.extend(shuffled_idx_list[remain])</div><div class="line">remain_list = np.sort(remain_list)</div></pre></td></tr></table></figure><p>这样<code>remain_list</code>即原始卷积输出需要剪掉的通道索引。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在尝试对PixelShuffle前的卷积层做剪枝时遇到了一些问题，对PixelShuffle的具体操作有了进一步的了解。&lt;/p&gt;
&lt;p&gt;PixelShuffle通过将通道重排对图像进行上采样，tf中的函数是&lt;code&gt;tf.depth_to_sapce&lt;/code&gt;，第一个
      
    
    </summary>
    
    
      <category term="维修指南" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.06</title>
    <link href="http://yoursite.com/2019/06/18/weekly-paper-06/"/>
    <id>http://yoursite.com/2019/06/18/weekly-paper-06/</id>
    <published>2019-06-18T01:49:16.000Z</published>
    <updated>2019-06-21T09:12:49.701Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers"><a href="#1️⃣-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers" class="headerlink" title="1️⃣ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers"></a>1️⃣ AutoSlim: Towards One-Shot Architecture Search for Channel Numbers</h3><p>这篇和<a href="https://arxiv.org/abs/1812.08928" target="_blank" rel="external">ICLR2019</a>、<a href="https://arxiv.org/abs/1903.05134" target="_blank" rel="external">Universally Slimmable Networks</a>是同一个作者，解决的问题都是通道剪枝。下面先了解一下本文。</p><p><strong>Why?</strong></p><p>Most channel pruning methods are grouneded on <strong>the importance of trained weights</strong>, so the slimmed layer usually consists channels of discrete index. Most NAS methods have high computational cost and time cost.</p><p><strong>How?</strong></p><p>Extending the work of slimmable networks and propose AutoSlim. The training process is as following:</p><ol><li><p>Train a slimmable model for a few epochs to get a benchmark performance estimator.</p><ul><li><p>Searching space is defined between the upper bound and lower bound of channel numbers. In each training iteration, randomly sample the number of channels in each layer. In each layer remove a group of channels. </p><blockquote><p>in resents, first sample the channel number of residual dentity pathway and then randomly and independenly sample channel number inside each residual block.</p></blockquote></li></ul></li><li><p>Evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop on validation set.</p></li><li><p>Obtain the optimized channel configurations under different resource constraints.</p></li><li><p>Train optimized architectures individually or slimmable network for full training epochs.</p></li></ol><p>The paper is based on the assumption that <strong>the importance of weight is implicitly ranked by its index</strong>, which means that the smaller index of one filter the more important of this filter.</p><h3 id="2️⃣-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks"><a href="#2️⃣-AutoGrow-Automatic-Layer-Growing-in-Deep-Convolutional-Networks" class="headerlink" title="2️⃣ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks"></a>2️⃣ AutoGrow: Automatic Layer Growing in Deep Convolutional Networks</h3><p>The method can be easily found in the title, to gradually grow the depth of DNN.</p><p>The  <em>network</em> is  composed of <em>sub-netwok</em>, and <em>sub-network</em> is composed of <em>sub-modules</em>.<br>$$<br>g\left(\mathcal{X}_{0}\right)=l\left(\boldsymbol{f}_{M-1}\left(\boldsymbol{f}_{M-2}\left(\cdots \boldsymbol{f}_{1}\left(\boldsymbol{f}_{0}\left(\mathcal{X}_{0}\right)\right) \cdots\right)\right)\right)<br>$$<br>AutoGrow is based on Network Morphism, but propose to initilize the last Batch Normalization layer in a residual block of <em>AdamInit</em> insted of <em>ZeroInit</em></p><p><strong>AdamInit</strong></p><p>given the new layers $\mathcal{W}$, we have:<br>$$<br>g\left(\mathcal{X}_{0} ; \mathbb{W}\right)=g\left(\mathcal{X}_{0} ; \mathbb{W} \cup \mathcal{W}\right) \forall \mathcal{X}_{0}<br>$$<br>freeze all parameters except the last Batch Normalization layer in $\mathcal{W}$, use Adam optimizer to optimize the last Batch Normalization layer.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-AutoSlim-Towards-One-Shot-Architecture-Search-for-Channel-Numbers&quot;&gt;&lt;a href=&quot;#1️⃣-AutoSlim-Towards-One-Shot-Architecture-Search-f
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.05</title>
    <link href="http://yoursite.com/2019/06/10/weekly-paper-05/"/>
    <id>http://yoursite.com/2019/06/10/weekly-paper-05/</id>
    <published>2019-06-10T02:29:31.000Z</published>
    <updated>2019-06-18T01:49:25.901Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-Dynamic-Capacity-Networks"><a href="#1️⃣-Dynamic-Capacity-Networks" class="headerlink" title="1️⃣ Dynamic Capacity Networks"></a>1️⃣ Dynamic Capacity Networks</h3><p>use two alternative sub-networks: </p><ol><li>coarse layers $f_c$ on the whole input $\mathbf {x}$  </li><li>fine layers $f_f$ at salient regions </li></ol><p><strong>coarse representation vectors</strong><br>$$<br>f_{c}(\mathbf{x})=\left\{\mathbf{c}_{i, j} |(i, j) \in\left[1, s_{1}\right] \times\left[1, s_{2}\right]\right\}<br>$$</p><p>$$<br>h_{c}(\mathbf{x})= \mathbf{o}_c = g\left(f_{c}(\mathbf{x})\right)<br>$$</p><p>$\mathbf{c}_{i, j}=f_{c}\left(\mathbf{x}_{i, j}\right) \in \mathbb{R}^{D}$ </p><p><strong>salient input regions</strong><br>$$<br>H=-\sum_{l=1}^{C} \mathbf{o}_{c}^{(l)} \log \mathbf{o}_{c}^{(l)}<br>$$</p><p>$$<br>M_{i, j}=\left|\nabla_{\mathbf{c}_{i, j}} H\right|_{2}<br>$$</p><p>$C$ is the number of class labels, $\mathbf{M} \in \mathbb{R}^{s_{1} \times s_{2}}$</p><p>select top $k$ input regions $\mathbf{X}^{s}=\left\{\mathbf{x}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}$ based on $\mathbf{M}$</p><p><strong>fine representation vectors</strong><br>$$<br>f_{f}\left(\mathbf{X}^{s}\right)=\left\{\mathbf{f}_{i, j} |(i, j) \in \mathbf{I}^{s}\right\}<br>$$<br>refined representation $f_r(\mathbf {x})$ by combining $f_c(\mathbf{x})$ and $f_f(\mathbf{X}^s)$</p><p><strong>loss</strong></p><ol><li><p>Cross Entropy<br>$$<br>J=-\sum_{i=1}^{m} \log p\left(y^{(i)} | \mathbf{x}^{(i)} ; \theta\right)<br>$$</p></li><li><p>encourage similarity between the coarse and fine representations</p></li></ol><p>$$<br>\sum_{\mathbf{x}_{i, j} \in \mathbf{X}^{s}}\left|f_{c}\left(\mathbf{x}_{i, j}\right)-f_{f}\left(\mathbf{x}_{i, j}\right)\right|_{2}^{2}<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-Dynamic-Capacity-Networks&quot;&gt;&lt;a href=&quot;#1️⃣-Dynamic-Capacity-Networks&quot; class=&quot;headerlink&quot; title=&quot;1️⃣ Dynamic Capacity Networks&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Gumbel Softmax</title>
    <link href="http://yoursite.com/2019/05/28/gumbel-softmax/"/>
    <id>http://yoursite.com/2019/05/28/gumbel-softmax/</id>
    <published>2019-05-28T07:29:09.000Z</published>
    <updated>2019-05-30T03:18:19.159Z</updated>
    
    <content type="html"><![CDATA[<p>在PAG里发现了Gumbel Sampling Trick，把离散的采样过程用公式表达出来，于是可以放进神经网络中进行求导和反向，觉得是很有意思的工作，想要多加深一些了解。</p><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><p>通过<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">博客</a>入了一下小门，结合<a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="external">知乎</a>，首先来理解一下Gumbel Sampling Trick用来做什么。</p><blockquote><p>已知一个离散随机变量X的分布，我们想得到一些服从这个分布的离散的x的值。</p></blockquote><p>比较简单的方法是用<code>np.random.choice</code>。比如我们现在有5个值，概率分布是<code>[0.1, 0, 0.3, 0.6, 0]</code>，即第4个元素最有可能被采样到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</div><div class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>])</div></pre></td></tr></table></figure><p>这样我们是获取到了值，但是这个过程在神经网络中无法求导和方向。于是gumbel-max出现了：</p><blockquote><p>将采样的过程公式化，公式中的参数为离散随机变量的概率分布。</p></blockquote><p>$$<br>z_{i}=\left\{\begin{array}{l}{1, i=\operatorname{argmax}_{j}\left(\log \left(p_{j}\right)+g_{j}\right)} \\ {0, \text { otherwise }}\end{array}\right.<br>$$</p><p>其中$g_{i}$代表gumbel噪声，$g_{i}=-\log \left(-\log \left(u_{i}\right)\right), u_{i} \sim U n i f o r m(0,1)$。输出$z_i$是一个$j$维的one-hot向量。</p><p>由于argmax不可导，用可导的softmax替代argmax<br>$$<br>\boldsymbol{z}=\operatorname{softmax}((\log (\boldsymbol{p})+\boldsymbol{g}) / \tau)<br>$$<br>参数$ \tau$越小，$z$越接近one-hot向量。</p><blockquote><p>我们把不可导的采样过程，从x本身转嫁到了求取x的公式中的一项g上面，而g不依赖于概率分布p。这样一来，x对p仍然是可导的，而我们得到的x仍然是离散值的采样。这样的采样过程转嫁的技巧叫再参化技巧(reparameterization trick)</p></blockquote><p>那么网络有哪些地方需要采样呢？接下来了解一下VAE的相关应用。</p><h3 id="相关应用"><a href="#相关应用" class="headerlink" title="相关应用"></a>相关应用</h3><p><strong>变分自动编码器VAE</strong></p><p><a href="http://kvfrans.com/variational-autoencoders-explained/" target="_blank" rel="external">这篇博客</a> 解释得很好，自动编码器由编码器(encoder, E)和解码器(decoder, D)构成，E对输入图像进行编码，生成隐向量， D对隐向量进行解码，输出图像。</p><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165000500-1207992534.jpg" alt="img"></p><p>但是这样我们必须通过图像来生成隐向量，局限性较大，可不可以随便来一个隐向量，输入进D就能生成图片呢？于是VAE就出现了。</p><blockquote><p>限制编码器生成服从单元高斯分布的隐向量。</p></blockquote><p>因此学习目标就可以分为两部分：1）生成图像和真实图像尽可能接近； 2）隐变量服从单元高斯分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">generation_loss = mean(square(generated_image - real_image))  </div><div class="line">latent_loss = KL-Divergence(latent_variable, unit_gaussian)  </div><div class="line">loss = generation_loss + latent_loss</div></pre></td></tr></table></figure><p>为了优化KL散度，需要引入👆🏻提到过的再参化技巧。</p><blockquote><p>E不直接生成隐向量，而是生成一个均值向量和一个方差向量。再通过均值和方差采样出隐向量。</p></blockquote><p><img src="https://images2018.cnblogs.com/blog/1428973/201808/1428973-20180813165407236-1369432498.png" alt="img"> </p><p>至此我们明白了<strong>采样</strong>是为了让数据尽可能服从某一分布，通过<strong>再参化技巧</strong>来学习这个分布的参数。高斯分布是连续的，直接可求导，那一些不连续的离散分布怎么办呀？这就回到了一开始的问题「Gumbel Sampling Trick」。</p><p><strong>分类再参化(Categorical reparameterization)</strong></p><p>ICLR 2017的<a href="https://arxiv.org/pdf/1611.01144.pdf" target="_blank" rel="external">这篇文章</a> 就利用Gumbel-Softmax分布，将离散的分类概率分布采样过程转化为了可求导的过程。</p><p><img src="https://i.loli.net/2019/05/30/5cef4476d308a17728.png" alt=""></p><p>上图反映了参数$ \tau$对连续概率分布(a)和离散的one-hot类别分布的影响。当$ \tau$太小时会导致梯度的方差过大，所以文章在实验中用了退火的策略来逐渐减小参数$ \tau$。还可以利用熵正则来学习$\tau$，自动调整Gumbel-Softmax分布采样的置信度。</p><p>本文的训练过程采用Straight-Through (ST) Gumbel Estimator，即前向用argmax，梯度回传时用softmax的梯度。</p><p><strong>参考链接：</strong></p><ul><li><p><a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="external">Gumbel-Softmax Trick和Gumbel分布</a></p></li><li><p><a href="https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/" target="_blank" rel="external">The Gumbel-Max Trick for Discrete Distributions</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在PAG里发现了Gumbel Sampling Trick，把离散的采样过程用公式表达出来，于是可以放进神经网络中进行求导和反向，觉得是很有意思的工作，想要多加深一些了解。&lt;/p&gt;
&lt;h3 id=&quot;问题引入&quot;&gt;&lt;a href=&quot;#问题引入&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>weekly-paper-04</title>
    <link href="http://yoursite.com/2019/05/25/weekly-paper-04/"/>
    <id>http://yoursite.com/2019/05/25/weekly-paper-04/</id>
    <published>2019-05-25T08:30:05.000Z</published>
    <updated>2019-05-27T12:12:52.191Z</updated>
    
    <content type="html"><![CDATA[<p>为了锻炼自己的英语写作能力，以后尽量用英文做进行归纳（⌘+C &amp; ⌘+V）～</p><h3 id="1️⃣-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression"><a href="#1️⃣-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression" class="headerlink" title="1️⃣ To prune, or not to prune: exploring the efficacy of pruning for model compression"></a>1️⃣ To prune, or not to prune: exploring the efficacy of pruning for model compression</h3><p>这篇是TensorFlow自己出的，直接在训练过程中融合L1剪枝。通过将操作融入TensoFlow的training graph，在训练过程中对权重进行排序，用一个mask将最小的weights置0。从inital sparsity values $s_i$开始，以$\Delta t$ 的剪枝频率，最终达到final sparsity value $s_f$<br>$$<br>s_{t}=s_{f}+\left(s_{i}-s_{f}\right)\left(1-\frac{t-t_{0}}{n \Delta t}\right)^{3} \text { for } t \in\left\{t_{0}, \quad t_{0}+\Delta t, \ldots, t_{0}+n \Delta t\right\}<br>$$<br>masks每隔$\Delta t$更新一次，直到达到$s_f$后不再更新。同时文章表明，$n$的选择与学习率的下降策略密切相关。</p><h3 id="2️⃣-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS"><a href="#2️⃣-OBJECT-DETECTORS-EMERGE-IN-DEEP-SCENE-CNNS" class="headerlink" title="2️⃣ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS"></a>2️⃣ OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS</h3><p>📍<a href="https://github.com/metalbubble/cnnvisualizer" target="_blank" rel="external">Github Repo</a></p><p><strong>Contributions</strong></p><ul><li>object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet</li><li>the same network can do both object localization and scene recognition in a single forward-pass.</li></ul><p><strong>Experiments</strong></p><ul><li><p>identify the differences in the type of images preferred at the different layers of each network</p></li><li><p>Places-CNN and ImageNet-CNN  prefer similar images in the earlier layers, while the later layers tend to be more specialized to the specific task of scene or object categorization.</p></li><li><p>understand the nature of the representation that the network is learning</p><ul><li><em>simplifying the input images:</em> 1) removing segments from the image to produce the smallest decrease of the correct classification score until the image is incorrectly classified 2) generate the minimal image representations using image set of SUN database. =&gt; use minimal image representations as inputs to show the contribute important information for the network to recognize the scene.</li><li><em>visualize the receptive fields (RFs) of units and their activatoin patterns:</em> use sliding-window to identify which regions of the image led to the high unit activations. =&gt; as the layers go deeper the RF size gradually increases and the activation regions become more semantically meaningful.</li><li><em>understan and quantify the precise semantic learnd by each unit: </em>ask AMT to indentify the common concepts that exists between the top scoring segmentations for each unit.</li></ul></li><li><p>emergence of objects as the internal representation</p><ul><li><p>what object classes emerge? =&gt; use pool5 to show the distribution of objects</p></li><li><p>why do those obejcts emerge? </p><ul><li><p>possibility 1:  the objects correspond to the most frequent ones in the database. (correlation is 0.54)</p></li><li><p>possibility 2:  the objects that allow discriminatin among scene categories. (correlation is 0.84)</p><p>=&gt; the network is automatically identifying the most discriminative object categories to a large extent</p></li></ul></li></ul></li></ul><h3 id="3️⃣-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations"><a href="#3️⃣-Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations" class="headerlink" title="3️⃣ Network Dissection: Quantifying Interpretability of Deep Visual Representations"></a>3️⃣ Network Dissection: Quantifying Interpretability of Deep Visual Representations</h3><p>📍<a href="https://github.com/CSAILVision/NetDissect-Lite" target="_blank" rel="external">Github Repo</a></p><p><strong>Questions</strong></p><ul><li>What is a disentangled representation, and how can its factors be quantified and detected?</li><li><p>Do interpretable hidden units reflect a special alignment of feature space, or are interpretations a chimera?</p></li><li><p>What conditions in state-of-the-art training lead to representations with greater or lesser entanglement?</p></li></ul><p><strong>Measurement of interpretability: three-step process of Network Dissection</strong></p><ol><li><p>Identify a broad set of human-labeld visual concepts.</p></li><li><p>Gather hidden variables’ response to known concepts.</p><ul><li>draw concepts $c$ from the Broden dataset.</li></ul></li><li><p>Quantify alignment of hidden variable — concept pairs.</p><ul><li><p>Scoring Unit Interpretability</p><p>input image $x$, activation map $A_{k}(\mathbf{x}) \stackrel{scale up}{\longrightarrow}S_k(x) $，individual unit activations $a_k$</p><p>top quantile level $T_k$： $P\left(a_{k}&gt;T_{k}\right)=0.005$</p><p>binary segmentation：$M_{k}(\mathbf{x}) \equiv S_{k}(\mathbf{x}) \geq T_{k}$</p><p>input annotaion mask $L_c$ </p><p>score：the accuracy of unit $k$ in detecting concept $c$<br>$$<br>I o U_{k, c}=\frac{\sum\left|M_{k}(\mathbf{x}) \cap L_{c}(\mathbf{x})\right|}{\sum\left|M_{k}(\mathbf{x}) \cup L_{c}(\mathbf{x})\right|}<br>$$</p></li></ul></li></ol><h3 id="4️⃣-Pixel-wise-Attentional-Gating-for-Scene-Parsing"><a href="#4️⃣-Pixel-wise-Attentional-Gating-for-Scene-Parsing" class="headerlink" title="4️⃣ Pixel-wise Attentional Gating for Scene Parsing"></a>4️⃣ Pixel-wise Attentional Gating for Scene Parsing</h3><p><strong>Contributions:</strong></p><ul><li>Dynamic computation depth: insert PAG at multiple lyaers of ResNet to control computational parsimony.</li><li>Dynamic spatial pooling: adaptively chooses the proper pooling size for each pixel to aggregate information for inference.</li><li>Experimetns on various pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation.</li></ul><p><img src="https://i.loli.net/2019/05/27/5ceb52642beaa24177.png" alt=""></p><p>binary spatial mask $\mathbf{G}$ on ResBottleneck:<br>$$<br>\begin{array}{ll}{\mathbf{X}=\mathcal{F}^{1}(\mathbf{I})} &amp; {\mathbf{X}=\mathcal{F}^{1}(\mathbf{I}), \mathbf{G}=\mathcal{G}(\mathbf{I})} \\ {\mathbf{Y}=\mathcal{F}^{2}(\mathbf{X})} &amp; {\mathbf{Y}=\mathcal{F}_{\mathbf{G}}^{2}(\mathbf{X})} \\ {\mathbf{Z}=\mathcal{F}^{3}(\mathbf{Y})} &amp; {\mathbf{Z}=\mathcal{F}_{\mathbf{G}}^{3}(\overline{\mathbf{G}} \odot \mathbf{X}+\mathbf{G} \odot \mathbf{Y})} \\ {\mathbf{O}=\mathbf{I}+\mathbf{Z}} &amp; {\mathbf{O}=\mathbf{I}+\mathbf{Z}}\end{array}<br>$$<br><strong>Methods:</strong></p><ul><li><p>Learning attention maps</p><blockquote><p>The key to the proposed PAG is the gating function G that produces a discrete (binary) mask which allows for reduced computation. However, producing the binary mask using hard thresholding is non-differentiable, and thus cannot be simply incorporated in CNN where gradient descent is used for training. To bridge the gap, we exploit the Gumbel-Max trick [19] and its recent continuous relaxation [39, 28].</p></blockquote><p>Gumbel distribution  $m \equiv-\log (-\log (u))$, where $u \sim \mathcal{U}[0,1]$</p><p>$g$ is a discrete random variable with probabilities  $P(g=k) \propto a_{k}$</p><p>$\left\{m_{k}\right\}_{k=1, \dots, K}$ is a sequence of i.i.d Gumbel random variables </p><p>sample from the discrete variable:<br>$$<br>g=\underset{k=1, \ldots, K}{\operatorname{argmax}}\left(\log \alpha_{k}+m_{k}\right)<br>$$<br>Gumbel Sampling Trick (replaces the argmax operation with a softmax): </p></li></ul><p>$$<br>\mathbf{g}=\operatorname{softmax}((\log (\boldsymbol{\alpha})+\mathbf{m}) / \tau)<br>$$</p><p>​        <strong>forwardd pass</strong>: discrete smaples of the argmax </p><p>​        <strong>backward pass</strong>: compute gradient of the softmax relaxation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了锻炼自己的英语写作能力，以后尽量用英文做进行归纳（⌘+C &amp;amp; ⌘+V）～&lt;/p&gt;
&lt;h3 id=&quot;1️⃣-To-prune-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compressi
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.03</title>
    <link href="http://yoursite.com/2019/05/19/weekly-paper-03/"/>
    <id>http://yoursite.com/2019/05/19/weekly-paper-03/</id>
    <published>2019-05-19T13:09:17.000Z</published>
    <updated>2019-05-23T10:42:12.181Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition"><a href="#1️⃣-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition" class="headerlink" title="1️⃣ On Compressing Deep Models by Low Rank and Sparse Decomposition"></a>1️⃣ On Compressing Deep Models by Low Rank and Sparse Decomposition</h3><p>本文将网络权重分解成低秩和稀疏的成分，利用贪心双边分解（GreBdec）算法进行模型压缩。</p><p>目标函数：<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{rank}(L) \leq r} \\ &amp;card(S) \leq c \end{array}<br>$$<br>假设$L=UV$，其中$U \in R^{m \times r}, V \in R^{r \times k}$。本文用两个卷积层进行低秩近似，$V$将通道数映射到$r$，$U$代表$1\times1$卷积。然后把低秩近似的结果和稀疏的结果相加利用mask乘到原filters上，修改目标函数：<br>$$<br>\begin{array}{cl}{\min _{L, S}} &amp; {\frac{1}{2 n}|Y-(L+S) X|_{F}^{2}} \\ {\text {s.t.}} &amp; {\frac{1}{2}|W-L-S|_{F}^{2} \leq \gamma} \\ &amp; rank(L) \leq r, \\ &amp; card(S) \leq c.\end{array}<br>$$<br>等同于利用迭代优化策略优化目标函数：<br>$$<br>\frac{1}{2 n}|Y-(L+S) X|_{2}^{2}+\frac{\lambda}{2}|W-L-S|_{F}^{2}<br>$$<br>其中<br>$$<br>\left\{\begin{array}{l}{L_{i}=\text { TruncatedGSVD }\left(B_{i} A^{\dagger}, r\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>本文用SVD-free的GreBdec算法进行优化，令$L=UV$<br>$$<br>\begin{array}{cl}{\min _{U, V, S}} &amp; {\frac{1}{2 n}|Y-(U V+S) X|_{F}^{2}+\frac{\lambda}{2}|W-U V-S|_{F}^{2}} \\ {\text {s.t.}} &amp; {\operatorname{card}(S) \leq c}\end{array}<br>$$<br>$U,V,S$通过以下公式更新：<br>$$<br>\left\{\begin{array}{l}{U_{i}=B_{i} V_{i-1}^{\top}\left(V_{i-1} A V_{i-1}^{\top}\right)^{\dagger}} \\ {V_{i}=\left(U_{i}^{\top} U_{i}\right)^{\dagger} U_{i}^{\top}\left(B_{i} A^{\dagger}\right)} \\ {S_{i}=P_{\Omega}(M), \text { and } M=S_{i-1}-\eta\left(A S_{i-1}-C_{i}\right)}\end{array}\right.<br>$$<br>然后又经过一番变换作者利用QR分解得到一个让$U,V$更快更新的规则：<br>$$<br>\left\{\begin{array}{l}{U_{i}=Q, Q R\left(B_{i} V^{\top}\right)=Q R} \\ {V_{i}=Q^{\top}\left(B_{i} A^{\dagger}\right)}\end{array}\right.<br>$$<br><img src="https://i.loli.net/2019/05/20/5ce2015dee80b29804.png" alt=""></p><p>###2️⃣ Spatial Transformer Networks</p><p>对输入图像进行空间上的变换，以学到图像的不变性(invariance)。配合<a href="https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html" target="_blank" rel="external">PyTorch Tutorial</a>食用。</p><p><img src="https://i.loli.net/2019/05/23/5ce663632bbf215753.png" alt=""></p><p>STN也类似一个插件，主要由两个模块组成：</p><ul><li>Localisation net：输入feature map $U \in \mathbb{R}^{H \times W \times C}$，输出变换参数$\theta=f_{\mathrm{loc}}(U)$。其中$\theta$是一个6维的仿射变换。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer localization-network</span></div><div class="line">self.localization = nn.Sequential(</div><div class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</div><div class="line">  nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>)</div><div class="line">)</div></pre></td></tr></table></figure><ul><li>Grid generator：对图像用$A_\theta$进行2D仿射变换，其中$(x_i^t, y_I^t)$为target像素点坐标，$\left(x_{i}^{s}, y_{i}^{s}\right)$为source采样点的坐标。</li></ul><p>$$<br>\left( \begin{array}{c}{x_{i}^{s}} \\ {y_{i}^{s}}\end{array}\right)=\mathcal{T}_{\theta}\left(G_{i}\right)=\mathrm{A}_{\theta} \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)=\left[ \begin{array}{ccc}{\theta_{11}} &amp; {\theta_{12}} &amp; {\theta_{13}} \\ {\theta_{21}} &amp; {\theta_{22}} &amp; {\theta_{23}}\end{array}\right] \left( \begin{array}{c}{x_{i}^{t}} \\ {y_{i}^{t}} \\ {1}\end{array}\right)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Regressor for the 3 * 2 affine matrix</span></div><div class="line">self.fc_loc = nn.Sequential(</div><div class="line">  nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),</div><div class="line">  nn.ReLU(<span class="keyword">True</span>),</div><div class="line">  nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)</div><div class="line">)</div></pre></td></tr></table></figure><p>为了在$U$上应用空间变换输出$V$，需要一个可导的采样函数生成采样点$\mathcal{T}_\theta(G)$。<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} k\left(x_{i}^{s}-m ; \Phi_{x}\right) k\left(y_{i}^{s}-n ; \Phi_{y}\right) \forall i \in\left[1 \ldots H^{\prime} W^{\prime}\right] \forall c \in[1 \ldots C]<br>$$<br>其中$k$为sampling kernel，可以定义为integer sampling kernel:<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \delta\left(\left\lfloor x_{i}^{s}+0.5\right\rfloor- m\right) \delta\left(\left\lfloor y_{i}^{s}+0.5\right\rfloor- n\right)<br>$$<br>也可以定义为bilinear sampling kernel：<br>$$<br>V_{i}^{c}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|x_{i}^{s}-m\right|\right) \max \left(0,1-\left|y_{i}^{s}-n\right|\right)<br>$$<br>对输入求偏导有：<br>$$<br>\frac{\partial V_{i}^{c}}{\partial x_{i}^{s}}=\sum_{n}^{H} \sum_{m}^{W} U_{n m}^{c} \max \left(0,1-\left|y_{i}^{s}-n\right|\right) \left\{\begin{array}{ll}{0} &amp; {\text { if }\left|m-x_{i}^{s}\right| \geq 1} \\ {1} &amp; {\text { if } m \geq x_{i}^{s}} \\ {-1} &amp; {\text { if } m<x_{i}^{s}}\end{array}\right. $$="" 把<em="">localisation network, grid generator, sampler结合起来构成一个STN模块：</x_{i}^{s}}\end{array}\right.></p><p><strong>STN</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Spatial transformer network forward function</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stn</span><span class="params">(self, x)</span>:</span></div><div class="line">  xs = self.localization(x)</div><div class="line">  xs = xs.view(<span class="number">-1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>)</div><div class="line">  theta = self.fc_loc(xs)</div><div class="line">  theta = theta.view(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line">  grid = F.affine_grid(theta, x.size())</div><div class="line">  x = F.grid_sample(x, grid)</div><div class="line"></div><div class="line">  <span class="keyword">return</span> x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse-Decomposition&quot;&gt;&lt;a href=&quot;#1️⃣-On-Compressing-Deep-Models-by-Low-Rank-and-Sparse
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>HEXO主题cactus修改</title>
    <link href="http://yoursite.com/2019/05/19/hexo-theme-cactus/"/>
    <id>http://yoursite.com/2019/05/19/hexo-theme-cactus/</id>
    <published>2019-05-19T03:05:14.000Z</published>
    <updated>2019-05-19T08:55:36.409Z</updated>
    
    <content type="html"><![CDATA[<p>cacuts的主题很简洁，用得蛮久，看到原库有更新，所以fork了新的版本并在上面做一些修改，顺便记录一下过程。</p><h3 id="主题镜像"><a href="#主题镜像" class="headerlink" title="主题镜像"></a>主题镜像</h3><p>首先根据<a href="https://help.github.com/en/articles/duplicating-a-repository" target="_blank" rel="external">Mirrow a repository</a>镜像一个库。在push的时候还遇到了403问题：</p><blockquote><p>remote: Permission to colorjam/hexo-theme-cactus-mirrored.git denied to xxx</p></blockquote><p>通过删除<strong>Keychain Access</strong>中存储的github.com的Internet password得到解决。然后把自己的库再Clone进<code>themes</code>中</p><h3 id="样式编辑"><a href="#样式编辑" class="headerlink" title="样式编辑"></a>样式编辑</h3><ul><li><p>主题颜色</p><p>在<code>source/css/_colors</code>下新建了一个<code>pink.styl</code>，同时修改<code>_config.yml</code>中的<code>colorscheme:pink</code>。</p></li><li><p>logo设置</p><p>把<code>source/images/</code>下的<code>favicon.ico</code>和<code>logo.png</code>换成自己喜欢的图片。修改<code>source/css/_partial/header.styl</code>中的<code>#logo</code> 的<code>background-size: contain</code></p></li><li><p>细节调整</p><p>删除<code>header.styl</code>中html的<code>border-top</code></p><p>链接样式：</p></li></ul><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">a</span></div><div class="line">  color: $color-text</div><div class="line">  <span class="selector-tag">text-decoration</span>: <span class="selector-tag">none</span></div><div class="line"></div><div class="line">  &amp;<span class="selector-pseudo">:hover</span></div><div class="line">  background-image: linear-gradient(transparent, transparent 4px, $color-link 4px, $color-link)</div><div class="line">  <span class="selector-tag">background-position</span>: <span class="selector-tag">bottom</span></div><div class="line">  <span class="selector-tag">background-size</span>: 100% 6<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background-repeat</span>: <span class="selector-tag">repeat-x</span></div></pre></td></tr></table></figure><p>​    行内代码样式：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">code</span></div><div class="line">  <span class="selector-tag">padding</span>: 0 5<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background</span>: <span class="selector-id">#f6f8fa</span></div><div class="line">  <span class="selector-tag">border-radius</span>: 2<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">-webkit-border-radius</span>: 2<span class="selector-tag">px</span></div></pre></td></tr></table></figure><h3 id="会动的粒子"><a href="#会动的粒子" class="headerlink" title="会动的粒子"></a>会动的粒子</h3><p>在背景加上<a href="https://github.com/VincentGarreau/particles.js/" target="_blank" rel="external">会动的粒子</a>，在<code>source/lib</code>里创建一个particles文件夹，把<code>particles.min.js</code>放进去。</p><p>在<code>layout.ejs</code>中加入</p><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"particles-js"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure><p>在<code>scripts.ejs</code>中添加脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;!-- particles --&gt;</div><div class="line">&lt;%- js(&apos;lib/particles/particles.min&apos;) %&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">particlesJS(&apos;particles-js&apos;, &#123;</div><div class="line">        ...</div><div class="line">        &#125;</div><div class="line">      )</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure><p>在<code>style.css</code>中添加样式：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="selector-id">#particles-js</span> &#123;</div><div class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</div><div class="line">  <span class="attribute">position</span>: absolute;</div><div class="line">  <span class="attribute">margin-left</span>: -<span class="number">28%</span>;</div><div class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cacuts的主题很简洁，用得蛮久，看到原库有更新，所以fork了新的版本并在上面做一些修改，顺便记录一下过程。&lt;/p&gt;
&lt;h3 id=&quot;主题镜像&quot;&gt;&lt;a href=&quot;#主题镜像&quot; class=&quot;headerlink&quot; title=&quot;主题镜像&quot;&gt;&lt;/a&gt;主题镜像&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="维修指南" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow内存泄漏</title>
    <link href="http://yoursite.com/2019/05/18/tf-memory-leak/"/>
    <id>http://yoursite.com/2019/05/18/tf-memory-leak/</id>
    <published>2019-05-18T05:13:31.000Z</published>
    <updated>2019-05-19T06:54:57.626Z</updated>
    
    <content type="html"><![CDATA[<p>用tf经常会出现OOM的现象，查了一下发现了一篇文章<a href="https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/" target="_blank" rel="external">How To Debug A Memory Leak In Tensorflow</a></p><p>由于tf存在内存泄漏问题，许多人会用 <a href="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" target="_blank" rel="external">tcmalloc</a> 来替代 malloc()。</p><p>但是运行程序的时候会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ERROR: ld.so: object <span class="string">'/usr/lib/libtcmalloc.so.4'</span> from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.</div></pre></td></tr></table></figure><p>踩了一系列坑以后发现，将<code>/usr/lib/libtcmalloc.so.4</code>改为<code>/usr/local/lib/libtcmalloc.so.4</code>即可。</p><p><strong>参考链接：</strong></p><ul><li><a href="https://www.cnblogs.com/Lelouch/p/3365672.html" target="_blank" rel="external">https://www.cnblogs.com/Lelouch/p/3365672.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用tf经常会出现OOM的现象，查了一下发现了一篇文章&lt;a href=&quot;https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;How
      
    
    </summary>
    
    
      <category term="维修指南" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>pair-wise-loss</title>
    <link href="http://yoursite.com/2019/05/17/pair-wise-loss/"/>
    <id>http://yoursite.com/2019/05/17/pair-wise-loss/</id>
    <published>2019-05-17T05:44:19.000Z</published>
    <updated>2019-05-17T08:08:40.815Z</updated>
    
    <content type="html"><![CDATA[<p>用Tensorflow复现论文中的pair wise loss<br>$$<br>\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathcal{R}} \sum_{j \in \mathcal{R}}\left(a_{ij}^{s}-a_{ij}^{t}\right){2}<br>$$<br>其中<br>$$<br>a_{i j}=\mathbf{f}_{i}^{\top} \mathbf{f}_{j} /\left(\left|\mathbf{f}_{i}\right|_{2}\left|\mathbf{f}_{j}\right|_{2}\right)<br>$$<br>$f_i$和$f_j$分别代表ith / jth像素点的c维特征。参考了余弦相似性的计算方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(x)</span>:</span></div><div class="line">    x = tf.reshape(x, [x.shape[<span class="number">0</span>], <span class="number">-1</span>, x.shape[<span class="number">-1</span>]])</div><div class="line">    norm = tf.nn.l2_normalize(x, <span class="number">2</span>)</div><div class="line">    a = tf.matmul(norm, norm, adjoint_b = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> a</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_loss</span><span class="params">(x, y)</span>:</span></div><div class="line">    _, h, w, _  = x.shape</div><div class="line">    pa = tf.reduce_sum(tf.pow((similarity(x) - similarity(y)), <span class="number">2</span>)) / tf.pow(tf.cast(h*w, tf.float32),<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> pa</div></pre></td></tr></table></figure><p>参考链接：</p><ul><li><a href="https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1" target="_blank" rel="external">https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用Tensorflow复现论文中的pair wise loss&lt;br&gt;$$&lt;br&gt;\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathc
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LQ-Nets</title>
    <link href="http://yoursite.com/2019/05/14/LQ-Nets/"/>
    <id>http://yoursite.com/2019/05/14/LQ-Nets/</id>
    <published>2019-05-14T09:02:31.000Z</published>
    <updated>2019-05-18T08:21:12.014Z</updated>
    
    <content type="html"><![CDATA[<p>传统的量化方法主要使用固定的或者手工设计的量化方案（均匀量化/对数量化）：<br>$$<br>Q(x)=q_{l}, \text { if } x \in\left(t_{l}, t_{l+1}\right]<br>$$<br>本文提出了可学习的量化方式：<br>$$<br>Q_{\text { ours }}(x, \mathbf{v})=\mathbf{v}^{\mathrm{T}} \mathbf{e}_{l}, \quad \text { if } x \in\left(t_{l}, t_{l+1}\right]<br>$$<br>其中 $\mathbf{v} \in \mathbb{R}^{K}$是$\mathbf{e}_{l} \in\{-1,1\}^{K}$, $K$代表bit数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;传统的量化方法主要使用固定的或者手工设计的量化方案（均匀量化/对数量化）：&lt;br&gt;$$&lt;br&gt;Q(x)=q_{l}, \text { if } x \in\left(t_{l}, t_{l+1}\right]&lt;br&gt;$$&lt;br&gt;本文提出了可学习的量化方式：&lt;br&gt;$$&lt;br
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.02</title>
    <link href="http://yoursite.com/2019/05/13/weekly-paper-02/"/>
    <id>http://yoursite.com/2019/05/13/weekly-paper-02/</id>
    <published>2019-05-13T02:50:43.000Z</published>
    <updated>2019-05-19T04:29:39.092Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION"><a href="#1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION" class="headerlink" title="1️⃣ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION"></a>1️⃣ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION</h3><p>在《Revisiting the Importance of Individual Units in CNNs via Ablation》的基础上看了这篇论文。</p><p>本文探究的是激活值的单方向依赖对网络泛化性能的影响，通过对units进行抑制/加噪声，表示网络对单反向的依赖能较好的预测其泛化性能。文章讲了一个故事，一个网络只通过记忆每张输入和其对应的输出，泛化性差(memorizing network)，另一个网络能够找到数据中的结构性，泛化性佳(structure-finding network)。memorizing network找到的最小描述长度应该大于structure-finding network。因此，memorizing network会使用更多的单反向，那么，如果随机扰乱单一方向，对memorizing network的影响应该大于structure-finding network。</p><p>通过对dropout和BN实验（两个方法都增强了网络的泛化性），表明尽管dropout能在一定程度上避免记忆随机标签，但不能避免训练过程中的过度单方向依赖。加了BN的网络进行神经元抑制时，训练精度会降得比较慢，说明BN也不鼓励单方向依赖。</p><p>接着文章验证了class selectivity与神经元重要性的关系。提出了两个问题：</p><ol><li><p>BN不鼓励单方向依赖，那么是否会影响单反向的类别信息分布？</p><p>文章使用class selectivity来衡量类别信息分布，high class selectivity说明关注的是单一类别，low class selectivity说明关注的是多个类别。没有BN的网络反而显示出更高的class selectivity。表明BN层鼓励feature map去学习多种类别的信息，而不是关注单一类别。</p></li><li><p>是否能够利用unit的class selectivity，判断unit的重要性？</p><p>文章发现class selectivity和网络浅层的feature map负相关，与网络深层则无关。作者利用互信息也做了相同的实验。得到一致的结果。以此说明class selectiviy并不能代表unit的重要性。</p></li></ol><blockquote><p>🧐 本文的结论是紧凑网络对单方向的依赖性较少，那么如何找到一个衡量unit方向性的函数，来进行网络压缩呢？</p></blockquote><h3 id="2️⃣-MaskConnect-Connectivity-Learning-by-Gradient-Descent"><a href="#2️⃣-MaskConnect-Connectivity-Learning-by-Gradient-Descent" class="headerlink" title="2️⃣  MaskConnect: Connectivity Learning by Gradient Descent"></a>2️⃣  MaskConnect: Connectivity Learning by Gradient Descent</h3><p>用梯度下降自动学习连接。和网络权重一起学习<em>connectivity masks</em>，来决定网络block之间的连接。</p><p>第$j$个block的输入可以由前面所有输出相加而成，用二值的$m$表示是否连接：<br>$$<br>\mathbf{x}_{j}=\sum_{k=1}^{j-1} m_{j, k} \cdot \mathbf{y}_{k}<br>$$<br>本文表示每个block只和$K$个连接效果最好：<br>$$<br>m_{j, k} \in\{0,1\} \forall j, k, \quad and \quad \sum_{k=1}^{j-1} m_{j, k}=K \forall j<br>$$<br>🔺 训练过程：</p><p><strong>Forward Propagation</strong>. 限制实值的mask的和为1，即$\sum_{k=1}^{j-1} \tilde{m}_{j, k}=1$，代表一个多项式分布，从中采样K个样本$a_{1}, a_{2}, \ldots, a_{K} \in\{1, \ldots,(j-1)\}$，激活对应的mask $m_{j, a_{k}} \leftarrow 1$。</p><p><strong>Backward Propagation.</strong> 第$k$个block输出的梯度通过二值$m_{j,k}$和$x_j$的梯度获得。</p><p><strong>Mask Update.</strong> 通过clip实值mask，限制它们在[0,1]的范围。</p><p>🔺 训练结束：</p><p>（1）为每个$m_j$激活$\tilde{m}_{j}$中top-K的连接，</p><p>（2）固定二值mask，ft网络权重$\theta$</p><blockquote><p>🧐 本文算是NAS的分支吧，搜索的只是网络块之间的连接。每个block有一个多项式分布，代表它与之前所有block连接的概率，从这个分布中采样激活的连接。结合我想做的东西，<strong>根据不同的输入图片选择不同的block</strong>，每个block的输出为一个num_classes的分布，每个元素代表某个类激活这个block概率，利用这个概率进行二项式分布的采样。</p></blockquote><h3 id="3️⃣-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION"><a href="#3️⃣-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION" class="headerlink" title="3️⃣ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION"></a>3️⃣ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION</h3><p>本文提出了两个压缩方法：1.<em> quantized distillation</em>：利用蒸馏训练权重是量化的小网络。 2. <em>differentiable quantization</em>：通过梯度下降优化量化点的位置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION&quot;&gt;&lt;a href=&quot;#1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATI
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Revisiting the Importance of Individual Units in CNNs via Ablation</title>
    <link href="http://yoursite.com/2019/05/12/Revisiting%20the%20Importance%20of%20Individual%20Units%20in%20CNNs%20via%20Ablation/"/>
    <id>http://yoursite.com/2019/05/12/Revisiting the Importance of Individual Units in CNNs via Ablation/</id>
    <published>2019-05-12T05:51:59.000Z</published>
    <updated>2019-05-19T09:48:53.400Z</updated>
    
    <content type="html"><![CDATA[<p>之前的一些工作通过可视化每个神经元的方式来理解神经网络，它们选择的是<em>high selectivity</em>的神经元，发现网络浅层识别的是具体的图案（e.g 纹理、图像），网络深层识别的是语义信息（e.g 狗头、车轮），论文[11]似乎打脸了这种方式，表明对于代表整体分类精度，<em>class selectivity</em>属性不能用来预测神经元的重要性。</p><p>本文表明这两种方式都是合理的。用<em>class selectivity</em>或其他属性来预测神经元的重要性从整体分类精度（网络的泛化性）上来看确实不好，但是能作为具体类别的判断依据。</p><p><strong>抑制神经元的方式</strong>：将其weight和bias设置成0。</p><p><strong>两种精度下降类型：</strong>overall accuracy drop &amp; max class accuracy drop</p><p><strong>判断神经元重要性的属性：</strong></p><ul><li><p>L1 Norm：<br>$$<br>\operatorname{norm}_{1}(i)=\left|w_{i}\right|_{1}=\sum_{j}\left|\left(w_{i}\right)_{j}\right|<br>$$</p></li><li><p>Class Correlation：<br>$$<br>\operatorname{corr}(i, k)=\frac{E\left[\left(x_{i}-\overline{x}_{i}\right)\left(p_{k}-\overline{p}_{k}\right)\right]}{\sigma_{x_{i}} \sigma_{p_{k}}}<br>$$</p></li><li><p>Class Selectivity：</p></li></ul><p>$$<br>\operatorname{select}(i, k)=\frac{\overline{x}_{i}^{k}-\overline{x}_{i}^{-k}}{\overline{x}_{i}^{k}+\overline{x}_{i}^{-k}}<br>$$</p><p>​        其中$\overline{x}_{i}^{k}$表示神经元$i$属于kth类别的平均激活值，$\overline{x}_{i}^{-k}$表示神经元$i$属于non-kth类别的平均激活值的均值。这个值的范围是[0, 1]，0表示一个神经元的平均激活值与其他类别都相同，1表示一个神经元只对某个类别的输入有反应。</p><ul><li>Concept Alighment：IoU between unit activation and gt concepts</li><li>Unit Visualization</li></ul><p>🔺 <strong>实验一：</strong>验证抑制单个神经元/一组神经元对两种精度下降类型的影响。</p><ul><li>实验方式：<ul><li>抑制某个神经元，横轴表示CLass，纵轴表示Class Accuracy Drop。</li><li>针对特定的网络层，根据Mac Class Accuracy Drop进行排序，绘制三条曲线（Overal Accuracy Drop / Max Class Accuracy Drop / Min Class Accuracy Drop）。</li><li>利用greedy的方式迭代地移除降低特定类准确率最多的神经元。绘制了特定类别精度下降的曲线，和所有类别平均精度下降的曲线。同时用random作为baseline。</li></ul></li><li>结论：<ul><li>抑制单个神经元对某些类别的分类精度影响很大，但对总体的精度影响不大，并且能通过可视化的形式看出这些抑制的神经元确实展现出了相应类别的特点。</li><li>greedy地抑制一组神经元，能使这个类别的精度大大降低，但是random的方式影响不大。</li></ul></li></ul><p>🔺 <strong>实验二：</strong>验证不同属性与精度下降之间的关系。</p><ul><li>实验方式：<ul><li>用斯皮尔曼相关系数和P值统计了不同属性值与精度下降之间的相关性。</li><li>用不同属性判断出的最重要的那个神经元来预测分类</li></ul></li><li>结论：<ul><li>对于整体精度下降：class selectivity，class correlation和concept alighment表示出正相关，L1是负相关。也就是说，当抑制class selectivity值很大的神经元，对整体网络的精度下降影响较小，与论文[11]中结论一致。</li><li>对于最大类别精度下降：每个属性基本都表现出负相关。说明抑制这些属性值大的神经元，对特定类别精度影响很大。</li><li>Concept Alignment似乎最能代表神经元的重要性</li></ul></li></ul><p>🔺 <strong>实验三：</strong>验证选择的神经元与其方向相关，而不是随机方向。</p><p>🔺 <strong>实验四：</strong>验证BN和Dropout的影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的一些工作通过可视化每个神经元的方式来理解神经网络，它们选择的是&lt;em&gt;high selectivity&lt;/em&gt;的神经元，发现网络浅层识别的是具体的图案（e.g 纹理、图像），网络深层识别的是语义信息（e.g 狗头、车轮），论文[11]似乎打脸了这种方式，表明对于代表
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.01</title>
    <link href="http://yoursite.com/2019/05/11/weekly-paper-01/"/>
    <id>http://yoursite.com/2019/05/11/weekly-paper-01/</id>
    <published>2019-05-10T23:57:11.000Z</published>
    <updated>2019-05-12T09:01:21.046Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019"><a href="#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019" class="headerlink" title="1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)"></a>1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)</h3><p>基于度量的一种剪枝方法。度量的是权重之间的连接敏感度。优点在于不需要layer-by-layer的训练过程。</p><p>砍掉某个连接$j$对loss的影响：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D})=L(\mathbf{1} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{1}-\mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)<br>$$<br>但是上式不可导，作者用$g_j$来代表评估指标，将离散的$e_j$松弛为连续值$\delta e_j$：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D}) \approx g_{j}(\mathbf{w} ; \mathcal{D})=\left.\frac{\partial L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})}{\partial c_{j}}\right|_{\mathbf{c}=1}=\lim _{\delta \rightarrow 0}\left.\frac{L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{c}-\delta \mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)}{\delta}\right|_{\mathbf{c}=1}<br>$$<br>定义了连接敏感度：<br>$$<br>s_{j}=\frac{\left|g_{j}(\mathbf{w} ; \mathcal{D})\right|}{\sum_{k=1}^{m}\left|g_{k}(\mathbf{w} ; \mathcal{D})\right|}<br>$$<br>算法过程：</p><p><img src="https://i.loli.net/2019/05/09/5cd3a33306172.png" alt=""></p><blockquote><p>连接敏感度的计算方式简单，就像是在权重上加了一些噪音，看一下它对loss的影响。但是它的算法过程是对随机初始化的网络权重进行剪枝，再训练q。这样的剪枝是否有意义？</p></blockquote><h3 id="2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks"><a href="#2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks" class="headerlink" title="2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks"></a>2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks</h3><p>📍<a href="https://github.com/anibali/dsntnn" target="_blank" rel="external">https://github.com/anibali/dsntnn</a></p><p>本文提出了一个空间可微的数值转换操作(DSNT)来找到输入图像的显著点坐标。</p><p>传统方法有<strong>Heatmap matching</strong>和<strong>Fully connected</strong>，代表性的运用是<em>Human pose estimation</em>和<em>STN</em>。前者不完全可微，后者缺乏空间泛化能力。</p><p><img src="https://i.loli.net/2019/05/07/5cd10b71062f2.png" alt=""></p><p>DSNT的输入是一个单通道归一化的大小为$m \times n$ 的 heatmap $\hat{Z}$，代表了概率分布。输出是显著点的坐标，即概率分布最大点的那个坐标值。用两个相同大小的矩阵$X$和$Y$分别代表$x-$和$y-$坐标，让坐标分布变为左上角是(-1, -1)，右下角是(1,1)。</p><p>我们可以通过坐标$c$计算出这个概率函数：<br>$$<br>\operatorname{Pr}\left(\mathbf{c}=\left[ \begin{array}{ll}{X_{i, j}} &amp; {Y_{i, j}}\end{array}\right]\right)=\hat{Z}_{i, j}<br>$$<br>传统方法利用的是$c$的模，DSNT利用的是$c$的期望$\boldsymbol{\mu}=\mathbb{E}[\mathbf{c}]$，可以用如下公式表示：<br>$$<br>\operatorname{DSNT}(\hat{Z})=\boldsymbol{\mu}=\left[\langle\hat{Z}, \boldsymbol{X}\rangle_{F}<br>\quad\langle\hat{\boldsymbol{Z}}, \boldsymbol{Y}\rangle_{F}\right]<br>$$</p><blockquote><p>The “mode” is the value that occurs most often. The “mean” is the “average” you’re used to, where you add up all the numbers and then divide by the number of numbers.</p></blockquote><p>一个直观的例子：</p><p><img src="https://i.loli.net/2019/05/07/5cd1422432340.png" alt=""></p><p>DSNT的损失函数由两个部分组成：<br>$$<br>\mathcal{L}(\hat{Z}, \boldsymbol{p})=\mathcal{L}_{e u c}(\operatorname{DSNT}(\hat{Z}), \boldsymbol{p})+\lambda \mathcal{L}_{r e g}(\hat{Z})<br>$$<br>第一部分直接计算预测坐标和gt的欧式距离：<br>$$<br>\mathcal{L}_{e u c}(\boldsymbol{\mu}, \boldsymbol{p})=|\boldsymbol{p}-\boldsymbol{\mu}|_{2}<br>$$<br>第二部分是正则项，文章对比了两种正则方法：</p><ol><li><p>方差正则：<br>$$<br>\begin{aligned} \operatorname{Var}\left[\mathrm{c}_{x}\right] &amp;=\mathbb{E}\left[\left(\mathrm{c}_{x}-\mathbb{E}\left[\mathrm{c}_{x}\right]\right)^{2}\right] \\ &amp;=\left\langle\hat{Z},\left(\boldsymbol{X}-\mu_{x}\right) \odot\left(\boldsymbol{X}-\mu_{x}\right)\right\rangle_{F} \end{aligned}<br>$$</p><p>$$<br>\mathcal{L}_{v a r}(\hat{Z})=\left(\operatorname{Var}\left[c_{x}\right]-\sigma_{t}^{2}\right)^{2}+\left(\operatorname{Var}\left[\mathrm{c}_{y}\right]-\sigma_{t}^{2}\right)^{2}<br>$$</p></li><li><p>分布正则（KL散度/JS散度）：<br>$$<br>\mathcal{L}_{D}(\hat{Z}, \boldsymbol{p})=D\left(p(\mathbf{c}) | \mathcal{N}\left(\boldsymbol{p}, \sigma_{t}^{2} \boldsymbol{I}_{2}\right)\right)<br>$$</p></li></ol><h3 id="3️⃣-Searching-for-MobileNetV3"><a href="#3️⃣-Searching-for-MobileNetV3" class="headerlink" title="3️⃣ Searching for MobileNetV3"></a>3️⃣ Searching for MobileNetV3</h3><ul><li><p>用搜索的方式搜索网络结构</p><ul><li>Block-wise：用MnasNet-A1作为初始大网络，修改了反馈$A C C(m) \times[L A T(m) / T A R]^{w}$</li><li>Layer-wise：NetAdapt的剪枝方式，修改了评估指标$\frac{\Delta \mathrm{Acc}}{|\Delta \mathrm{latency}|}$</li></ul></li><li><p>加入了SEblock，重新设计了网络的输出层</p><p><img src="https://i.loli.net/2019/05/09/5cd39f284e1fb.png" alt=""></p><p><img src="https://i.loli.net/2019/05/09/5cd39f84c5807.png" alt=""></p></li><li><p>基于swish提出了h-swish替代ReLU<br>$$<br>\mathrm{h}-\operatorname{swish}[x]=x \frac{\operatorname{Re} \mathrm{LU} 6(x+3)}{6}<br>$$</p></li><li><p>在语义分割任务上基于R-ASPP，提出了LR-ASSP。</p><p><img src="https://i.loli.net/2019/05/09/5cd3a1b27ccf1.png" alt=""></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019&quot;&gt;&lt;a href=&quot;#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>namesilo域名购买 &amp; github pages域名设置</title>
    <link href="http://yoursite.com/2019/05/10/set-blog-domain/"/>
    <id>http://yoursite.com/2019/05/10/set-blog-domain/</id>
    <published>2019-05-10T15:42:05.000Z</published>
    <updated>2019-05-12T10:56:08.729Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-域名购买和设置"><a href="#1-域名购买和设置" class="headerlink" title="1. 域名购买和设置"></a>1. 域名购买和设置</h3><p>首先在<a href="http://xn--eqrt2g.xn--vuq861b/#" target="_blank" rel="external">工信部公示</a>查看可备案的网站后缀，在<a href="https://www.namesilo.com/" target="_blank" rel="external">namesilo</a>上购买喜欢的域名，可以使用支付宝付款。</p><ul><li>进入「域名管理」，点击刚申请到的域名</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd613b55c18e.png" alt=""></p><ul><li>更新「DNS记录」</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd612451ffa6.png" alt=""></p><p>在其中添加以下三条记录：</p><p><img src="https://i.loli.net/2019/05/11/5cd612ba05f60.png" alt=""></p><blockquote><p>A记录类型在<a href="https://help.github.com/en/articles/troubleshooting-custom-domains#dns-configuration-errors" target="_blank" rel="external">这里</a>查看Github的地址，最后一行CNAME是自己的<code>usernmae.github.io</code>，实际上A记录和CNAME指向的是相同的IP地址，设置一项即可。</p></blockquote><h3 id="2-修改域名DNS服务器"><a href="#2-修改域名DNS服务器" class="headerlink" title="2. 修改域名DNS服务器"></a>2. 修改域名DNS服务器</h3><ul><li>在namesilo上将域名服务商修改为<a href="https://www.dnspod.cn/console/dns" target="_blank" rel="external">DNSpod</a></li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6152a1859a.png" alt=""></p><blockquote><p>DNSpod提供了两个免费的DNS地址：f1g1ns1.dnspod.net / f1g1ns2.dnspod.net</p></blockquote><ul><li>登陆DNSpod，添加域名和记录</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6160b109d2.png" alt=""></p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190511082546709.png" alt="image-20190511082546709"></p><h3 id="3-更新Github上的自定义域名"><a href="#3-更新Github上的自定义域名" class="headerlink" title="3. 更新Github上的自定义域名"></a>3. 更新Github上的自定义域名</h3><p>进入本地存储博客的文件夹</p><p><img src="https://i.loli.net/2019/05/11/5cd6186e83770.png" alt=""></p><p>创建一个没有后缀的<code>CNAME</code>文件，输入购买的域名。</p><p><img src="https://i.loli.net/2019/05/11/5cd6190c11dab.png" alt=""></p><p>部署到github上，就可以利用自定义的域名访问博客啦～</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</div></pre></td></tr></table></figure><p><strong>参考链接：</strong></p><ul><li><a href="http://cps.ninja/2016/10/09/customize-your-blog-domain/" target="_blank" rel="external">http://cps.ninja/2016/10/09/customize-your-blog-domain/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-域名购买和设置&quot;&gt;&lt;a href=&quot;#1-域名购买和设置&quot; class=&quot;headerlink&quot; title=&quot;1. 域名购买和设置&quot;&gt;&lt;/a&gt;1. 域名购买和设置&lt;/h3&gt;&lt;p&gt;首先在&lt;a href=&quot;http://xn--eqrt2g.xn--vuq861
      
    
    </summary>
    
    
      <category term="指南" scheme="http://yoursite.com/tags/%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>沿着某个维度进行操作</title>
    <link href="http://yoursite.com/2019/05/10/along-axis/"/>
    <id>http://yoursite.com/2019/05/10/along-axis/</id>
    <published>2019-05-10T04:40:24.000Z</published>
    <updated>2019-05-10T04:57:27.083Z</updated>
    
    <content type="html"><![CDATA[<p>一直对沿着axis进行操作不太理解，今天决定还是要搞明白。</p><p>比如<code>np.sum(a, aixs=1)</code>，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动<code>a[0][0],a[0][1], a[0][2]​</code>。<br><img src="https://pic3.zhimg.com/80/v2-7a0716230a6f3d4840a6098001b1d2a2_hd.jpg" alt="img"></p><p>于是将红色框的元素相加得到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],</div><div class="line">[<span class="number">9</span>],</div><div class="line">[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>如果是<code>np.sum(a, axis=0)</code>，即<code>a[0][0],a[1][0], a[2][0]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],[<span class="number">9</span>],[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://zhuanlan.zhihu.com/p/31275071" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/31275071</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直对沿着axis进行操作不太理解，今天决定还是要搞明白。&lt;/p&gt;
&lt;p&gt;比如&lt;code&gt;np.sum(a, aixs=1)&lt;/code&gt;，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动&lt;code&gt;a[0][0],a[0][1], a[0][2]​&lt;/code
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令 - 显示文件指定行</title>
    <link href="http://yoursite.com/2019/05/05/linux-cat/"/>
    <id>http://yoursite.com/2019/05/05/linux-cat/</id>
    <published>2019-05-05T01:42:07.000Z</published>
    <updated>2019-05-12T05:29:04.159Z</updated>
    
    <content type="html"><![CDATA[<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">head -n [行数a] # 显示开始a行</div><div class="line">tail -n [行数a] # 显示最后a行</div><div class="line">tail -n +[行数a] # 从a行以后开始显示</div></pre></td></tr></table></figure><p>单独使用 <code>tail</code> 和 <code>head</code> 比较简单，组合使用时它们的顺序是有讲究的。</p><p>从200行开始显示10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | tail -n +200 | head -n 10</div></pre></td></tr></table></figure><p>等同于显示200行到210行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | head -n 210 | tail -n +200</div></pre></td></tr></table></figure><h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><p>还有一种使用<code>sed</code>命令的方式，从200行开始现实10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sed -n <span class="string">'200, 210p'</span> [filename]</div></pre></td></tr></table></figure><p>参考链接：</p><ul><li><a href="https://blog.csdn.net/vitaminc4/article/details/78136696" target="_blank" rel="external">https://blog.csdn.net/vitaminc4/article/details/78136696</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;cat&quot;&gt;&lt;a href=&quot;#cat&quot; class=&quot;headerlink&quot; title=&quot;cat&quot;&gt;&lt;/a&gt;cat&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;d
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/08/06/adversiarial-compression/"/>
    <id>http://yoursite.com/2018/08/06/adversiarial-compression/</id>
    <published>2018-08-06T02:13:32.000Z</published>
    <updated>2018-08-17T07:36:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对抗网络压缩"><a href="#对抗网络压缩" class="headerlink" title="对抗网络压缩"></a>对抗网络压缩</h2><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrfwrb0vj314e0fwq4u.jpg" alt="network_architecture"></p><p>对抗网络压缩过程是在老师和学生之间进行博弈，判别器的任务是判别输入样本是来自老师or学生。老师网络事先用标签训练完成，在对抗压缩的过程中不更新。</p><h3 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h3><ol><li>D接受两个网络最后一层的特征图$f^k_{t}(x)$和$f^l_s(x)$作为输入，利用交叉熵进行real/fake判别（蓝色线）</li><li>利用学生和老师的logits计算L2损失，引导学生模仿老师，产生输出（绿色线）</li><li>D接受学生网络dropout后的特征作为输入（红色线），欺骗D将其判别为real。</li></ol><h3 id="目标函数："><a href="#目标函数：" class="headerlink" title="目标函数："></a>目标函数：</h3><p>借鉴cGAN的思想，定义对抗压缩的损失函数为：</p><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrynfkouj316e04cdgr.jpg" alt=""></p><p>由于利用的是特征图作为D的输入，学生网络中还有许多参数需要更新，因此需要加入一个损失函数，减少老师网络和学生网络输出数据的差异：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftzs7ncxe2j315o03c0t5.jpg" alt=""></p><p>最后再加上正则项，得出最终的优化目标：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftzs779uxaj315i03m0t9.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;对抗网络压缩&quot;&gt;&lt;a href=&quot;#对抗网络压缩&quot; class=&quot;headerlink&quot; title=&quot;对抗网络压缩&quot;&gt;&lt;/a&gt;对抗网络压缩&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/0069RVTdly1ftzr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/07/29/compression_models/"/>
    <id>http://yoursite.com/2018/07/29/compression_models/</id>
    <published>2018-07-29T06:39:32.000Z</published>
    <updated>2018-11-12T08:00:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟</p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><p>论文：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">Aggregated Residual Transformations for Deep Neural Networks</a></p><p>ResNeXt提出了聚集变换(aggregated transofrmations)：<br>$$<br>\mathcal { F } ( \mathbf { x } ) = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } )<br>$$<br>$C$为基数(cardinality)，表明进行聚集变换的集合大小。$\mathcal { T } _ { i } ( \mathbf { x } )$代表一系列变换。</p><p><strong>ResNext与ResNet</strong></p><p>ResNet block：$\mathbf { y } = \mathcal { F } \left( \mathbf { x } , \left\{ W _ { i } \right\} \right) + \mathbf { x }$</p><p>ResNeXt block：$\mathbf { y } = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } ) + \mathbf { x }$</p><p>在计算代价相同的情况下，画出来就是下图的东东（左边的代表ResNet block，右边代表ResNeXt block）：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftrwnc8rz2j30t60cuq4s.jpg" alt="resnet_resnext"></p><p>ResNeXt将ResNet的一个大变换，分解为一系列小变换，然后将结果聚集起来（相加）。两个结构都做了跳跃连接。</p><p><strong>ResNeXt、Inception、Group Convolution</strong></p><p>文章还比较了与Inception组件和组卷积的关系，如下图所示：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftrwicz2c8j31f80fs0wu.jpg" alt=""></p><p>文章在后续的实验中表明，在计算代价近似的情况下，基数$C$越多，准确率越高。</p><h2 id="clcNet"><a href="#clcNet" class="headerlink" title="clcNet"></a>clcNet</h2><p>论文：<a href="https://arxiv.org/abs/1712.06145" target="_blank" rel="external">clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions</a></p><p>本文将深度卷积和组卷积归为一种通用卷积操作——通道局部卷积(channel local convolution, CLC)，并提出了通道依赖图(channel dependency graph, CDG)的表示方法。下图表示了常规卷积(a)、组卷积(b)、深度卷积(c)的CDG。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftry0ix2cwj30vm0b8q4u.jpg" alt=""></p><blockquote><p>CDG这个概念之前看的一篇文章中也提到了：<a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="external">Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast</a>，很直观地对比了ResNet、ResNeXt、MobileNet以及ShuffleNet的组件。</p></blockquote><p>通过CDG可以很明显的看出通道之间的依赖关系（箭头由输出指向输入）。这个依赖关系可以用通道感知域(channel receptive field, CRF)来表示。CRF代表每个输出通道所依赖的输入通道，CRF大小为每个输出通道依赖的输入通道数量。</p><p>分析上图三种卷积操作的CRF大小（size）：(a) $size = 6$，(b) $size =6/3 = 2$，(c) $size=1$</p><p>在这些概念的基础上，作者提出了<strong>全通道感知域(full channel receptive field, FCRF)</strong>，FCRF意味着_CRF大小 = 输入通道数_，即每个输出通道都依赖于所有的输入通道。作者表明FCRF能够更有效地表示特征，并且在实验中能获得更高的准确率。</p><p>作者还提出了交叉组卷积(interlaced group convolution, IGC)，如下图(b)所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt3uwgee8j30vq0ikac6.jpg" alt="igc"></p><p>与组卷积(a)进行比较，可以看出filed数量即为每组的通道数。IGC实际上就是将每组的输出通道，分到不同的field里去。因此交叉组卷积的CRF大小和组卷积是相同的。</p><p><strong>CLC block</strong>是本文提出的组件，由3x3的IGC和1x1的GC构成，组件结构如下图所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt48c72dkj30x80hqq54.jpg" alt="clcblock"></p><p>在CLC block的基础上，作者探索了FCRF的规则，假设IGC的分组数为$g_1$，$M、L、N$分别为输入、中间阶段、输出的通道数，GC的分组数为$g_2$，要达到FCRF，应满足以下条件：<br>$$<br>L/g_2 \geq  g_1  \quad or  \quad  g _ { 1 } g _ { 2 } \leq L<br>$$<br>基于FCRF规则，使得计算代价最小：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftt4m3cte8j30x808y3zy.jpg" alt="computational_cost"></p><h2 id="ShuffleNet-v2"><a href="#ShuffleNet-v2" class="headerlink" title="ShuffleNet v2"></a>ShuffleNet v2</h2><p>本文指出模型的运算速度不单单取决于每秒浮点运算次数（FLOPS），还有其他的考虑因素，比如内存访问代价（MAC）、运算平台等。通过一系列的控制实验，对于模型的设计提出了4个指导方针：</p><ol><li>相同的输入输出通道宽度，能减少内存访问代价。</li><li>分组过大，会增加内存访问代价。</li><li>网络分割会降低并行度。</li><li>不能忽略逐元素操作。</li></ol><p>基于指导方针，设计了新的ShuffleNet单元：</p><p>针对输入输出相同的单元，在一开始加入了通道分割（Channel Split），取消了组卷积和相加操作，以符合提出的指导方针2和4；针对降采样的单元，利用两路相似的操作进行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟&lt;/p&gt;
&lt;h2 id=&quot;SqueezeNet&quot;&gt;&lt;a href=&quot;#SqueezeNet&quot; class=&quot;headerlink&quot; title=&quot;Sq
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Network compression, speedup</title>
    <link href="http://yoursite.com/2018/07/27/Network-compression-speedup/"/>
    <id>http://yoursite.com/2018/07/27/Network-compression-speedup/</id>
    <published>2018-07-27T03:19:32.000Z</published>
    <updated>2018-07-27T03:30:13.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h2><h3 id="Singular-value-Decomposition-SVD"><a href="#Singular-value-Decomposition-SVD" class="headerlink" title="Singular value Decomposition(SVD)"></a>Singular value Decomposition(SVD)</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto95l6e4aj30w60cajt9.jpg" alt=""><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto96k0qcsj30uy04e755.jpg" alt=""></p><h3 id="Flattened-Convolutions"><a href="#Flattened-Convolutions" class="headerlink" title="Flattened Convolutions"></a>Flattened Convolutions</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto96skp45j30vo0aoac6.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto98lg9m4j30w00asac3.jpg" alt=""></p><h2 id="Weight-Pruning"><a href="#Weight-Pruning" class="headerlink" title="Weight Pruning"></a>Weight Pruning</h2><h3 id="Magnitude-based-method"><a href="#Magnitude-based-method" class="headerlink" title="Magnitude-based method"></a>Magnitude-based method</h3><ul><li>Iterative Pruning + Retraining<br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99b7u7hj30x8084dh3.jpg" alt=""></li><li>Dynamic Network Surgery<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto99ja0soj30vm06ogml.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto99q0hilj30wi0cyq5s.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99wbecrj30wm082dh2.jpg" alt=""></li></ul><h3 id="Hessian-based-method"><a href="#Hessian-based-method" class="headerlink" title="Hessian-based method"></a>Hessian-based method</h3><ul><li>Diagonal Hessian-based method: Optimal Brain Damage<br>💫  . Motivation<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9a3vy26j30yy04s3zc.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9a9srdtj310s0dqac9.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ae9ircj30zo0d040y.jpg" alt=""></li><li>Full Hessian-based method: Optimal Brain Surgeon<br>💫  . Motivation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9ajoj63j310s0d4q5t.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9aof4apj310s0dutbh.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9atm2jzj311c0aiwgd.jpg" alt=""></li></ul><h2 id="Quantization-method"><a href="#Quantization-method" class="headerlink" title="Quantization method"></a>Quantization method</h2><h3 id="Full-Quantization"><a href="#Full-Quantization" class="headerlink" title="Full Quantization"></a>Full Quantization</h3><ul><li>Fixed-point format<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ay9ggsj310o0b4tbh.jpg" alt=""><br>⛓ .  Rounding Modes<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9b5dkn2j310u0c6taq.jpg" alt=""><br>🍄 . Multiply and accumulate (MACC) operation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9b9p5z4j30zs0ceq5c.jpg" alt=""></li><li>Code book<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9begz5mj30zc0feq7k.jpg" alt=""></li></ul><h3 id="Quantization-with-full-precision-copy"><a href="#Quantization-with-full-precision-copy" class="headerlink" title="Quantization with full-precision copy"></a>Quantization with full-precision copy</h3><ul><li>Binnaryconnect<br>💫  . Motivation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9c757zjj31000ag0ue.jpg" alt=""><br>⛓ .  Binarization<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9ccdumnj31080eetbw.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9cijm0jj30zo07o3zt.jpg" alt=""><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9cmyf5sj311i082gnq.jpg" alt=""></li><li>Binarized Neural Network (BNN)<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9css9thj3114066gn9.jpg" alt=""><br>⛓ .  Method<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9cxe89pj311k0ecgoo.jpg" alt=""></li></ul><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Matrix-Factorization&quot;&gt;&lt;a href=&quot;#Matrix-Factorization&quot; class=&quot;headerlink&quot; title=&quot;Matrix Factorization&quot;&gt;&lt;/a&gt;Matrix Factorization&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
</feed>
