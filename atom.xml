<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-23T08:44:01.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>卷积神经网络学习笔记</title>
    <link href="http://yoursite.com/2018/01/23/cnn/"/>
    <id>http://yoursite.com/2018/01/23/cnn/</id>
    <published>2018-01-23T06:53:12.000Z</published>
    <updated>2018-01-23T08:44:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络和普通的神经网络很相似，但是明确表示了输入是图像，并且允许我们编码一些特征，因此就使得提升了网络前向传播的效率，并且大大减少了网络中的参数。下面就让我们一起瞅瞅CNN加入了哪些神奇的东西吧 :)</p><a id="more"></a><blockquote><p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p></blockquote><p>上面这句话透露了很多有意思的东西，包括<strong>3D volume</strong>、<strong>Layers</strong>、<strong>Parameters</strong>，让我们一个个来说说～</p><h3 id="3D-volume"><a href="#3D-volume" class="headerlink" title="3D volume"></a>3D volume</h3><p>普通的神经网络将输入图像排列成一个向量，通过和对应数量的参数点乘获得预测结果：</p><p><img src="/2018/01/23/cnn/fc_layer.png" alt=""></p><p>但CNN保留了输入图像的空间结构，将其看作是3维（<strong>width, height, depth</strong>）结构的，并利用较少的参数过滤映射出下一层：</p><p><img src="/2018/01/23/cnn/cn_layer.png" alt=""></p><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>我们主要使用三种类型的层来构建卷积神经网络，分别是：<strong>卷积层（Convolutional Layer）</strong>、<strong>汇聚层（ Pooling Layer）</strong>、和<strong>全连接层（Fully-Connected Layer）</strong></p><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>卷积层是构建卷积神经网络的核心层。卷积层的参数是由一些可学习的滤波器集合（参数）构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。</p><p><strong>局部连接</strong>：我们已经知道全连接需要与对应数量的参数点乘，但这对于大尺寸的图像来说运算量是非常大的。因此我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的<strong>感受野（receptive field）</strong>。它的尺寸其实就是滤波器的空间尺寸。</p><p>❗️我们让在滤波器在宽度和高度上滑动，但总是让滤波器的深度=输入数据深度。</p><p><strong>空间排列</strong>：有了输入和参数，我们应该就可以得到输出了。但这里还要提一下3个控制输出数据尺寸的超参数：<strong>深度（depth），步长（stride）</strong>和<strong>零填充（zero-padding）</strong>。</p><ol><li><p>此深度非彼深度。上面我们提到了输入数据的深度，现在这个输出数据的深度其实是使用的<strong>滤波器数量</strong>。</p><p><img src="/2018/01/23/cnn/maps.png" alt=""></p></li><li><p>在滑动滤波器的时候我们要指定每次移动多少个像素，也就是步长。</p><p>假设使用3x3滤波器在[7x7]的输入数据上滑动 =&gt; [5x5]的输出数据，而使用2个步长滑动，会让输出数据在空间上变小 =&gt; [3x3]的输出数据</p></li><li><p>有时候会用0在输入数据体的边缘处进行填充，可以让输出数据与输入数据空间保持一致。</p><p><img src="/2018/01/23/cnn/zero_padding.png" alt=""></p></li></ol><p>我们就可以通过输入数据尺寸\(W_1\timeslH_1\timesD1)\\、滤波器尺寸（F）、步长（S）和零填充的数量（P），计算出输出数据的空间尺寸：(W-F+2P)/S+1</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;卷积神经网络和普通的神经网络很相似，但是明确表示了输入是图像，并且允许我们编码一些特征，因此就使得提升了网络前向传播的效率，并且大大减少了网络中的参数。下面就让我们一起瞅瞅CNN加入了哪些神奇的东西吧 :)&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络方法小结</title>
    <link href="http://yoursite.com/2018/01/21/neural-nets-note/"/>
    <id>http://yoursite.com/2018/01/21/neural-nets-note/</id>
    <published>2018-01-21T03:02:06.000Z</published>
    <updated>2018-01-23T06:50:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。</p><a id="more"></a><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>机器学习本质上是数据工程，数据的分布对于算法学习过程是有影响的。所以在整个学习开始前，需要对<strong>原始数据</strong>进行预处理。通常的方法有：</p><ul><li><p><strong>均值减法（Mean subtraction）</strong>：使数据零均值化<code> X -= np.mean(X)</code></p></li><li><p><strong>归一化（Normalization）</strong>：使数据范围近似相等<code>X /= np.std(X, axis=0)</code></p><p><img src="/2018/01/21/neural-nets-note/data_preprocessing1.jpeg" alt=""></p></li><li><p><strong>主成分分析（Principal Component Analysis，PCA）</strong>：降低数据维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></div><div class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></div><div class="line">U,S,V = np.linalg.svd(cov) <span class="comment"># 奇异值分解</span></div><div class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></div><div class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># 降维</span></div></pre></td></tr></table></figure></li><li><p><strong>白化（Whitening）</strong>：降低数据的冗余度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 除以特征值 </span></div><div class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/data_preprocessing2.jpeg" alt=""></p></li></ul><p>❗️在进行预处理的过程中，对所有数据统一处理后再划分训练集/验证集/测试集的做法是<strong>错误的</strong>。正确做法是：先将数据划分为训练集/验证集/测试集，对<strong>训练集</strong>进行操作后，将其运用于验证集和测试集。</p><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>除了数据X，神经网络还有一些很重要的数值就是<strong>权重W</strong>和<strong>偏置b</strong>。</p><p><strong>初始化权重</strong>有以下几种方法：</p><ul><li><p><strong>小随机数初始化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</div></pre></td></tr></table></figure></li><li><p><strong>使用1/sqrt(n)校准方差</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = np.random.randn(n) / sqrt(n)</div><div class="line">W = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n) <span class="comment"># ReLU神经元的特殊初始化</span></div></pre></td></tr></table></figure></li><li><p><strong>稀疏初始化（Sparse initialization）</strong></p></li></ul><p><strong>初始化偏置</strong>：通常初始为全0。</p><h3 id="损失函数（Loss-Functions）"><a href="#损失函数（Loss-Functions）" class="headerlink" title="损失函数（Loss Functions）"></a>损失函数（Loss Functions）</h3><p><strong>数据损失</strong>是一个有监督学习问题，用于衡量分类算法的预测结果和真实标签结果的一致性。我们要计算所有样本的平均数据损失，向减小这个数值的方向不断努力。可以说这就是深度学习的根本目的了。因此也就产生了一系列计算数据损失的损失函数：</p><ul><li><p><strong>Multiclass Support Vector Machine loss（SVM）</strong>：SVM分类器的损失函数想要SVM在正确分类的得分比不正确分类的得分来得高，且至少高出一个边界值𝚫。<br>$$<br>L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)<br>$$<br>这里要提一下<strong>折页损失（hinge loss）</strong>，即\(max(0, —)\)函数，它常用于“maximum-margin”的算法。因为SVM分类器就属于这类算法，所以我们用名称SVM来代表这类损失函数。</p></li><li><p><strong>Softmax</strong>：Softmax分类器将评分值视为每个分类的未归一化的对数概率（\(e^{f_{y_{i}}}\)，把折页损失替换成了<strong>交叉熵损失（cross-entropy loss）</strong>，它所做的就是最小化在估计分类概率和“真实”分布之间的交叉熵：<br>$$<br>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)<br>$$</p></li></ul><p>❗️在实际编程实现Softmax的过程中，中间项 \(e^{f_{y_i}}\) 和 \(\sum_j e^{f_j}\) 因为存在指数函数，数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化的小技巧非常重要。通常将𝐶设为\(\log C = -\max_j f_j\)<br>$$<br>\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}<br>= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}<br>= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}<br>$$<br><img src="/2018/01/21/neural-nets-note/svmvssoftmax.png" alt="svmvssoftmax"></p><h3 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h3><p>除了数据损失部分，正则项也是在损失函数很重要的一部分。如果模型太复杂即出现过拟合（overfitting），我们就需要加入一些正则项来解决这个问题。</p><ul><li><p><strong>L2正则化</strong>：\(R(W) = \sum_k\sum_lW_{k,l}^2\) </p></li><li><p><strong>L1正则化</strong>：\(R(W) = \sum_k\sum_l|W_{k,l}|\) </p></li><li><p><strong>最大范式约束（Max norm constraints）</strong>：给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。</p></li><li><p><strong>随机失活（Dropout）</strong>：让神经元以超参数𝑝的概率被激活或者被设置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></div><div class="line">U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 随机失活遮罩</span></div><div class="line">H1 *= U1 <span class="comment"># drop!</span></div></pre></td></tr></table></figure></li><li><p><strong>批量归一化（Batch Normalization）</strong>：为了使神经网络的每一层输入数据都满足正态分布，除了一开始的数据预处理，科学家们引入了批量归一化。就是在使用每层的激活函数前，对采样的批数据进行归一化处理。</p></li></ul><p>❗️在开始最优化前我们最好做一些<strong>合理性检查</strong>：</p><ul><li>寻找特定情况的正确损失值</li><li>提高正则化强度时导致损失值变大</li><li>对小数据子集过拟合</li></ul><h3 id="最优化（Optimization）"><a href="#最优化（Optimization）" class="headerlink" title="最优化（Optimization）"></a>最优化（Optimization）</h3><p>机器学习的过程不断修改权重W，使Loss能够减小。最常见的方法就是进行<strong>反向传播（Backpropagation）</strong>。通过微分公式和链式法则计算出<strong>解析梯度</strong>，沿着梯度的负方向更新权重。但解析梯度非常容易算错，所以我们可以计算<strong>数值梯度</strong>来进行<strong>梯度检查</strong>，有两种比较方式：</p><ul><li><strong>使用中心化公式</strong></li></ul><p>$$<br>\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in}<br>$$</p><ul><li><strong>使用相对误差</strong><br>$$<br>\frac{\mid f’_a - f’_n \mid}{\max(\mid f’_a \mid, \mid f’_n \mid)}<br>$$</li></ul><p>不断迭代计算梯度并更新权重的最优化方法就是<strong>梯度下降法（Gradient descent）</strong>。迭代的思路主要有两种：</p><ul><li><strong>批量梯度下降（Batch Gradient Descent，BGD）</strong>：每次使用所有数据计算梯度</li><li><strong>随机梯度下降（Stochastic Gradient Descent，SGD）</strong>：每次只使用一个数据计算梯度</li><li><strong>小批量数据梯度下降（Mini-batch gradient descent）</strong>：每次使用一个小批量数据计算</li></ul><p>❗️即使SGD在技术上是指每次使用1个数据来计算梯度，但人们会使用SGD来指代小批量数据梯度下降</p><p>因为深度学习的参数通常处于高维空间，不同的更新方法会导致不同的优化速度。</p><ul><li><p><strong>普通更新</strong>：沿着负梯度方向改变参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure></li><li><p><strong>动量更新</strong>：从物理角度出发，负梯度与质点的加速度是成比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></div><div class="line">x += v <span class="comment"># 与位置融合</span></div></pre></td></tr></table></figure></li><li><p><strong>Nesterov动量</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">v_prev = v <span class="comment"># 存储备份</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/gradientdescent.png" alt="loss"></p><p>​</p></li></ul><p>第二类常用的最优化方法是基于牛顿法的。在这些方法中最流行的是<strong>L-BFGS</strong>。但在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p><p>上面这些更新方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。因此就有人提出了适应性学习率算法：</p><ul><li><p><strong>Adagrad</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></li><li><p><strong>RMSprop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure></li><li><p><strong>Adam</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</div><div class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</div><div class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</div></pre></td></tr></table></figure><p><img src="/2018/01/21/neural-nets-note/opt2.gif" alt="loss"></p></li></ul><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>在训练深度网络的时候，让学习率随着时间衰减通常是有帮助的。</p><ul><li><strong>随步数衰减</strong>：每进行几个周期就根据一些因素降低学习率</li><li><strong>指数衰减</strong>：\(\alpha = \alpha_0 e^{-k t})\\</li><li><strong>1/t衰减</strong>：\(\alpha = \alpha_0 / (1 + k t ))\\</li></ul><h3 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h3><p>整个训练过程我们会遇到很多超参数，掌握一些小技巧对于超参数初的始化和调整是必不可少的。</p><ul><li><strong>超参数范围</strong>：在对数尺度上进行超参数搜索。一个典型的学习率应该看起来是这样：<strong>learning_rate = 10 \</strong> uniform(-6, 1)<strong>。对于正则化强度，可以采用同样的策略。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：</strong>dropout=uniform(0,1)**）。</li><li><strong>随机搜索优于网格搜索</strong>：通常，有些超参数比其余的更重要。通过随机搜索，可以更精确地发现那些比较重要的超参数的好数值。</li><li><strong>小心边界上的最优值</strong>：假设我们使用<strong>learning_rate = 10 \</strong> uniform(-6,1)**来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</li><li><strong>从粗到细地分阶段搜索</strong>：在实践中，先进行初略范围搜索，让模型训练一个周期就可以了；第二个阶段就是根据好结果出现的地方，缩小范围进行搜索，这时可以让模型运行5个周期；最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</li><li><strong>贝叶斯超参数最优化</strong>：主要是研究在超参数空间中更高效的导航算法。</li></ul><h3 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h3><p>可以通过可视化的方式来检查我们的损失值、精确值等是否符合预期。</p><ul><li><p><strong>损失值</strong>：损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时，震荡就会最小。</p><p><img src="/2018/01/21/neural-nets-note/loss.png" alt="loss"></p><p>​</p></li><li><p><strong>训练集和测试集的准确率</strong>：在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度</p><p><img src="/2018/01/21/neural-nets-note/accuracies.jpeg" alt="accuracies"> </p></li><li><p><strong>权重更新比例</strong>：权重中更新值的数量和全部值的数量之间的比例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></div><div class="line">param_scale = np.linalg.norm(W.ravel())</div><div class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></div><div class="line">update_scale = np.linalg.norm(update.ravel())</div><div class="line">W += update <span class="comment"># 实际更新</span></div><div class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></div></pre></td></tr></table></figure></li><li><p><strong>每层的激活数据及梯度分布</strong>：可以输出网络中所有层的激活数据和梯度分布的柱状图。如果看到任何奇怪的分布情况，那都不是好兆头。</p></li><li><p><strong>权重可视化</strong>：如果数据是图像像素数据，可以对第一层权重进行可视化，观察特征分布。</p><p><img src="/2018/01/21/neural-nets-note/firstlayer.png" alt="accuracies"></p></li></ul><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>有时候单个模型并不能满足需求，可以进行模型集成来获得额外的性能提高。进行集成有以下几种方法：</p><ul><li><strong>同一个模型，不同的初始化</strong>：使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。</li><li><strong>在交叉验证中发现最好的模型</strong>：使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。</li><li><strong>一个模型设置多个记录点</strong>：如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。</li><li><strong>在训练的时候跑参数的平均值</strong>：在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。</li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li><p><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-2/</a></p></li><li><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a></p></li><li><p><a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">Loss function</a></p><p>​</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在cs231n上学习了关于神经网络一系列数据处理、参数训练、结果分析的方法，为加深印象做一些整理。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>训练全连接网络</title>
    <link href="http://yoursite.com/2018/01/19/FullyConnectedNets-train/"/>
    <id>http://yoursite.com/2018/01/19/FullyConnectedNets-train/</id>
    <published>2018-01-19T03:08:25.000Z</published>
    <updated>2018-01-21T03:06:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习的训练过程不是盲目的，应该掌握一定的技巧和方法。参考<a href="https://www.reddit.com/r/cs231n/comments/443y2g/hints_for_a2/" target="_blank" rel="external">Hints for A2</a>和<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf" target="_blank" rel="external">Lecture</a>记录一下FullyConnectedNets的训练过程。在开始训练之前要先<strong>预处理数据</strong>和<strong>选择网络结构</strong>，然后我们就可以开始训(tiao)练(can)啦～</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>首先我们要先检测一下_loss_是不是合理的，例如十个分类，loss就应该是-np.log(0.1) = 2.3左右。</p><p>然后我们需要确保在最初的几次迭代中_loss_是下降的，在这里_regularization_(L2/drop out)可以扔掉，_learning rate decay_也可以不设置。我们尝试不同的_weigtht scale_和_learning_rate_，将两者定位在一个大致范围。那么先用较小的数据集和1个epoch来调调<strong>weight scale</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">weight_scale = [<span class="number">1e-5</span>, <span class="number">1e-3</span>, <span class="number">1e-1</span>, <span class="number">1e2</span>, <span class="number">1e6</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> ws, lr, hl, ur <span class="keyword">in</span> product(weight_scale, learning_rate, hidden_layers, update_rule):</div><div class="line">    num_layer = len(hl)</div><div class="line">    model = FullyConnectedNet(hl, weight_scale=ws, use_batchnorm=<span class="keyword">False</span>)</div><div class="line">    solver = Solver(model, small_data,</div><div class="line">                    print_every=print_every, num_epochs=num_epochs, batch_size=batch_size,</div><div class="line">                    update_rule=ur,</div><div class="line">                    verbose=<span class="keyword">True</span>,</div><div class="line">                    optim_config=&#123;</div><div class="line">                      <span class="string">'learning_rate'</span>: lr</div><div class="line">                    &#125;</div><div class="line">                    </div><div class="line">             )</div><div class="line">    solver.train()</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/weight_scale3.png" alt=""></p><p>可以看出 <code>weight_scale=1e-1</code> 的时候表现较好，1e+2就开始爆炸了。Hints里说ReLU nets的lecture里给了”correct” value，不知道是不是lecture里的1e-2，但设置成这个值表现确实比较好，所以就它了！</p><p>接下来调整<strong>learning_rate</strong>来查看loss的变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">1e-7</span>, <span class="number">1e-5</span>, <span class="number">1e-3</span>, <span class="number">1e-1</span>, <span class="number">1e2</span>]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/lr1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr3.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr4.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/lr5.png" alt=""></p><p>可以看到_learning_rate_太小(1e-7)时，loss的变化不大；当_learning_rate_太大(1e+2)时，loss就爆炸了。这里有一个小trick：当看到loss &gt; 3倍的初始值时，就可以停止了。</p><h3 id="Learning-rate"><a href="#Learning-rate" class="headerlink" title="Learning rate"></a>Learning rate</h3><p>我们大致知道了_learning_rate_同学的表现情况，现在开始缩小_learning_rate_的范围，并使用原始数据集，来看看训练结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">learning_rate = [<span class="number">10</span> ** uniform(<span class="number">-5</span>, <span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</div></pre></td></tr></table></figure><p>其中比较好的几个结果：</p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate1.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate3.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/learning_rate4.png" alt=""></p><p>可以看到1.74e-4同学表现较好，那我们就把learning_rate设置为这个，epoch增加为2，再来看看loss的变化：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_loss.png" alt=""></p><p>看到loss趋于平坦，那我们把<strong>learning rate decay</strong>设置为0.95看看效果：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_decay_95.png" alt=""></p><p>表现似乎还不如不设置呢？那把<strong>learning rate decay</strong>设置为0.9：</p><p><img src="/2018/01/19/FullyConnectedNets-train/lr_decay_90.png" alt=""></p><p>好像差别也不大，那设置回1.0。然后开始考虑<strong>增强模型训练能力</strong></p><h3 id="Increasing-model-capacity"><a href="#Increasing-model-capacity" class="headerlink" title="Increasing model capacity"></a>Increasing model capacity</h3><p>一开始我设置的网络结构是3层，每层100个神经元，增强模型的训练能力可以通过<strong>增加神经元</strong>或者<strong>增加层数</strong>。然后就需要重新训练学习率了（望天</p><p>我先将层数增加为5层，改用小的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden_layers = [[<span class="number">100</span>]*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>,<span class="number">6</span>)]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_1.png" alt=""></p><p>可以看到这个结果并不是很好，需要通过上面的方法，查找合适的学习率。</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_2.png" alt=""></p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_3.png" alt=""></p><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>决定使用一下<strong>batchnorm</strong>来看看效果～</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_batch.png" alt=""></p><p>可以看到准确率一下子提高了～ 但是还没有达到我们想要的50%以上。这时候设置<strong>learning rate decay</strong>为0.95，并训练2个epoch，可以看到准确度又提高了一点。但是还不如上面三层的结果？</p><p><img src="/2018/01/19/FullyConnectedNets-train/layer5_decay.png" alt=""></p><p>俺现在要把神经元加到500个，回去用三层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden_layers = [[<span class="number">500</span>]*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>,<span class="number">4</span>)]</div></pre></td></tr></table></figure><p><img src="/2018/01/19/FullyConnectedNets-train/layer3_500.png" alt=""></p><p>✌ 在2个epoch准确率达到50%了，开心～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习的训练过程不是盲目的，应该掌握一定的技巧和方法。参考&lt;a href=&quot;https://www.reddit.com/r/cs231n/comments/443y2g/hints_for_a2/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hint
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>cs231n assignment1 学习笔记</title>
    <link href="http://yoursite.com/2017/11/10/cs231n-assignment1/"/>
    <id>http://yoursite.com/2017/11/10/cs231n-assignment1/</id>
    <published>2017-11-10T13:43:02.000Z</published>
    <updated>2017-12-10T09:53:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 <a href="https://matplotlib.org/faq/osx_framework.html#osxframework-faq" target="_blank" rel="external">Working with Matplotlib on OSX</a>，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。</p><h3 id="k-Nearest-Neighbor-kNN-exercise"><a href="#k-Nearest-Neighbor-kNN-exercise" class="headerlink" title="k-Nearest Neighbor (kNN) exercise"></a>k-Nearest Neighbor (kNN) exercise</h3><p>点击「run」执行每个框框，<code>dists = classifier.compute_distances_two_loops(X_test)</code> 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。</p><a id="more"></a><p>进行预测的时候发现准确度无论如何都是0.14，查了一下发现 <code>predict_labels</code> 函数没有修改😂 </p><p>二次循环的函数比较简单，一次循环边洗澡边思考，然后头脑风暴矩阵变换，开心地想出来了☺️ 核心代码<code>dists[i, :] = np.sqrt(np.sum((X[i, :] - self.X_train) ** 2, axis = 1)).T</code></p><p>无循环函数估摸着应该是要运用数学公式来解决。假设测试集(2x3)：<br>$$<br>testX = \begin{pmatrix} x_{11}&amp;x_{12}&amp;x_{13} \\x_{21}&amp;x_{22}&amp;x_{23} \end{pmatrix}<br>$$<br>训练集(4x3)：<br>$$<br>trainX = \begin{pmatrix}y_{11}&amp;y_{12}&amp;y_{13} \\y_{21}&amp;y_{22}&amp;y_{23}\\y_{31}&amp;y_{32}&amp;y_{33}\\y_{41}&amp;y_{42}&amp;y_{43}\ \end{pmatrix}<br>$$<br>最后生成 dist (2x4) 的距离矩阵，自然联想到矩阵乘法 (2x4) := (2x3) x (3x4)。刚好代码中也有提示矩阵乘法，那么 \(trainX\) 必然是转置一下的。<br>$$<br>trainX^T = \begin{pmatrix}y_{11}&amp;y_{21}&amp;y_{31}&amp;y_{41} \\y_{12}&amp;y_{22}&amp;y_{32}&amp;y_{42}\\y_{13}&amp;y_{23}&amp;y_{33}&amp;y_{43}\\ \end{pmatrix}<br>$$<br>我们取出一个测试集和一个训练集，计算一下它们的欧氏距离：<br>$$<br>dist[1, 1] = \sqrt{(x_{11}-y_{11})^2 + (x_{12}-y_{12})^2 + (x_{13}-y_{13})^2 }<br>$$<br>我们试一下把每个平方拆开：<br>$$<br>dist[1, 1] = \sqrt{x_{11}^2 - 2x_{11}y_{11}+y_{11}^2 + x_{12}^2 - 2x_{12}y_{12}+y_{12}^2+x_{13}^2 - 2x_{13}y_{13}+y_{13}^2}<br>$$<br>整理出根号里头的家伙：<br>$$<br>x_{11}^2+ x_{12}^2+x_{13}^2 +y_{11}^2+y_{12}^2 +y_{13}^2 - 2(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13})<br>$$<br>嗯。。。再一次感叹数学之美。。。于是「根号里头的家伙」就被分成了三个部分</p><ul><li>\(x_{11}^2+ x_{12}^2 +x_{13}^2\) 相当于 \(testX\) 元素平方，再按列相加，即把矩阵压缩成一列。</li><li>\(y_{11}^2+y_{12}^2 +y_{13}^2\) 相当于 \(trainX^T\) 元素平方，再按行相加 ，即把矩阵压缩成一行。</li><li>\(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13}\) 相当于\(testX * trainX\)</li></ul><p>于是就可以<strong>broadcasting</strong>啦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">transXtrain = self.X_train.T <span class="comment"># 转置训练集</span></div><div class="line">sumX = np.sum(X ** <span class="number">2</span>, axis = <span class="number">1</span>)[:, np.newaxis] <span class="comment">#（500x1）</span></div><div class="line">sumtransXtrain = np.sum(transXtrain ** <span class="number">2</span>, axis = <span class="number">0</span>)[np.newaxis] <span class="comment">#（1x5000）</span></div><div class="line">dists = np.sqrt( sumX + sumtransXtrain - <span class="number">2</span> * np.dot(X, transXtrain)) <span class="comment"># broadcasting &amp; matrix multiplication</span></div></pre></td></tr></table></figure><p>最后一个部分是关于k-折交叉验证，根据作业提示，循环每个可能的k值，运行k-nn算法 num_folds 次，每次选择一个子集作为训练集，最后一个子集为验证集。但参考了网上关于10折交叉验证的说法，是轮流将其中9份作为训练数据，1份作为测试数据的。所以我就循环了num+folds-1次。（后又参考网上其他实现代码，发现应该选择一个自己作为验证集，其他所有自己合并作为训练集。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">size = X_train_folds[<span class="number">0</span>].shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</div><div class="line">    k_to_accuracies[k] = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds<span class="number">-1</span>): <span class="comment"># each case </span></div><div class="line">        classifier.train(X_train_folds[i], y_train_folds[i]) <span class="comment"># use one as training data</span></div><div class="line">        predict_labels = classifier.predict(X_train_folds[<span class="number">-1</span>], k, num_loops = <span class="number">0</span>)</div><div class="line">        accuracy = sum(predict_labels == y_train_folds[<span class="number">-1</span>]) / size <span class="comment"># last fold as validation set</span></div><div class="line">        k_to_accuracies[k].append(accuracy)</div></pre></td></tr></table></figure><p><img src="/2017/11/10/cs231n-assignment1/knn.png" alt="cross-validation"></p><p>最后根据交叉验证的结果选择k=8最佳。</p><h3 id="Multiclass-Support-Vector-Machine-exercise"><a href="#Multiclass-Support-Vector-Machine-exercise" class="headerlink" title="Multiclass Support Vector Machine exercise"></a>Multiclass Support Vector Machine exercise</h3><p>在第一部分补充dW就卡了超级久。。。首先要先看官方的课程笔记 <a href="http://cs231n.github.io/optimization-1/#opt3" target="_blank" rel="external">optimization-1</a> ，我们要使用微积分来计算dW。当 $j = y_j $时公式如下：<br>$$<br>\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} 𝟙(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i<br>$$<br>代表梯度减少 margin &gt; 0 的个数乘以样本。</p><p>当 $ j  \neq y_j $时，公式如下：<br>$$<br>\nabla_{w_j} L_i = \ 𝟙(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i<br>$$<br>首先是实用循环方式的函数。一开始我想着那就用当 margin &gt; 0 时设置一个cnt来进行计数，然后还弄了一个非常麻烦的 tile</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">correct_j = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</div><div class="line">  cnt = <span class="number">0</span></div><div class="line">  scores = X[i].dot(W) <span class="comment"># (1, C)</span></div><div class="line">  correct_class_score = scores[y[i]]</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</div><div class="line">    <span class="keyword">if</span> j == y[i]:</div><div class="line">      correct_j = j</div><div class="line">      <span class="keyword">continue</span></div><div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div><div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">        loss += margin</div><div class="line">        cnt += <span class="number">1</span></div><div class="line">        </div><div class="line">  dW_test = np.tile(X[i].T[:,np.newaxis],(<span class="number">1</span>, num_classes)) <span class="comment"># 错的错的</span></div><div class="line">  dW_test[:, correct_j] =  cnt * X[i].T</div></pre></td></tr></table></figure><p>在实在想不出来的情况下，偷偷找了一下其他同学的代码，发现其实只要非常简单的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div><div class="line">loss += margin</div><div class="line">    dW[:, j] += X[i].T</div><div class="line">    dW[:, y[i]] -= X[i].T</div></pre></td></tr></table></figure><p>这里意识到了自己代码的两个问题：</p><ul><li><p>偏导都是累积的过程也就是说中间的符号是 += 或者 -= </p></li><li><p>当 $ j \neq y_j $ 时，dW只有相应的 j 列发生变化。</p></li><li><p>$$<br>\hat{y}(x) := \underbrace {w_0 + \sum_{i=1}^{n} w_i x_i }_{\text{线性回归}} + \underbrace {\sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{ij} x_i x_j}_{\text{交叉项（组合特征）}} \qquad \text{(n.ml.1.9.1)}<br>$$</p></li></ul><p>接下来是使用向量方式的函数。根据 <a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" rel="external">linear classification note</a> 写出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[y] + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>但在这里我测试了一下 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; print(scores[y].shape)</div><div class="line"></div><div class="line">(500,10)</div></pre></td></tr></table></figure><p>预料中结果应该是一个向量，由label索引所在值组成。但事实证明<code>scores[y]</code>只是把scoers重新排列了一次。参考<a href="https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy" target="_blank" rel="external">[1]</a>，修改代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[np.arange(scores.shape[<span class="number">0</span>]), y][:, np.newaxis] + <span class="number">1</span>)</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(scores[0]- scores[0][y[0]]) #1</div><div class="line">print((scores - scores[np.arange(scores.shape[0]), y][:, np.newaxis])[0]) #2</div><div class="line">print((scores - scores[y])[0]) #3</div><div class="line">print(scores[0]-scores[y[0]]) #4</div></pre></td></tr></table></figure><p>验证一下：12结果一致，34结果一致，12和34不一致。</p><p>同时也要记得在margins上作label索引处理，完整核心代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">num_train = X.shape[<span class="number">0</span>]</div><div class="line">scores = X.dot(W) <span class="comment"># scores.shape =  (N, C)</span></div><div class="line">margins = np.maximum(<span class="number">0</span>, scores - scores[np.arange(num_train), y][:, np.newaxis] + <span class="number">1</span>)</div><div class="line">margins[np.arange(num_train), y] = <span class="number">0</span> </div><div class="line">loss = np.sum(margins) / num_train + reg * np.sum(W * W)</div></pre></td></tr></table></figure><p>使用向量来完成梯度也卡了挺久。提示说重用计算过的值，那肯定就是margins啦，通过它们shape来判断了一下，估摸着要用X.T(D, N) * margins(N, C)来完成，但是空想不出来，于是写一个具体的例子来理解一下：</p><p><img src="/2017/11/10/cs231n-assignment1/svm_dw.jpg" alt="IMG_3133(20171115-204831)"></p><p>因为我们只需判断margins是否大于0，首先将其转化为只含0，1的矩阵。接着可以计算出每一行1的数量（即margin&gt;0的个数），将分类所在位置改为该数量，最后与X的转置相乘即可。完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mat = np.zeros(margins.shape) <span class="comment"># mat.shape = N x C</span></div><div class="line">idx = np.where(margins &gt; <span class="number">0</span>) <span class="comment"># return index of margins &gt; 0</span></div><div class="line">mat[idx] = <span class="number">1</span> </div><div class="line">count = np.sum(mat, axis=<span class="number">1</span>) <span class="comment"># count the number of margins &gt; 0</span></div><div class="line">mat[np.arange(num_train), y] = -count </div><div class="line"></div><div class="line">dW = np.dot(X.T, mat) / num_train</div></pre></td></tr></table></figure><p>但素plot loss的结果长这样。。。总觉得哪里怪怪的</p><p><img src="/2017/11/10/cs231n-assignment1/plot_loss.png" alt="IMG_3133(20171115-204831)"></p><p>然后plot weights的结果长这样。。。更觉得哪里怪怪的了</p><p><img src="/2017/11/10/cs231n-assignment1/plot_weights1.png" alt="IMG_3133(20171115-204831)"></p><p>感谢踩过坑的同学 <a href="https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/" target="_blank" rel="external">https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/</a> 原来是dW忘记做正则化了。加上<code>dW += reg * W </code>我们就可以看见光滑的曲线和多样的色彩了，噢耶！</p><p><img src="/2017/11/10/cs231n-assignment1/plot_loss2.png" alt="IMG_3133(20171115-204831)"></p><p><img src="/2017/11/10/cs231n-assignment1/plot_weights2.png" alt="IMG_3133(20171115-204831)"></p><h3 id="Softmax-exercise"><a href="#Softmax-exercise" class="headerlink" title="Softmax exercise"></a>Softmax exercise</h3><p>这次作业和svm的类似，分别通过循环和矩阵运算实现softmax，计算loss和gradient</p><p>首先是通过循环实现，理解一下softmax的公式</p><p><img src="/2017/11/10/cs231n-assignment1/softmax_loss.png" alt=""></p><p>根据 <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf" target="_blank" rel="external">lecture3-slides</a>，softmax的scores是非标准化的log概率（括号里看起来相当复杂，其实就是概率）。计算过程看下图比较明确：</p><p><img src="/2017/11/10/cs231n-assignment1/probability.png" alt=""></p><p>一开始还按照公式循环计算，然后发现标准化后，概率相加为1，因此只要取出所属类别的概率log就可以了，so easy～</p><p>关于梯度的计算结合了wiki以及参考 <a href="https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function" target="_blank" rel="external">derivative-of-softmax-loss-function</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  “”“省略部分代码”“”</div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  num_classes = W.shape[<span class="number">1</span>]</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</div><div class="line">    scores = X[i].dot(W)  <span class="comment"># (1,C)</span></div><div class="line">    probabilities = np.exp(scores) <span class="comment"># exp</span></div><div class="line">    probabilities /= np.sum(probabilities) <span class="comment">#normalize</span></div><div class="line">    loss -= np.log(probabilities[y[i]]) <span class="comment">#log</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes): <span class="comment"># pi-yi</span></div><div class="line">      <span class="keyword">if</span>(j == y[i]):</div><div class="line">        dW[:, j] += X[i] * (probabilities[y[i]] - <span class="number">1</span>)</div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        dW[:, j] += X[i] * probabilities[j]</div></pre></td></tr></table></figure><p>然后让我们迎来矩阵的计算～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div><div class="line">  <span class="string">"""省略部分代码"""</span></div><div class="line">  num_train = X.shape[<span class="number">0</span>]</div><div class="line">  scores = X.dot(W)</div><div class="line">  probabilities = np.exp(scores) <span class="comment"># exp</span></div><div class="line">  probabilities /= np.sum(probabilities, axis=<span class="number">1</span>)[:, np.newaxis] <span class="comment"># normalize</span></div><div class="line"></div><div class="line">  mat = probabilities[np.arange(num_train), y] <span class="comment"># 获取对应分类的概率</span></div><div class="line">  loss = -np.sum(np.log(mat)) / num_train + reg * np.sum(W * W)</div><div class="line"></div><div class="line">  mat = np.zeros_like(probabilities) </div><div class="line">  mat[np.arange(num_train), y] = <span class="number">1</span> <span class="comment"># 对应分类置为1</span></div><div class="line">  dW = X.T.dot(probabilities - mat) / num_train</div></pre></td></tr></table></figure><h3 id="Implementing-a-Neural-Network"><a href="#Implementing-a-Neural-Network" class="headerlink" title="Implementing a Neural Network"></a>Implementing a Neural Network</h3><p>loss的计算参考上面的softmax，但是要注意正则化时W1和W2都需要加上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算loss</span></div><div class="line">probabilities = np.exp(scores)  <span class="comment"># exp</span></div><div class="line">probabilities /= np.sum(probabilities, axis=<span class="number">1</span>)[:, np.newaxis]  <span class="comment"># normalize</span></div><div class="line">mat = probabilities[np.arange(N), y]  <span class="comment"># 获取对应分类的概率</span></div><div class="line">loss = -np.sum(np.log(mat)) / N + reg * (np.sum(W1*W1) + np.sum(W2*W2))</div></pre></td></tr></table></figure><p>反向传播的时候有一项很重要的<code>dh1[h1 &lt;=0] = 0</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算backward pass</span></div><div class="line"><span class="comment"># softmax gradient derivation</span></div><div class="line">dscores = probabilities</div><div class="line">dscores[range(N), y] -= <span class="number">1</span></div><div class="line">dscores /= N  <span class="comment"># (N, C)</span></div><div class="line"><span class="comment"># dW2 and db2</span></div><div class="line">grads[<span class="string">'W2'</span>] = h1.T.dot(dscores) + <span class="number">2</span> * reg * W2 <span class="comment"># (H, C)</span></div><div class="line">grads[<span class="string">'b2'</span>] = np.sum(dscores, axis=<span class="number">0</span>)</div><div class="line"><span class="comment"># next backprop into hidden layer</span></div><div class="line">dh1 = dscores.dot(W2.T) <span class="comment"># dh1.size = (N, H)</span></div><div class="line">dh1[h1 &lt;=<span class="number">0</span>] = <span class="number">0</span></div><div class="line"><span class="comment"># dW1 and db1</span></div><div class="line">grads[<span class="string">'W1'</span>] = X.T.dot(dh1) + <span class="number">2</span> * reg * W1 <span class="comment"># W1.size = (D, H)</span></div><div class="line">grads[<span class="string">'b1'</span>] = np.sum(dh1, axis = <span class="number">0</span>)</div></pre></td></tr></table></figure><p>作业要求说有参数的导数值与数值计算出的差值应该小于1e-8，之前正则化的时候都是使用<code>reg * W2</code>，但是现在要乘以2以后误差才会小于1e-11（包括后面准确率也会高一些）</p><p>predict的时候注意使用ReLu，否则就卡在0.38以下（如下图），准确率上不去0.4。</p><p><img src="/2017/11/10/cs231n-assignment1/hs300.png" alt=""></p><p>下面我们要开始调参啦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">best_net = <span class="keyword">None</span> <span class="comment"># store the best model into this </span></div><div class="line">best_val = <span class="number">-1</span></div><div class="line">results = &#123;&#125;</div><div class="line"></div><div class="line">hidden_size = [<span class="number">200</span>, <span class="number">300</span>, <span class="number">500</span>, <span class="number">800</span>, <span class="number">1000</span>]</div><div class="line">learning_rate = [<span class="number">9e-4</span>]</div><div class="line">iters = [<span class="number">100</span>]</div><div class="line">regularization_strength = [<span class="number">0.01</span>]</div><div class="line"></div><div class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> product(hidden_size, learning_rate, iters, regularization_strength):</div><div class="line">    </div><div class="line">    hs = item[<span class="number">0</span>]</div><div class="line">    lr = item[<span class="number">1</span>]</div><div class="line">    it = item[<span class="number">2</span>]</div><div class="line">    rg = item[<span class="number">3</span>]</div><div class="line">    net = TwoLayerNet(input_size, hs, num_classes)</div><div class="line"></div><div class="line">    <span class="comment"># Train the network</span></div><div class="line">    stats = net.train(X_train, y_train, X_val, y_val,</div><div class="line">                num_iters=it, batch_size=<span class="number">200</span>,</div><div class="line">                learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</div><div class="line">                reg=rg, verbose=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Predict on the validation set</span></div><div class="line">    val_acc = (net.predict(X_val) == y_val).mean()</div><div class="line">    train_acc = (net.predict(X_train) == y_train).mean()</div><div class="line">    results[(hs, lr, it, rg)] = (train_acc, val_acc)</div><div class="line">    </div><div class="line">    <span class="comment"># Check best validation accuracy</span></div><div class="line">    <span class="keyword">if</span>(val_acc &gt; best_val):</div><div class="line">        best_val = val_acc</div><div class="line">        best_net = net</div><div class="line">        </div><div class="line"><span class="comment"># Print out results.\</span></div><div class="line"><span class="keyword">for</span> hs, lr, it, reg <span class="keyword">in</span> sorted(results):</div><div class="line">    </div><div class="line">    train_acc, val_acc = results[(hs, lr, it, reg)]</div><div class="line">    print(<span class="string">'hs:%d it:%d lr:%e rg:%e train_acc:%f val_acc：%f'</span> %(hs, it, lr, reg, train_acc, val_acc))</div><div class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</div></pre></td></tr></table></figure><p>首先我们固定学习率、迭代次数（小一点）、正则化系数，修改隐藏层大小 = [200, 300, 500, 800, 1000]</p><p><img src="/2017/11/10/cs231n-assignment1/change_hs.png" alt=""></p><p>很明显隐藏层越大精确度更高，接下来固定其他参数，来调整一下学习率</p><p>发现学习率在 &gt; -e2 数量级都会报错？？？应该是学习率太大会导致W超出-1到1的范围吧。</p><p><img src="/2017/11/10/cs231n-assignment1/learning_rate_error.png" alt=""></p><p>测试学习率 = [3e-3, 4e-4, 5e-5, 6e-6]</p><p><img src="/2017/11/10/cs231n-assignment1/change_lr1.png" alt=""></p><p>似乎学习率大一些的结果比较好，那在这个数量级周围再测试一下</p><p><img src="/2017/11/10/cs231n-assignment1/change_lr2.png" alt=""></p><p>8e-2的学习率还是崩啦，这样看起来学习率设置成2e=e会比较好。接下来我们调大迭代次数和隐藏层大小，来调整正则化参数</p><p><img src="/2017/11/10/cs231n-assignment1/change_hs_rg.png" alt=""></p><p>嘻嘻最高到0.534了，可以加分了～</p><p><strong>参考链接</strong></p><ul><li><a href="https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy" target="_blank" rel="external">https://stackoverflow.com/questions/37290879/how-to-extract-elements-from-a-matrix-using-a-vector-of-indices-using-numpy</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 &lt;a href=&quot;https://matplotlib.org/faq/osx_framework.html#osxframework-faq&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Working with Matplotlib on OSX&lt;/a&gt;，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。&lt;/p&gt;
&lt;h3 id=&quot;k-Nearest-Neighbor-kNN-exercise&quot;&gt;&lt;a href=&quot;#k-Nearest-Neighbor-kNN-exercise&quot; class=&quot;headerlink&quot; title=&quot;k-Nearest Neighbor (kNN) exercise&quot;&gt;&lt;/a&gt;k-Nearest Neighbor (kNN) exercise&lt;/h3&gt;&lt;p&gt;点击「run」执行每个框框，&lt;code&gt;dists = classifier.compute_distances_two_loops(X_test)&lt;/code&gt; 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记五：自适应增强算法</title>
    <link href="http://yoursite.com/2017/11/05/machine-learning-in-action-note5/"/>
    <id>http://yoursite.com/2017/11/05/machine-learning-in-action-note5/</id>
    <published>2017-11-05T01:14:40.000Z</published>
    <updated>2017-11-23T03:18:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：</p><ul><li><p>Bagging（bootstrap aggregating）</p><p>Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。</p></li><li><p>Boosting</p><p>Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。</p><p>根据<a href="https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8" target="_blank" rel="external">LightGBM介绍视频</a>对Bossting做一些补充：本质上来说，Boosting的方法都是在训练好一个子模型后，统计一下现有复合模型的拟合情况，从而调节接下来学习任务的setting，使得接下来加入复合模型的子模型符合降低整体loss的目标。</p></li></ul><p>Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。</p><a id="more"></a><p>AdaBoost的每个样本都有一个权重，构成向量D。首先初始化每个样本的权重相等，在训练集上训练出一个弱分类器，然后计算出错误率 𝝐，通过错误率计算该分类器的 alpha 值，通过这个 alpha 值计算该分类器的权重。<br>$$<br>𝜶 = \frac{1}{2}\ln\frac{1-𝝐}{𝝐}<br>$$<br>接着对权重做出调整，降低分对的样本权重，<br>$$<br>D_{i+1} = \frac{D_i^{i}e^{-𝜶}}{Sum(D)}<br>$$</p><p>提高分错的样本权重。</p><p>$$<br>D_{i+1} = \frac{D_i^{i}e^{𝜶}}{Sum(D)}<br>$$<br>不断迭代达到一定数量或错误率为0，最后输出集成的弱分类器，通过加权来进行预测。算法过程如下图所示：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/adaboost.png" alt="adaboost"></p><p>###训练：决策树桩</p><p>算法的关键就在于如何训练弱分类器，并把它们集成一个强分类器。本章我们使用决策树桩（decision stump）来实现AdaBoost。决策树桩的树桩意味着我们只用单个特征来进行决策。首先我们设定一个阈值 _threshVal_ 与比较规则 _threshIneq_ ，在规则下判断特征值与阈值的大小。当 _threshIneq == ‘lt’_，将所有比 _threshVal_ 小的归为 -1；当 _threshIneq == ‘gt’_ 时，将所有比 _threshVal_ 大的归为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(data, dimen, threshVal, threshIneq)</span>:</span></div><div class="line">    <span class="string">"""测试是否某个值大于或小于阈值"""</span></div><div class="line">    retArr = ones((shape(data)[<span class="number">0</span>], <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</div><div class="line">        retArr[data[:, dimen] &lt;= threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        retArr[data[:, dimen] &gt; threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> retArr</div></pre></td></tr></table></figure><p>这里利用数组过滤来对比阈值大小 <code>retArr[data[:, dimen] &lt;= threshVal]</code> 可以很简单滴获取到返回数组，这个返回数组就是一个弱分类器预测的结果。建立决策树桩的过程就是找出错误率最低的弱分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(data, labels, D)</span>:</span></div><div class="line">    <span class="string">"""遍历样本的每一列，返回错误率最小的弱分类器"""</span></div><div class="line">    dataMat = mat(data); labelMat = mat(labels).T</div><div class="line">    m, n = shape(dataMat)</div><div class="line">    numSteps = <span class="number">10</span>; bestStump = &#123;&#125;; bestClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    minError = inf</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n): <span class="comment"># 每个特征</span></div><div class="line">        rangeMin = dataMat[:, i].min(); rangeMax = dataMat[:, i].max()</div><div class="line">        stepSize = (rangeMax - rangeMin) / numSteps</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">-1</span>, int(numSteps + <span class="number">1</span>)): <span class="comment"># 每个步长</span></div><div class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment"># 每个不等号</span></div><div class="line">                threshVal = (rangeMin + float(j) * stepSize)</div><div class="line">                predictedVals = stumpClassify(dataMat, i, threshVal, inequal) <span class="comment"># 建立一棵决策树桩</span></div><div class="line">                errArr = mat(ones((m, <span class="number">1</span>)))</div><div class="line">                errArr[predictedVals ==  labelMat] = <span class="number">0</span></div><div class="line"></div><div class="line">                <span class="comment"># 计算错误率</span></div><div class="line">                <span class="comment"># AdaBoost和分类器交互的地方</span></div><div class="line">                weightedError = D.T * errArr</div><div class="line">                <span class="comment">#print("split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f" %(i, threshVal, inequal, weightedError))</span></div><div class="line">                <span class="keyword">if</span> weightedError &lt; minError: <span class="comment"># 保存错误率较小的弱分类器</span></div><div class="line">                    minError = weightedError</div><div class="line">                    bestClassEst = predictedVals.copy() <span class="comment"># 预测结果</span></div><div class="line">                    bestStump[<span class="string">'dim'</span>] = i</div><div class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</div><div class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</div><div class="line">    <span class="keyword">return</span> bestStump, minError, bestClassEst</div></pre></td></tr></table></figure><p>我们知道了如何建立一个弱分类器，那么接下来就到了集成弱分类器的部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(data, labels, numIt = <span class="number">40</span>)</span>:</span></div><div class="line">    weakClassArr = []</div><div class="line">    m = shape(data)[<span class="number">0</span>]</div><div class="line">    <span class="comment"># D保存每个样本的权重</span></div><div class="line">    <span class="comment"># AdaBoost算法会降低分对的样本权重，提高分错的样本权重</span></div><div class="line">    D = mat(ones((m, <span class="number">1</span>))/m)</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</div><div class="line">        bestStump, error, classEst = buildStump(data, labels, D)</div><div class="line">        <span class="comment"># 计算该分类器的权重</span></div><div class="line">        alpha = float(<span class="number">.5</span> * log((<span class="number">1</span> - error) / max(error, <span class="number">1e-16</span>))) <span class="comment"># 确保没有错误时不发生除零溢出</span></div><div class="line">        bestStump[<span class="string">'alpha'</span>] = alpha</div><div class="line">        weakClassArr.append(bestStump)</div><div class="line"></div><div class="line">        <span class="comment"># 1 / 计算下一次迭代的权重向量D</span></div><div class="line">        expon = multiply(<span class="number">-1</span> * alpha * mat(labels).T, classEst)</div><div class="line">        D = multiply(D, exp(expon))</div><div class="line">        D = D / D.sum() <span class="comment"># D是一个概率分布, sum = 1</span></div><div class="line"></div><div class="line">        <span class="comment"># 2 / 计算总错误率</span></div><div class="line">        aggClassEst += alpha * classEst</div><div class="line">        aggErrors = multiply(sign(aggClassEst) != mat(labels).T, ones((m, <span class="number">1</span>)))</div><div class="line">        errorRate = aggErrors.sum() / m</div><div class="line">        <span class="keyword">if</span> errorRate == <span class="number">0</span>: <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> weakClassArr</div></pre></td></tr></table></figure><p>###分类：分类器加权</p><p>将每个弱分类器的结果进行加权，输出最终预测的分类结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass, classifierArr)</span>:</span></div><div class="line">    dataMatrix = mat(datToClass)</div><div class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)): <span class="comment"># 每个弱分类器</span></div><div class="line">        classEst = stumpClassify(dataMatrix, classifierArray[i][<span class="string">'dim'</span>],classifierArray[i][<span class="string">'thresh'</span>], classifierArray[i][<span class="string">'ineq'</span>])</div><div class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>] * classEst <span class="comment"># 加权</span></div><div class="line">    <span class="comment"># 返回预测结果</span></div><div class="line">    <span class="keyword">return</span> sign(aggClassEst)</div></pre></td></tr></table></figure><h3 id="非均衡分类问题：ROC曲线"><a href="#非均衡分类问题：ROC曲线" class="headerlink" title="非均衡分类问题：ROC曲线"></a>非均衡分类问题：ROC曲线</h3><p>之前我们都只用错误率来判断分类器的好坏，假设分类问题的结果是不均衡的，比如预测一个人一个人得癌症／没有得癌症，这个分类的代价是不同的，只看错误率是没什么意义的。因此我们引入一些别的性能指标来判断分类器</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">+1</th><th style="text-align:center">-1</th></tr></thead><tbody><tr><td style="text-align:center">+1</td><td style="text-align:center">真正例（TP）</td><td style="text-align:center">伪反例（FN）</td></tr><tr><td style="text-align:center">-1</td><td style="text-align:center">伪正例（FP）</td><td style="text-align:center">真反例（TN）</td></tr></tbody></table><ul><li>正确率（Precision） = TP /（TP + FP）</li><li>召回率（Recall） = TP /（TP + FN）</li></ul><p>ROC代表接受者特征（reciver operating characteristic）曲线。横轴是伪正例的比例 （假阳率 = FP /（FP+TN）），纵轴是真正例的比例（真阳率 = TP / （TP + FN））。</p><blockquote><p>为了创建ROC曲线，首先将分类样例按照其测试强度排序。先从排名最低的样例开始，所有排名更低的样例都被视为反例，而所有排名更高的样例都被判为正例。该情况对应点为(1, 1)。然后将其移到排名次低的样例中去，如果该样例属于正例，那么对真阳率进行修改；如果该样例属于反例，那么对假阴率进行修改。</p></blockquote><p>虽然没看懂上面这段话，但看懂了原代码，感觉循环 <code>for index in sortedIndicies.tolist()[0]</code> 超迷的，tolist() 是什么鬼，[0] 又是什么鬼。对原函数进行分析，首先是对传入参数 _aggClassEst_ 进行 _argsort_ 排序。_aggClassEst_ 是一个矩阵， shape ==  (298,1)，如果直接排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort1.png" alt=""></p><p>如果将其转置再进行排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort2.png" alt=""></p><p>根据 <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argsort.html" target="_blank" rel="external">numpy.argsort</a> 关于一维数组的排序，我们先将其转换为一维数组，并将其<strong>shape</strong>转换 (298 ,)。转换有两种方法：</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aggClassEst.getA().reshape((-1, ))</div></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">squeeze(aggClassEst.getA())</div></pre></td></tr></table></figure></li></ul><p>在这里简单说明一下 (R，1)和 (R，) 的区别。NumPy数组的<strong>shape</strong>为 (R， ) 意味着这个数组只有一个索引，以一个有12个元素的数组为例：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/narray1.png" alt=""></p><p>当我们将其 <code>reshape((3, 4))</code> 后，它有了两个索引：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape34.png" alt=""></p><p>当我们 <code>reshape((12, 1))</code>，它其实也是有两个索引，只是其中一个恒为0：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape1.png" alt=""></p><p>于是修改代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></div><div class="line">    cur = (<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 保留绘制光标的位置</span></div><div class="line">    ySum = <span class="number">0</span> <span class="comment"># 计算AUC</span></div><div class="line">    numPosClas = sum(array(classLabels) == <span class="number">1</span>)</div><div class="line">    yStep = <span class="number">1</span> / float(numPosClas)</div><div class="line">    xStep = <span class="number">1</span> / float(len(classLabels) - numPosClas)</div><div class="line">    <span class="comment"># 获取排序后的索引</span></div><div class="line">    <span class="comment"># 从 (1, 1) 绘制到 (0, 0)</span></div><div class="line">    sortedIndicies = predStrengths.argsort()</div><div class="line">    <span class="comment"># fig = plt.figure();  fig.clf();ax = plt.subplot(111)</span></div><div class="line">    fig, ax = plt.subplots()</div><div class="line">    <span class="comment"># for index in sortedIndicies.tolist()[0]:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sortedIndicies:</div><div class="line">        print(predStrengths[i])</div><div class="line">        <span class="keyword">if</span> classLabels[i] == <span class="number">1</span>:</div><div class="line">            delX = <span class="number">0</span>; delY = yStep</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            delX = xStep; delY = <span class="number">0</span></div><div class="line">            ySum += cur[<span class="number">1</span>]</div><div class="line">        ax.plot([cur[<span class="number">0</span>], cur[<span class="number">0</span>]-delX], [cur[<span class="number">1</span>], cur[<span class="number">1</span>]-delY], c= <span class="string">'b'</span>)</div><div class="line">        cur = (cur[<span class="number">0</span>] - delX, cur[<span class="number">1</span>] - delY)</div><div class="line">    ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'b--'</span>)</div><div class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</div><div class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</div><div class="line">    ax.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    plt.show()</div><div class="line">    print(<span class="string">"the Area Under the Curve is: "</span>, ySum * xStep)</div><div class="line"></div><div class="line">predStrengths = aggClassEst.getA().reshape((<span class="number">-1</span>, ))</div><div class="line">plotROC(predStrengths, labels)</div></pre></td></tr></table></figure><p><img src="/2017/11/05/machine-learning-in-action-note5/roc.png" alt=""></p><p><strong>参考链接</strong></p><p>1) <a href="https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r" target="_blank" rel="external">Difference between numpy.array shape (R, 1) and (R,)</a></p><p>2）<a href="https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8" target="_blank" rel="external">LIghtGBM官方介绍视频</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bagging（bootstrap aggregating）&lt;/p&gt;
&lt;p&gt;Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Boosting&lt;/p&gt;
&lt;p&gt;Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&quot;https://www.kesci.com/apps/home/project/5a0aecde60680b295c25f5d8&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;LightGBM介绍视频&lt;/a&gt;对Bossting做一些补充：本质上来说，Boosting的方法都是在训练好一个子模型后，统计一下现有复合模型的拟合情况，从而调节接下来学习任务的setting，使得接下来加入复合模型的子模型符合降低整体loss的目标。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记四：支持向量机</title>
    <link href="http://yoursite.com/2017/10/22/machine-learning-in-action-note4/"/>
    <id>http://yoursite.com/2017/10/22/machine-learning-in-action-note4/</id>
    <published>2017-10-22T00:50:53.000Z</published>
    <updated>2017-11-07T11:54:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。</p><p>上一章学习的「对数几率函数」中，我们提到了</p><blockquote><p>利用线性回归模型的预测结果去逼近真实标记的对数几率</p></blockquote><p>标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。</p><p>上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svmpic.jpg" alt="svmpic"></p><a id="more"></a><p>👆🏻有一条线把苹果和香蕉分开了（在二维空间中就是一条线），这样分开不同训练样本的线就称为「划分超平面」。距离这条线最近的几个点就是「支持向量」，它们到这条线的距离就为「间隔」。那么我们再来回顾一下刚说的一句话：</p><blockquote><p>找出具有“最大间隔”（maximum margin）的「划分超平面」</p></blockquote><p>能够划分训练样本的超平面可能有很多，我们应该要找的，是位于“正中间”的那个划分超平面，也就是距离不同类别都尽可能远的那个超平面。这样进行预测时误差才会尽可能小。</p><p>说到这里，我们可以明确一下SVM算法的设计问题了。因为越接近超平面的点越“难”分割，找到这些点就万事大吉了。因此SVM学习分类器最重要的是找到哪些样本作为「支持向量」。其本质是一个最优化问题。</p><p>一个最优化问题通常有两个最基本的因素：1）目标函数：希望什么东西的指标达到最好；2）优化对象：你希望通过改变哪些因素使目标函数达到最优。在SVM中，目标函数是「最大间隔」，优化对象就是「划分超平面」。下面我们就需要对这两个基本因素进行数学描述。</p><p>###SVM的数学建模</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svm_model.png" alt="svmpic"></p><p>划分超平面的线性方程：<br>$$<br>w^Tx + b = 0<br>$$<br>其中，<strong>w</strong> = （w~1~； w~2~；… ； w~d~）为法向量。样本空间中任意点 <strong>x</strong> 到超平面的距离（几何间隔）可写为：<br>$$<br>d = \frac{|w^Tx + b|}{||w||}<br>$$<br>其中，$||w||$ 为向量的模。前面我们说到，SVM的分类结果是+1／-1，令：<br>$$<br>\begin{cases}w^Tx + b≥+1, y_i = +1 \\ w^Tx + b≤-1, y_i = -1<br>\end{cases}<br>$$<br>支持向量使得等号成立。两个异类支持向量到超平面的距离之和为<br>$$<br>𝜸 = \frac{2}{||w||}<br>$$<br>显然，为了最大化间隔，仅需最大化$||w||^{-1}$，等价于最小化$||w||^2$。</p><p>到这里，我们可以给出SVM的数学描述：<br>$$<br>\min_{w,b}\frac{1}{2}||w||^2 \\  s.t. \quad y_i(w^Tx+b) ≥1,i = 1,2,…,m<br>$$</p><p>缩写s. t. 表示“Subject to”，是“服从某某条件”的意思。根据参考链接[3]解释一下这个条件的含义。我们定义「函数间隔」为<br>$$<br>y(w^Tx+b)=yf(x)≥𝜸<br>$$<br>前面乘上类别 y 之后保证间隔的非负性（因为 f(x)&lt;0 对应于 y=−1 的那些点）。</p><p>###对偶问题</p><p>上面SVM的数学描述其实是一个<strong>二次优化问题</strong>——目标函数是二次的，约束条件是线性的。引入「拉格朗日乘子法」求解，对每条约束添加拉格朗日乘子𝜶~i~ ≥ 0，则该问题的拉格朗日函数可写为<br>$$<br>L（w,b,𝜶) =\frac{1}{2}||w||^2 + 𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))<br>$$<br>其中，\(𝜶 = （𝛼_1;𝛼_2;…;𝛼_m)\)。我们令<br>$$<br>𝜃(w) =\max _{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$</p><p>则上式的最优值为 $𝜃(w) = \frac{1}{2}||w||^2$, 即我们需要优化的SVM数学模型。具体公式为</p><p>$$<br>\min_{w,b} 𝜃(w)  =\min_{w,b}\max_{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$<br>将min和max交换位置得到原始问题的对偶问题</p><p>$$<br>\max_{𝜶_i≥ 0}\min_{w,b}\quad L（w,b,𝜶)<br>$$<br><strong>为什么可以转化呢？</strong>因为瘦死的骆驼比马大，「最大值中的最小值」也比「最小值中的最大值」来得大。</p><p><strong>那么先求最大值和先求最小值有什么区别呢？</strong>因为这样更容易求解。我们通过偏导先求 <strong>L</strong> 关于 𝒘 和 𝑏 极小，再求 <strong>L</strong> 的极大。分别令 ∂/∂w 和 ∂/∂b 为零可得<br>$$<br>\frac{∂L}{∂w} = 0 ⇒ w = \sum_{i=1}^m𝜶_iy_ix_i<br>$$</p><p>$$<br>\frac{∂L}{∂b} = 0 ⇒ \sum_{i=1}^m𝜶_iy_i = 0<br>$$</p><p>代回 <strong>L</strong> 得到SVM数学描述的对偶问题<br>$$<br>max_𝜶\quad \sum_{i=1}^m𝜶_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m𝜶_i𝜶_jy_iy_jx_i^Tx_j^T<br>$$</p><p>$$<br>s.t\quad\sum_{i=1}^m𝜶_iy_i = 0,\quad𝜶_i≥ 0,i =1,2,…,m<br>$$</p><p>解出𝜶后，求出w与b即可得到模型<br>$$<br>\begin {align_} f(x) &amp; = w^T+b \\ &amp; = \sum_{i=1}^m𝜶_iy_ix_i^Tx + b\end {align_}<br>$$<br>所以只要求出了w和b，将测试数据带入上面这个模型，即可得出预测值。</p><p>实际上，所有非支持向量的𝜶值都为零，<strong>最终模型只与支持向量有关</strong>。回忆一下我们的拉格朗日函数<br>$$<br>\max _{𝜶_i≥ 0}\quad L（w,b,𝜶) =\max_{𝜶_i≥ 0}\quad \frac{1}{2}||w||^2 + \color{red}{𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))}<br>$$<br>如果 x~i~ 是支持向量，那么红色标出部分为0（因为支持向量函数间隔为1）。对于非支持向量，函数间隔大于1，那么红色标出部分将小于0，为满足整个式子最大，只能𝜶~i~ 为0。因此我们预测的过程只需要计算少量向量的内积，速度是很快的。</p><h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>好的，接下来问题就转换为<strong>如何求解对偶问题</strong>。1996年（竟然在我出生这年搞事情🙂）John Platt发布了SMO（Sequntial Minimal Optimization）算法，将大优化问题分解为多个小优化问题求解。SMO的基本思路是先固定𝜶~i~之外的所有参数，然后求𝜶~i~上的极值。由于存在约束 $sum_{i=1}^m𝜶_iy_i = 0$ ，若固定 𝜶~i~ 之外的其他变量，则 𝜶~i~ 可由其他变量导出。于是SMO每次选择两个变量𝜶~i~ 和 𝜶~j~ ，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li>选取一对需要更新的变量 𝜶~i~ 和 𝜶~j~ ；</li><li>固定 𝜶~i~ 和 𝜶~j~ 以外参数，求解获得更新后的 𝜶~i~ 和 𝜶~j~ </li></ul><p>好的，理解SMO实在是无能为力了，我们进入下一个话题「核函数」。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>上面我们解决了线性分类问题，但SVM还可以解决非线性的分类问题，这就需要引入「核函数」，来将数据从一个特征空间转换到另一个特征空间，在新空间下再利用模型对数据进行处理。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/kernel.gif" alt="kernel"></p><p>径向基函数（radial bias function）是SVM常用的一个核函数，具体公式为<br>$$<br>k(x,y) = exp(\frac{-||x-y||^2}{2𝜎^2})<br>$$<br>其中 𝜎 是用户定义的函数值跌落到0的速度参数。</p><h3 id="SVM的一般流程"><a href="#SVM的一般流程" class="headerlink" title="SVM的一般流程"></a>SVM的一般流程</h3><ol><li>收集数据：可以使用任意方法</li><li>准备数据：需要数值型数据</li><li>分析数据：可视化分割超平面是很有帮助的</li><li>训练算法：SVM算法最耗时的地方。该过程主要实现两个参数调优</li><li>测试算法：计算十分简单</li><li>使用算法：几乎所有分类问题都可以用SVM来解决，值得一提的是，SVM本身是一个二类分类器，你需要修改一些代码来适应多分类问题</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在作者给的源码中，有可视化数据的部分，修改了一下用于测试不同k值rbf选择的支持向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fr = open(<span class="string">'testSetRBF.txt'</span>) <span class="comment"># generate data</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</div><div class="line">    lineSplit = line.strip().split(<span class="string">'\t'</span>)</div><div class="line">    xPt = float(lineSplit[<span class="number">0</span>])</div><div class="line">    yPt = float(lineSplit[<span class="number">1</span>])</div><div class="line">    label = float(lineSplit[<span class="number">2</span>])</div><div class="line">    <span class="keyword">if</span> (label &lt; <span class="number">0</span>):</div><div class="line">        xcord0.append(xPt)</div><div class="line">        ycord0.append(yPt)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        xcord1.append(xPt)</div><div class="line">        ycord1.append(yPt)</div><div class="line">k1 = <span class="number">1.3</span> <span class="comment"># 修改测试</span></div><div class="line">sVs = svm.testRbf(k1)</div><div class="line">m = shape(sVs)[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">    x = sVs.A[i][<span class="number">0</span>]</div><div class="line">    y = sVs.A[i][<span class="number">1</span>]</div><div class="line">    circle = Circle((x,y), <span class="number">0.05</span>, facecolor=<span class="string">'none'</span>, edgecolor=(<span class="number">0</span>, <span class="number">0.8</span>, <span class="number">0.8</span>), linewidth=<span class="number">3</span>, alpha=<span class="number">0.5</span>)</div><div class="line">    ax.add_patch(circle)</div><div class="line">ax.scatter(xcord0, ycord0, marker=<span class="string">'s'</span>, s=<span class="number">30</span>)</div><div class="line">ax.scatter(xcord1, ycord1, marker=<span class="string">'o'</span>, s=<span class="number">30</span>, c=<span class="string">'red'</span>)</div><div class="line">plt.title(<span class="string">'RBF k1 = %f, %d Support Vectors'</span> %(k1, m))</div><div class="line">plt.show()</div><div class="line">fr.close()</div></pre></td></tr></table></figure><p><img src="/2017/10/22/machine-learning-in-action-note4/k_1.3.png" alt="k_1.3"></p><p><img src="/2017/10/22/machine-learning-in-action-note4/k_0.1.png" alt="k_0.1"></p><p>可以看出参数值越小支持向量的范围越模糊，选择合适的参数还是很重要的。</p><p><strong>参考链接</strong></p><ul><li><a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">https://www.zhihu.com/question/21094489</a></li><li><a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="external">零基础学SVM—Support Vector Machine(一)</a> </li><li><a href="http://blog.pluskid.org/?p=632" target="_blank" rel="external">http://blog.pluskid.org/?p=632</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。&lt;/p&gt;
&lt;p&gt;上一章学习的「对数几率函数」中，我们提到了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;利用线性回归模型的预测结果去逼近真实标记的对数几率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。&lt;/p&gt;
&lt;p&gt;上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2017/10/22/machine-learning-in-action-note4/svmpic.jpg&quot; alt=&quot;svmpic&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记三：对数几率回归</title>
    <link href="http://yoursite.com/2017/10/20/machine-learning-in-action-note3/"/>
    <id>http://yoursite.com/2017/10/20/machine-learning-in-action-note3/</id>
    <published>2017-10-20T08:24:26.000Z</published>
    <updated>2017-11-07T11:53:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。</p><p>「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：<br>$$<br>f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i<br>$$<br>有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>$$<br>\ln y = w^Tx + b<br>$$<br>这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）<br>$$<br>y = g^{-1}(w^Tx + b)<br>$$<br>其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。</p><a id="more"></a><p>其次，我们想用回归分析来做<strong>分类任务</strong>怎么办？那么我们可以找一个单调可微函数，将分类任务的真实标记y与线性回归模型的预测值 <strong>z=w^T^x+b</strong> 联系起来。考虑最简单的二分类任务，我们想到了“单位阶跃函数”（unit-step function）<br>$$<br>y = \begin{cases}0, z<0\\0.5, z="0\\1,z">0<br>\end{cases}<br>$$<br>但单位阶跃函数不连续，因此不能作为g(･)。于是聪明的人们就找到了Sigmoid函数，也称对数几率函数（logistic function）：<br>$$<br>y = \frac{1}{1+e^{-z}}<br>$$<br>类比上面的线性回归我们可以得到：<br>$$<br>\ln\frac{y}{1-y} = w^Tx + b<br>$$<br>由此就引发了关于 Logistic Regression 中文名的思考。大多数看到的翻译都是「逻辑回归」，其中逻辑一词代表什么一直不懂。但在西瓜书中作者称之为「对数几率回归」，其言语是让我信服的。</0\\0.5,></p><p>将 y 视为样本x作为正例的可能性，则 1-y 时其反例的可能性，两者比值称为“对率”（odds）：<br>$$<br>\frac{y}{1-y}<br>$$<br>对率反映了x作为正例的相对可能性。对几率取对数则得到“对数几率”（log odds，亦称logit）：<br>$$<br>\ln\frac{y}{1-y}<br>$$<br>由此可看出，实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。</p><p>不管怎么说，在理解一个回归模型的时候至少先把它的名字念对吧233</p><h3 id="分析：可视化分析数据"><a href="#分析：可视化分析数据" class="headerlink" title="分析：可视化分析数据"></a>分析：可视化分析数据</h3><p>在调用plotBestFit时，有一个<code>weights.getA())</code>这个getA()是啥玩意儿？根据参考链接[1]-11楼的小伙伴给出了答案，矩阵通过这个getA()这个方法可以将自身返回成一个n维数组对象</p><p><img src="/2017/10/20/machine-learning-in-action-note3/geta_test.png" alt=""></p><p>从上图输出类型可以看出，使用getA函数将矩阵转化为了数组</p><p><img src="/2017/10/20/machine-learning-in-action-note3/matnarr.png" alt=""></p><p>当矩阵只有一行的时候输出 mat[1] 是会报错的</p><p><img src="/2017/10/20/machine-learning-in-action-note3/mat_trans.png" alt=""></p><p>将 mat 转制后 mat[1] 输出的是一个1x1的矩阵而不是一个数值</p><p>###训练：梯度下降（Gradient Descent）</p><p>在对数几率函数的训练过程中，最重要的就是如何训练权值w。为了使其公式化，我们定义了一个“代价函数”（cost function），来衡量预测值和实例标记的差距：<br>$$<br>J(w) = \frac{1}{2}\sum_{i=1}^m(h_w(x^{(i)})-y^{i})^2<br>$$<br>我们希望这个值很小，于是就有了梯度下降算法的迭代公式：<br>$$<br>w_j := w_j - 𝛼\frac{\partial }{\partial w_j}J(w)<br>$$<br>让我们针对一个训练样本 (x, y) 算算这个偏导是啥：<br>$$<br>\begin{align}\frac{\partial }{\partial w_j}J(w)   &amp; =   \frac{\partial }{\partial w_j}\frac{1}{2}(h_w(x)-y)^2\\<br>&amp; =  2･\frac{1}{2}(h_w(x)-y)･\frac{𝜕}{𝜕w_j}(h_w(x)-y)\\<br>&amp; = (h_w(x)-y)･\frac{𝜕}{𝜕w_j}(\sum_{i=0}^nw_ix_i-y)\\<br>&amp; = (h_w(x)-y)x_j\end{align}<br>$$<br>因此对于一个训练样本有：<br>$$<br>w_j := w_j - 𝛼(y^{i}-h_w(x^{i}))x_j^{(i)}<br>$$<br>运用在多于一个样本的训练集上时，我们有两种选择：</p><ol><li><p><strong>批量梯度下降（batch gradient descent）</strong></p><p>将所有权值初始化为1</p><p>循环R次: </p><p>​    计算整个训练集的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataMatrix = mat(dataMatIn) <span class="comment"># m x n</span></div><div class="line">    labelMat = mat(classLabels).transpose() <span class="comment">#  1x100 to 100x1</span></div><div class="line">    m, n = shape(dataMatrix)</div><div class="line">    alpha = <span class="number">0.001</span></div><div class="line">    maxCycles = <span class="number">500</span></div><div class="line">    weights = ones((n,<span class="number">1</span>)) <span class="comment"># n x 1</span></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</div><div class="line">        h = sigmoid(dataMatrix * weights) <span class="comment"># m x 1, 整个训练集</span></div><div class="line">        error = labelMat - h</div><div class="line">        weights = weights + alpha * dataMatrix.transpose()*error</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure></li><li><p><strong>随机梯度下降（stochastic gradient descent／incremental gradient descent）</strong></p><p>将所有权值初始化为1</p><p>对每一个训练样本：</p><p>​    计算训练样本的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">        h = sigmoid(sum(dataArr[i] * weights)) <span class="comment"># single value，一个训练样本</span></div><div class="line">        error = classLabels[i] - h <span class="comment"># single value</span></div><div class="line">        weights = weights + alpha * error * dataArr[i]</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><p>两种方法的区别在于计算 <strong>weights</strong> 时，「批量梯度下降」每次都用整个训练集来更新权值。假设我们有m个实例和n个特征，每次就要做 <strong>m*n</strong> 次乘法，当 <strong>m</strong> 非常大时，计算代价是很高的。而「随机梯度下降」每次只用一个训练样本，它同时也是一种在线学习算法。虽然它有时候很难安全等于最优值，但也差不多了。因此，我们一般使用「随机梯度下降」来训练权值。</p><p>接下来我们对随机梯度下降函数做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatIn, classLabels, numIter=<span class="number">150</span>)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter): <span class="comment"># 迭代次数</span></div><div class="line">        dataIndex = list(range(m))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">            <span class="comment"># 1/ Alpha changes with each iteration</span></div><div class="line">            alpha = <span class="number">4</span>/(<span class="number">1</span>+iter+i) + <span class="number">0.01</span></div><div class="line">            <span class="comment"># 2/ Update vectors are randomly selected</span></div><div class="line">            randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</div><div class="line">            h = sigmoid(sum(dataArr[randIndex] * weights)) <span class="comment"># single value</span></div><div class="line">            error = classLabels[randIndex] - h <span class="comment"># single value</span></div><div class="line">            weights = weights + alpha * error * dataArr[randIndex]</div><div class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><h3 id="预测：从疝气病预测病马的死亡率"><a href="#预测：从疝气病预测病马的死亡率" class="headerlink" title="预测：从疝气病预测病马的死亡率"></a>预测：从疝气病预测病马的死亡率</h3><p>这里插播一下如何处理数据的缺失值。面对缺失的一些特征我们有如下选择：</p><ol><li>使用可用特征的均值来填补缺失值</li><li>使用特殊值来填补缺失值，如-1</li><li>忽略有缺失值的样本</li><li>使用相似样本的均值填补缺失值</li><li>使用另外的机器学习算法预测缺失值</li></ol><p>在「对数几率回归」中，特征的缺失值可以用0来填补。因为 <code>weights = weights + alpha _ error _ dataArr[randIndex]</code>，当对应特征值为0时，该特征的系数值将不变，则 <code>weights = weights</code>。由于 <code>sigmoid（0）=0.5</code>，对于结果的预测不具有任何倾向性。但是标记缺失的样本我们就不得不丢弃了啊（都不知道你有病没病要你何用</p><p>好的，说了这么多，作者已经帮我们处理完数据了。</p><p>终于我们要开始预测了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></div><div class="line">    prob = sigmoid(sum(inX*weights))</div><div class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div></pre></td></tr></table></figure><p><img src="/2017/10/20/machine-learning-in-action-note3/test_result.png" alt=""></p><p>我的妈呀训练快一小时呢。。。</p><p><strong>参考链接：</strong></p><p>[1] <a href="http://tieba.baidu.com/p/2905471495" target="_blank" rel="external">http://tieba.baidu.com/p/2905471495</a></p><p>[2] 周志华《机器学习》</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。&lt;/p&gt;
&lt;p&gt;「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：&lt;br&gt;$$&lt;br&gt;f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i&lt;br&gt;$$&lt;br&gt;有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即&lt;br&gt;$$&lt;br&gt;\ln y = w^Tx + b&lt;br&gt;$$&lt;br&gt;这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）&lt;br&gt;$$&lt;br&gt;y = g^{-1}(w^Tx + b)&lt;br&gt;$$&lt;br&gt;其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>image-classification-note</title>
    <link href="http://yoursite.com/2017/10/19/image-classification-note/"/>
    <id>http://yoursite.com/2017/10/19/image-classification-note/</id>
    <published>2017-10-19T07:41:20.000Z</published>
    <updated>2017-10-21T09:39:25.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems:"></a>Problems:</h3><ol><li>Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.</li><li>Viewpoint variation: All pixels change when the camera moves</li><li>Illumination: There can be lighting conditions going on in the scene</li><li>Deformation: Cats can assume a lot of different, varied poses and positions.</li><li>Occlusion: You might only see a part of a cat.</li><li>Background Clutter: The foreground of the cat look similar in appearance</li><li>Intraclass variation: Cats can come in different shapes and sizes and colors and ages</li></ol><a id="more"></a><h3 id="An-image-classifier"><a href="#An-image-classifier" class="headerlink" title="An image classifier"></a>An image classifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify_image</span><span class="params">(image)</span>:</span></div><div class="line"><span class="comment"># Some magic here?</span></div><div class="line">    <span class="keyword">return</span> class_label</div></pre></td></tr></table></figure><p> <strong>no obvious way</strong> to hard-code the algorithm for recognizing a cat, or other classes.</p><h3 id="Data-Driven-Approach"><a href="#Data-Driven-Approach" class="headerlink" title="Data-Driven Approach"></a>Data-Driven Approach</h3><ol><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(images, labels)</span>:</span></div><div class="line"><span class="comment"># Machine learning</span></div><div class="line"><span class="keyword">return</span> model</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, test_images)</span>:</span></div><div class="line"><span class="comment"># Use model to predict labels</span></div><div class="line">    <span class="keyword">return</span> test_labels</div></pre></td></tr></table></figure><p>Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One called <strong>train</strong>, that’s going to input images and labels and then output a model, another function called <strong>predict</strong>, which will input the model and make predictions for images.</p><p>#Nearest Neighbor classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span>:</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line"><span class="keyword">pass</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></div><div class="line"><span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></div><div class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></div><div class="line">    self.Xtr = X</div><div class="line">    self.ytr = y</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">     <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></div><div class="line">    num_test = X.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></div><div class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</div><div class="line">    </div><div class="line">    <span class="comment"># loop over all test rows</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</div><div class="line"><span class="comment"># find the nearest training image to the i'th test image</span></div><div class="line">     <span class="comment"># using the L1 distance (sum of absolute alue differences)</span></div><div class="line">        distances = np.sum(np.abs(self.Xtr - X[i, :]), axis = <span class="number">1</span>)</div><div class="line">        min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></div><div class="line">Ypred[i] = self.ytr[min_index] <span class="comment">#predict the label of the nearest example</span></div><div class="line">     <span class="keyword">return</span> Ypred</div></pre></td></tr></table></figure><p>Q: With N examples. how fast are training and prediction?</p><p>A: Train O(1), predict O(N)</p><p>This is bad: we want classifiers that are <strong>fast</strong> at prediciton; <strong>slow</strong> for training is ok.</p><h3 id="k-Nearest-Neighbors"><a href="#k-Nearest-Neighbors" class="headerlink" title="k-Nearest Neighbors"></a>k-Nearest Neighbors</h3><p>Instead of copying label from nearest neighbor, thake <strong>majority vote</strong> form K closest points.</p><p>###Hyperparameters</p><ul><li>What is the best value of <strong>k</strong> to use?</li><li>What is the best <strong>distance</strong> to use?</li></ul><p>These are <strong>hyperparameters</strong>: choices about the algorithm that we set rather than learn</p><p>_Very problem-dependent._</p><p>_Must try them all out and see what works best._</p><h3 id="Setting-Hyperparameters"><a href="#Setting-Hyperparameters" class="headerlink" title="Setting Hyperparameters"></a>Setting Hyperparameters</h3><ul><li>Split data into <strong>train</strong>, <strong>val</strong>, and <strong>test</strong>; choose hyperparameters on val and evaluate on test</li><li><strong>Cross-Validation</strong>: Split data into **folds, try each fold as validation and average the results. Useful for small datasets but not used too frequently in deep learning.</li></ul><h3 id="k-Nearest-Neighbor-on-images-never-used"><a href="#k-Nearest-Neighbor-on-images-never-used" class="headerlink" title="k-Nearest Neighbor on images never used"></a>k-Nearest Neighbor on images never used</h3><ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li><li>Curse of dimensionality</li></ul><h3 id="k-Nearest-Neighbors-Summary"><a href="#k-Nearest-Neighbors-Summary" class="headerlink" title="k-Nearest Neighbors: Summary"></a>k-Nearest Neighbors: Summary</h3><ul><li>In <strong>Image classification</strong> we start with a <strong>training set</strong> of images and labels, and must predict labels on the <strong>test set</strong></li><li>The *K-Nearest Neighbors classifier predicts labels based on nearest training examples</li><li>Distance metric and K are <strong>hyperparameters</strong></li><li>Choose hyperparameters using the <strong>validation set</strong>; only run on the test set once at the very end!</li></ul><p>#Linear Classification</p><p>These deep neural networks are kind of like Legos and this linear classifier is kind of like the most basic building blocks of these giant networks.</p><p>f(x, W) = Wx + b</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Problems&quot;&gt;&lt;a href=&quot;#Problems&quot; class=&quot;headerlink&quot; title=&quot;Problems:&quot;&gt;&lt;/a&gt;Problems:&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.&lt;/li&gt;
&lt;li&gt;Viewpoint variation: All pixels change when the camera moves&lt;/li&gt;
&lt;li&gt;Illumination: There can be lighting conditions going on in the scene&lt;/li&gt;
&lt;li&gt;Deformation: Cats can assume a lot of different, varied poses and positions.&lt;/li&gt;
&lt;li&gt;Occlusion: You might only see a part of a cat.&lt;/li&gt;
&lt;li&gt;Background Clutter: The foreground of the cat look similar in appearance&lt;/li&gt;
&lt;li&gt;Intraclass variation: Cats can come in different shapes and sizes and colors and ages&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记二：朴素贝叶斯</title>
    <link href="http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"/>
    <id>http://yoursite.com/2017/10/18/machine-learning-in-action-note2/</id>
    <published>2017-10-18T00:52:21.000Z</published>
    <updated>2018-01-21T08:35:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p><p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p><p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p><p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母 <strong>P(X=x)</strong> 对所有c~i~都没差，那我们大可不必计算出这个值。</p><h3 id="朴素贝叶斯学习与分类的算法过程："><a href="#朴素贝叶斯学习与分类的算法过程：" class="headerlink" title="朴素贝叶斯学习与分类的算法过程："></a>朴素贝叶斯学习与分类的算法过程：</h3><p>输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x</p><p>输出：实例x的分类</p><p>1) 计算先验概率和条件概率</p><p>$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$</p><p>$$<br>P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}<br>$$</p><p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p><p>2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算<br>$$<br>P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><p>3) 确定实例x的类<br>$$<br>y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><a id="more"></a><h3 id="一个栗子：邮件分类问题"><a href="#一个栗子：邮件分类问题" class="headerlink" title="一个栗子：邮件分类问题"></a>一个栗子：邮件分类问题</h3><p>公式都有了，到底要如何实现呢？我们通过邮件分类问题来引入解决办法。</p><p>问题：假设有很多个邮件，标记为<strong>1-垃圾邮件</strong>，<strong>0-正常邮件</strong>，给定一个实例判断它属于哪一类。</p><p>思路：首先在啥都没有的情况下，我们要处理邮件，将其转化为python可以理解的list，并为出现的所有单词构建一个词汇表，每封邮件对应为词汇表上的一个向量，所有的邮件构成一个矩阵，利用矩阵计算出先验概率和条件概率，将给定实例转化为向量，通过比较大小确定实例的类。</p><ol><li><p>处理邮件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</div><div class="line">        wordList = textParse(open(<span class="string">'email/spam/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>,<span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList) <span class="comment"># 文档合集</span></div><div class="line">        fullText.extend(wordList) <span class="comment"># 单词合集</span></div><div class="line">        classList.append(<span class="number">1</span>)</div><div class="line">        wordList = textParse(open(<span class="string">'email/ham/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>, <span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">0</span>)</div></pre></td></tr></table></figure></li><li><p>构建词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></div><div class="line">    vocabSet = set([])</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</div><div class="line">        vocabSet = vocabSet | set(document) <span class="comment"># 不重复的单词</span></div><div class="line">    <span class="keyword">return</span> list(vocabSet)</div><div class="line"> </div><div class="line">vocabList = vocabSet(docList)</div></pre></td></tr></table></figure></li><li><p>将所有邮件分为训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">trainIndex = list(range(<span class="number">50</span>)); testIndex = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    randIndex = int(random.uniform(<span class="number">0</span>, len(trainIndex)))</div><div class="line">    testIndex.append(trainIndex[randIndex])</div><div class="line">    <span class="keyword">del</span>(trainIndex[randIndex])</div></pre></td></tr></table></figure></li><li><p>将所有邮件转换为矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></div><div class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabList:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span> <span class="comment"># 转换为词汇表对应向量</span></div><div class="line">    <span class="keyword">return</span>  returnVec</div><div class="line"></div><div class="line">trainMat = []; trainClasses = []</div><div class="line"><span class="keyword">for</span> docIndex <span class="keyword">in</span> trainIndex:</div><div class="line">trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) </div><div class="line">    trainClasses.append(classList[docIndex])</div></pre></td></tr></table></figure></li><li><p>计算先验概率和条件概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></div><div class="line">    numTrainDocs = len(trainMatrix)</div><div class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</div><div class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs) <span class="comment"># 先验概率 p(class=1)</span></div><div class="line"></div><div class="line">    <span class="comment"># 1/ Initialize probabilities</span></div><div class="line">    <span class="comment"># 拉普拉斯平滑处理</span></div><div class="line">    p0Num = ones(numWords) <span class="comment"># p(xi|c0)</span></div><div class="line">    p1Num = ones(numWords) <span class="comment"># p(xi|c1)</span></div><div class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</div><div class="line"></div><div class="line">        <span class="comment"># 2 / Vector addition</span></div><div class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</div><div class="line">            p1Num += trainMatrix[i]</div><div class="line">            p1Denom += sum(trainMatrix[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            p0Num += trainMatrix[i]</div><div class="line">            p0Denom += sum(trainMatrix[i])</div><div class="line"></div><div class="line">    <span class="comment"># 3/ Element-wise division</span></div><div class="line">    <span class="comment"># 条件概率</span></div><div class="line">    p1Vect = log(p1Num / p1Denom)</div><div class="line">    p0Vect = log(p0Num / p0Denom)</div><div class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</div><div class="line"></div><div class="line">p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</div></pre></td></tr></table></figure><p>让我们把条件概率用普通话稍微翻译一下：<br>$$<br>P(X=x_i|c_k) = \frac{单词x_i出现次数}{c_k类文档总单词数}<br>$$<br>把所有的 \(P(X=x_i|c_k)\) 捋到一起就是一个向量 piVcet。</p><p>在上面的代码中我们其实处理了两种极端情况：</p><ol><li>早些时候这个代码是<code>p0Num = zeros(numWords); p0Denom =0</code>（p1同理）。如果某个单词没有出现，当计算乘积 ΠP(X=x~i~|c~k~)时，结果就会为0，这显然就没有办法进行预测了嘛。拉普拉斯就提出用加1的方法估计没有出现过的现象的概率。</li><li>还有一个是下溢出问题。在计算乘积  \(\prod P(X=x_i|c_k)\)时，由于概率都是很小的数值，程序有可能下溢出而得不到正确答案。解决办法就是对乘积取自然对数。</li></ol></li><li><p>交叉验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></div><div class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</div><div class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1</span>-pClass1)</div><div class="line">    <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    </div><div class="line">    errorCount = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testIndex:</div><div class="line">        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])</div><div class="line">classifyResult = classifyNB(array(wordVector), p0V, p1V, pSpam)</div><div class="line">    <span class="keyword">if</span>  classifyResult != classList[docIndex]:</div><div class="line">   errorCount += <span class="number">1</span></div><div class="line">print(<span class="string">"the error rate is: "</span>, float(errorCount)/len(testIndex))</div></pre></td></tr></table></figure></li></ol><p>这里我们终于要计算\(\prod P(X=x_i|c_k)\)了。<code>vec2Classify _ piVec</code> 表示先找出实例有的单词，由于\(\ln(a_b) = ln(a)+ln(b)\)，条件概率的相乘便转换为矩阵元素相加。</p><p>🙂 适用于量少的数据集／可以处理多类别问题</p><p>🙁 对输入数据的准备方式较敏感</p><p>🛠 标称型数据</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。&lt;/p&gt;
&lt;p&gt;假设有i个分类，我们需要比较的其实是后验概率 &lt;strong&gt;P(Y=c~k~|X=x)&lt;/strong&gt; 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。&lt;/p&gt;
&lt;p&gt;那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：&lt;br&gt;$$&lt;br&gt;P(A|B) = P(A)\frac{P(B|A)}{P(B)}&lt;br&gt;$$&lt;br&gt;让我们来代入一下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}&lt;br&gt;$$&lt;br&gt;给一个训练集，&lt;strong&gt;P(Y=c~i~)&lt;/strong&gt;是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：&lt;br&gt;$$&lt;br&gt;P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)&lt;br&gt;$$&lt;br&gt;由于分母 &lt;strong&gt;P(X=x)&lt;/strong&gt; 对所有c~i~都没差，那我们大可不必计算出这个值。&lt;/p&gt;
&lt;h3 id=&quot;朴素贝叶斯学习与分类的算法过程：&quot;&gt;&lt;a href=&quot;#朴素贝叶斯学习与分类的算法过程：&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯学习与分类的算法过程：&quot;&gt;&lt;/a&gt;朴素贝叶斯学习与分类的算法过程：&lt;/h3&gt;&lt;p&gt;输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x&lt;/p&gt;
&lt;p&gt;输出：实例x的分类&lt;/p&gt;
&lt;p&gt;1) 计算先验概率和条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算&lt;br&gt;$$&lt;br&gt;P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;3) 确定实例x的类&lt;br&gt;$$&lt;br&gt;y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记一：kNN和决策树</title>
    <link href="http://yoursite.com/2017/10/15/machine-learning-in-action-note1/"/>
    <id>http://yoursite.com/2017/10/15/machine-learning-in-action-note1/</id>
    <published>2017-10-15T13:58:12.000Z</published>
    <updated>2017-11-06T14:02:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。</p><p>算法的一般流程为：收集数据 -&gt; 准备数据 -&gt; 分析数据 -&gt; 训练算法 -&gt; 测试算法 -&gt; 使用算法</p><p>第一个笔记本包括kNN分类算法和决策树算法。</p><h1 id="kNN分类算法"><a href="#kNN分类算法" class="headerlink" title="kNN分类算法"></a>kNN分类算法</h1><p>kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。</p><h3 id="kNN的一般流程"><a href="#kNN的一般流程" class="headerlink" title="kNN的一般流程"></a>kNN的一般流程</h3><ol><li>收集数据</li><li>准备数据：最好使用结构化数据格式，因为计算距离需要数值。</li><li>分析数据</li><li>训练算法：此步骤不适用于kNN算法</li><li>测试算法：计算错误率</li><li>使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。</li></ol><a id="more"></a><p>我们一般使用欧氏距离公式来计算距离（以两点为例）：</p><p>$$<br>d = \sqrt{(x_0-y_0)^2+(x_1-y_1)^2+…+(x_n-y_n)^2)}<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div></pre></td></tr></table></figure><p>对距离进行排序，并找出最小的k个距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line">classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k): </div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div></pre></td></tr></table></figure><p>最后对出现次数进行排序，预测结果即为出现次数最多的类别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line"><span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>原书中sorted的第一个参数为<code>classCount.iteritems</code>，根据python3作出修改。</p><p>完整的分类函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></div><div class="line">    <span class="string">"""k-Nearest Neighbors algorithm</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    :param inX: A row vector under test</span></div><div class="line"><span class="string">    :param dataSet: The training data</span></div><div class="line"><span class="string">    :param labels: Labels of training data</span></div><div class="line"><span class="string">    :return: Prediction for the class of inX</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Distance calculation</span></div><div class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div><div class="line">    sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line"></div><div class="line">    <span class="comment"># 2 Voting with lowest k distances</span></div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div><div class="line"></div><div class="line">    <span class="comment"># 3 Sort dictionary</span></div><div class="line">    sortedClassCount = sorted(classCount.items(),</div><div class="line">                              key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>🙂：准确度高／对异常值不敏感／不假设数据</p><p>🙁：计算复杂度高／占用大量内存</p><p>🛠：数值型／标称型</p><h1 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h1><p>决策树算法（Decision trees）也是一种常见的分类算法。我们每次用一个特征来对数据集进行分类，迭代直到分出的数据集都属于一个类别时，训练完成，训练模型即为一个树结构。</p><h3 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h3><ol><li>数据收集</li><li>准备数据：这个构造树的过程只适用于标称型数据，因此需要离散化连续的数值</li><li>分析数据</li><li>训练算法：构造一个树的数据结构</li><li>测试算法：使用经验树计算错误率</li><li>使用算法：这可以应用于任何监督学习任务。通常，决策树树可以更好的理解数据的内在含义。</li></ol><p>判断使用哪一个特征来进行分类即为训练算法的关键。在这里，我们使用特征的「信息增益」来判断。好了，我们得来复习一下「信息增益」。</p><p>说到信息增益就有一个不得不提的家伙叫「熵」，它是随机变量的平均量，代表了随机变量的不确定性。啥玩意儿？？？一个事件发生的概率越小，所含的信息量就越大，如果所有事件发生的概率都很小，平均信息量就比较大。讲人话。「熵」越大，数据集越混乱。</p><p>用公式来表示：<br>$$<br>H = -\sum_{i=1}^np(x_i)log_2p(x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></div><div class="line">    numEntries = len(dataSet)</div><div class="line"></div><div class="line">    labelCounts = &#123;&#125; <span class="comment"># 用一个字典记录一个类别的出现次数</span></div><div class="line">    <span class="keyword">for</span> featVect <span class="keyword">in</span> dataSet:</div><div class="line">        currentLabel = featVect[<span class="number">-1</span>]</div><div class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</div><div class="line">            labelCounts[currentLabel] = <span class="number">0</span></div><div class="line">        labelCounts[currentLabel] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># 计算熵</span></div><div class="line">    shannonEnt = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">        prob = float(labelCounts[key])/numEntries <span class="comment"># 概率</span></div><div class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> shannonEnt</div></pre></td></tr></table></figure><p>还有一个家伙叫「条件熵」，是已知条件下，随机变量的不确定性。即当我们固定了一个特征时，整个系统的信息量。然而这个特征的取值也不止一个，因此我们就需要求出它的平均值。</p><p>用公式来表示：<br>$$<br>H(C|X) = p_1H(C|x_1)+p_2(C|x_2)+…p_n(C|x_n) = \sum_{i=1}^np_iH(C|x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line">featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">newEntropy = <span class="number">0</span></div><div class="line">    </div><div class="line">    <span class="comment"># 计算条件熵</span></div><div class="line"><span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">      subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">   prob = len(subDataSet)/float(len(dataSet))</div><div class="line">   newEntropy += prob * calcShannonEnt(subDataSet)</div></pre></td></tr></table></figure><p>接着又来了一个家伙，它就是「信息增益」。<strong>信息增益 = 熵 - 条件熵</strong>。这个信息增益代表着，系统固定一个特征后，<strong>不确定性的减少程度</strong>。我们当然希望这个减少程度尽可能大，使得系统更加有组织纪律，那么信息增益很大就表明这个特征很关键！！</p><p>我们将上面的内容整理进一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></div><div class="line">    </div><div class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment"># 获取特征的数量</span></div><div class="line">    baseEntropy = calcShannonEnt(dataSet)</div><div class="line">    bestInfoGain = <span class="number">0</span>; bestFeature = <span class="number">-1</span></div><div class="line">    </div><div class="line">    <span class="comment"># 分别计算每一个特征的信息增益</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line"></div><div class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">        uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">        newEntropy = <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># 计算条件熵</span></div><div class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">            subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">            prob = len(subDataSet)/float(len(dataSet))</div><div class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line">        <span class="comment"># 计算信息增益</span></div><div class="line">        infoGain = baseEntropy - newEntropy</div><div class="line">        </div><div class="line">        <span class="comment"># 找到最大的信息增益</span></div><div class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line">    <span class="keyword">return</span>  bestFeature <span class="comment"># 返回最佳分类特征</span></div></pre></td></tr></table></figure><p>整理到这里，其实有一个疑惑，我觉得信息增益并没有什么卵用，每次找出条件熵最小的不就行了？？？经实验是可以的。</p><p>在treePlotter.py中发现一个神奇的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></div><div class="line">    <span class="comment"># 2 Draws annotations with arrows</span></div><div class="line">    createPlot.ax.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           xytext = centerPt, textcoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodeType, arrowprops=arrow_args)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></div><div class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</div><div class="line">    fig.clf()</div><div class="line">    createPlot.ax = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>)</div><div class="line">    plotNode(<span class="string">'a decision node'</span>, (<span class="number">.5</span>, <span class="number">.1</span>), (<span class="number">.1</span>, <span class="number">.5</span>), decisionNode)</div><div class="line">    plotNode(<span class="string">'a leaf node'</span>, (<span class="number">.8</span>, <span class="number">.1</span>), (<span class="number">.3</span>, <span class="number">.8</span>), leafNode)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Python中一切皆为对象，在createPlot函数中，为这个函数对象绑定了一个属性ax，变成了一个全局的变量，可以在plotNode函数中调用。</p><p>书中作者利用treePlotter中写好的树结构来进行预测，但tree.py里头不有一个createDataSet函数和createTree函数吗？尝试一下利用这两个函数来生成并预测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels)</div></pre></td></tr></table></figure><p>Buuuuuuuut……….</p><p><img src="/2017/10/15/machine-learning-in-action-note1/no_surfacing_error.png" alt=""></p><p>检查了一下调用createTree函数前后labels的值，发现调用前后labels的值发生了变化，因此对原函数createTree稍作修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></div><div class="line"></div><div class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Stop when all classes are equal</span></div><div class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</div><div class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 2 When just one feature, return majority</span></div><div class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> majorityCnt(classList)</div><div class="line"></div><div class="line">    subLabels = labels[:] <span class="comment"># 将labels全部复制到subLabels，进行接下来的处理</span></div><div class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">    bestFeatLabel = subLabels[bestFeat] <span class="comment"># 利用subLabels来获取最佳分类特征</span></div><div class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</div><div class="line">    <span class="keyword">del</span>(subLabels[bestFeat])</div><div class="line"></div><div class="line">    <span class="comment"># 3 Get list of unique values</span></div><div class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">    uniqueVals = set(featValues)</div><div class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">        <span class="comment">#subLabels = labels[:] 作者在这里才进行参数复制，导致labels的值发生改变</span></div><div class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</div><div class="line">    <span class="keyword">return</span> myTree</div></pre></td></tr></table></figure><p>接下来就可以愉快利用该函数进行分类了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> treePlotter</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">省略一堆作者源代码</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels) <span class="comment"># 训练数据</span></div><div class="line">treePlotter.createPlot(myTree) <span class="comment"># 显示模型</span></div><div class="line">print(classify(myTree, labels, [<span class="number">1</span>, <span class="number">1</span>])) <span class="comment"># 预测数据</span></div></pre></td></tr></table></figure><p>看一看生成的决策树模型：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/tree1.png" alt=""></p><p>我们可以引入pickle模块来将训练出的模型序列化，保存在磁盘中，以便后续的调用。因为书本作者是使用Python2的，我打算用Python3来完成，在下面的代码中就遇到了问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></div><div class="line">    <span class="keyword">import</span> pickle</div><div class="line">    <span class="comment">#Python2用法：fw = open(filename,'w')</span></div><div class="line">    <span class="comment">#改为python3:</span></div><div class="line">    <span class="keyword">with</span> open(filename,<span class="string">'wb'</span>) <span class="keyword">as</span> fw:</div><div class="line">        pickle.dump(inputTree, fw)</div><div class="line">    fw.close()</div></pre></td></tr></table></figure><p>接下来就要引入稍微大一点的数据集来进行训练了，然鹅….</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fr = open(<span class="string">'lenses.txt'</span>)</div><div class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readline()]</div><div class="line">lensesLables = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</div><div class="line">print(lenses)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/readlines_error.png" alt=""></p><p>这输出的啥玩意儿？仔细一看，<strong>fr.readlines()</strong>函数写错了，少了一个<strong>sssssssss</strong>。修改好以后我们就来看看训练完的决策树吧：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree1.png" alt=""></p><p>？？？这又啥玩意儿？？这看得下去？？？？那就只能修改一下plotMidText函数了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></div><div class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>])/<span class="number">2</span> + cntrPt[<span class="number">0</span>]</div><div class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>])/<span class="number">2</span> + cntrPt[<span class="number">1</span>]</div><div class="line">    createPlot.ax.text(xMid, yMid, txtString, fontsize=<span class="number">8</span>, horizontalalignment=<span class="string">'center'</span>,verticalalignment=<span class="string">'center'</span>, rotation=<span class="number">30</span>)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree2.png" alt=""></p><p>完美！</p><p>🙂：计算复杂度不高／便于人们理解学习结果／对中间的缺失值不敏感／可以处理无关的特征值</p><p>🙁：可能会过拟合</p><p>🛠：数值型／标称型</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。&lt;/p&gt;
&lt;p&gt;算法的一般流程为：收集数据 -&amp;gt; 准备数据 -&amp;gt; 分析数据 -&amp;gt; 训练算法 -&amp;gt; 测试算法 -&amp;gt; 使用算法&lt;/p&gt;
&lt;p&gt;第一个笔记本包括kNN分类算法和决策树算法。&lt;/p&gt;
&lt;h1 id=&quot;kNN分类算法&quot;&gt;&lt;a href=&quot;#kNN分类算法&quot; class=&quot;headerlink&quot; title=&quot;kNN分类算法&quot;&gt;&lt;/a&gt;kNN分类算法&lt;/h1&gt;&lt;p&gt;kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。&lt;/p&gt;
&lt;h3 id=&quot;kNN的一般流程&quot;&gt;&lt;a href=&quot;#kNN的一般流程&quot; class=&quot;headerlink&quot; title=&quot;kNN的一般流程&quot;&gt;&lt;/a&gt;kNN的一般流程&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;收集数据&lt;/li&gt;
&lt;li&gt;准备数据：最好使用结构化数据格式，因为计算距离需要数值。&lt;/li&gt;
&lt;li&gt;分析数据&lt;/li&gt;
&lt;li&gt;训练算法：此步骤不适用于kNN算法&lt;/li&gt;
&lt;li&gt;测试算法：计算错误率&lt;/li&gt;
&lt;li&gt;使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>来生成一个中国地图吧</title>
    <link href="http://yoursite.com/2017/07/25/china-map/"/>
    <id>http://yoursite.com/2017/07/25/china-map/</id>
    <published>2017-07-25T02:49:22.000Z</published>
    <updated>2017-11-06T14:07:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>第一部分应该是根据作者2012年的文章进行的，第二部分根据作者2016年新的文章，在命令行显示人口密度。</p><a id="more"></a><h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><h3 id="1-获取地图文件并转换格式"><a href="#1-获取地图文件并转换格式" class="headerlink" title="1/ 获取地图文件并转换格式"></a>1/ 获取地图文件并转换格式</h3><p>首先据教程<a href="https://sandbox.idre.ucla.edu/sandbox/tutorials/installing-gdal-for-windows" target="_blank" rel="external">https://sandbox.idre.ucla.edu/sandbox/tutorials/installing-gdal-for-windows</a> 安装GDAL，并添加环境变量，要利用GDAL的org2org来转换文件。根据参考链接1，下载好全球地图以后，命令行运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ogr2ogr -f GeoJSON -where &quot;SU_A3 = &apos;CHN&apos; OR SU_A3 = &apos;TWN&apos;&quot; countries.json ne_10m_admin_0_countries.shp</div></pre></td></tr></table></figure><p>获取我们需要的城市，包括大陆和台湾。-where后面是我们需要筛选的条件，根据参考链接中国家的代码，中国是CHN台湾是TWN。</p><p>接下来是获取省份，命令行运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ogr2ogr -f GeoJSON -where &quot;gu_a3 = &apos;CHN&apos;&quot; provinces_china.json ne_10m_admin_1_states_provinces.shp</div></pre></td></tr></table></figure><h3 id="2-压缩文件"><a href="#2-压缩文件" class="headerlink" title="2/ 压缩文件"></a>2/ 压缩文件</h3><p>链接2给出了一个topojson的方法，不过我npm安装完里没有这玩意儿，仔细看了一下文章说是把GeoJSON文件格式转化为TopoJSON。有geo2topo这个方法，于是运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">geo2topo --id-propety SU_A3 -p name=NAME -p name -o countries_china_topo.json countries.json</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">geo2topo --id-propety SU_A3 -p name=NAME -p name -o provinces_china_topo.json provinces_china.json</div></pre></td></tr></table></figure><p>这个文件格式还是挺大的，继续上<a href="http://mapshaper.org/" target="_blank" rel="external">http://mapshaper.org/</a> 网站压缩，导出后可以看到结尾mini是最小的。</p><p><img src="/2017/07/25/china-map/cmp_countreis.png" alt=""></p><p><img src="/2017/07/25/china-map/cmp_provinces.png" alt=""></p><h3 id="3-敲代码"><a href="#3-敲代码" class="headerlink" title="3/ 敲代码"></a>3/ 敲代码</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;<span class="selector-tag">style</span>&gt;</div><div class="line">  <span class="selector-id">#china</span> &#123;</div><div class="line">    <span class="attribute">stroke</span>: <span class="number">#fff</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="selector-class">.province</span> &#123;</div><div class="line">    <span class="attribute">fill</span>: <span class="number">#cde</span>;</div><div class="line">    <span class="comment">/* stroke-width: 1; */</span></div><div class="line">    <span class="attribute">stroke-width</span>: <span class="number">2px</span>;</div><div class="line">    <span class="attribute">cursor</span>: pointer;</div><div class="line">  &#125;</div><div class="line">  <span class="selector-class">.province</span><span class="selector-pseudo">:hover</span> &#123;</div><div class="line">    <span class="attribute">fill</span>: lightblue;</div><div class="line">  &#125;</div><div class="line">&lt;/style&gt;</div></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> width = <span class="built_in">window</span>.innerWidth,</div><div class="line">    height = <span class="built_in">window</span>.innerHeight,</div><div class="line">    countreis,</div><div class="line">    state</div><div class="line"></div><div class="line"><span class="keyword">var</span> projection = d3.geoMercator()</div><div class="line">  .center([<span class="number">104</span>, <span class="number">36</span>])</div><div class="line">  .scale(<span class="number">850</span>)</div><div class="line">  .translate([width / <span class="number">2</span>, height / <span class="number">2</span>])</div><div class="line"></div><div class="line"><span class="keyword">var</span> path = d3.geoPath()</div><div class="line">    .projection(projection)</div><div class="line"></div><div class="line"><span class="keyword">var</span> zoom = d3.zoom()</div><div class="line">    .scaleExtent([<span class="number">0.5</span>, <span class="number">10</span>])</div><div class="line">    .on(<span class="string">'zoom'</span>, zoomed)</div><div class="line"></div><div class="line"><span class="keyword">var</span> svg = d3.select(<span class="string">'.china-map'</span>)</div><div class="line">    .attr(<span class="string">"width"</span>, width)</div><div class="line">    .attr(<span class="string">"height"</span>, height)</div><div class="line">    .call(zoom)</div><div class="line"></div><div class="line"><span class="keyword">var</span> g = svg.append(<span class="string">'g'</span>)</div><div class="line"></div><div class="line">d3.json(<span class="string">'./provinces_china_mini.json'</span>, (err, chn) =&gt; &#123;</div><div class="line">  <span class="keyword">if</span>(err) <span class="built_in">console</span>.log(error(err))</div><div class="line">  <span class="built_in">console</span>.log(chn)</div><div class="line"></div><div class="line">  g.append(<span class="string">'g'</span>)</div><div class="line">    .attr(<span class="string">'id'</span>, <span class="string">'china'</span>)</div><div class="line">    <span class="comment">// .attr('id', 'provinces')</span></div><div class="line">    .selectAll(<span class="string">'path'</span>)</div><div class="line">    .data(topojson.feature(chn, chn.objects.provinces_china).features)</div><div class="line">    .enter()</div><div class="line">    .append(<span class="string">'path'</span>)</div><div class="line">    .attr(<span class="string">'class'</span>, <span class="string">'province'</span>)</div><div class="line">    .attr(<span class="string">'d'</span>, path)</div><div class="line">    .on(<span class="string">'click'</span>, clicked)</div><div class="line">    </div><div class="line">  g.selectAll(<span class="string">'.provinces-label'</span>)</div><div class="line">    .data(topojson.feature(chn, chn.objects.provinces_china).features)</div><div class="line">    .enter().append</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">zoomed</span>(<span class="params"></span>) </span>&#123;</div><div class="line">  g.attr(<span class="string">'transform'</span>, <span class="string">`translate(<span class="subst">$&#123;d3.event.transform.x&#125;</span>, <span class="subst">$&#123;d3.event.transform.y&#125;</span>) scale(<span class="subst">$&#123;d3.event.transform.k&#125;</span>)`</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>第一部分参考链接：</p><ol><li><a href="https://yukun.im/javascript/533" target="_blank" rel="external">https://yukun.im/javascript/533</a></li><li><a href="http://www.tnoda.com/blog/2013-12-07" target="_blank" rel="external">Interactive Map with d3.js</a></li><li><a href="https://msdn.microsoft.com/en-us/library/ee783932(v=cs.10" target="_blank" rel="external">Table of Country/Region Names and Codes</a>.aspx)</li></ol><h1 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h1><p>利用<strong>ndjson-reduce</strong>和<strong>ndjson-map</strong>把转化为GeoJSON格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ndjson-reduce &lt; ca-albers-density.ndjson | ndjson-map &quot;&#123;type: &apos;FeatureCollection&apos;, features: d&#125;&quot; &gt; ca-albers-densi ty.json</div></pre></td></tr></table></figure><p>（没想到windows这么看重单双引号</p><p>看看转换后的地图：</p><p><img src="/2017/07/25/china-map/ca-albers-density.png" alt=""></p><p>好像和原先的地图。。。倒了？？？</p><p>接下来安装d3<code>npm install -g d3</code>，通过<code>-r d3</code>引入d3模块，使用 Viridis 主体颜色来填充地区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ndjson-map -r d3 &quot;(d.properties.fill = d3.scaleSequential(d3.interpolateViridis).domain([0, 4000])(d.properties.de nsity), d)&quot; &lt; ca-albers-density.ndjson &gt; ca-albers-color.ndjson</div></pre></td></tr></table></figure><p>把上面生成的GeoJSON文件转化成SVG</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure><p>第二部分参考链接：</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一部分应该是根据作者2012年的文章进行的，第二部分根据作者2016年新的文章，在命令行显示人口密度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Tensorflow初体验</title>
    <link href="http://yoursite.com/2017/06/20/Tensorflow-test/"/>
    <id>http://yoursite.com/2017/06/20/Tensorflow-test/</id>
    <published>2017-06-20T02:49:03.000Z</published>
    <updated>2017-10-21T09:38:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>谷歌开源了物体识别系统的API，开源地址：<a href="https://github.com/tensorflow/models/tree/master/object_detection" target="_blank" rel="external">Tensor Flow Object Detectoin</a>，刚好最近也在学ML，学习了python，对sklearn的库也有一丢丢了解，来感受一下google的技术~</p><p>&lt;–more–&gt;</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>先抛出<a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/installation.md" target="_blank" rel="external">官方安装教程</a>，但是这个安装过程不太完整，比如 protobuf 的安装等，于是自己记录并做一些补充。</p><ol><li><p>安装 Tensorflow 包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow</div></pre></td></tr></table></figure></li><li><p>把 github 的 tensorflow/models 克隆下来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure></li><li><p>安装 google protobuf</p><p><strong>Protobuf</strong> 是 google 开发的一种数据结构。它提供了一种灵活、高效、自动序列化结构数据的机制，可以联想 XML，但是比 XML 更小、更快、更简单。</p><p>安装链接：<a href="https://github.com/google/protobuf/releases/tag/v3.3.0。我下载了" target="_blank" rel="external">https://github.com/google/protobuf/releases/tag/v3.3.0。我下载了</a> protoc-3.3.0-win32.zip，专供不想自己配置的懒逼下载，载完后直接将bin目录添加到环境变量PATH中即可。</p></li><li><p>编译 protobuf 库文件</p><p>在 <strong>tensorflow/models</strong> 文件夹下运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure></li><li><p>将目录添加到PYTHONPATH中</p><p>windows下没有export命令，根据文字描述，要把 <strong>slim</strong> 文件夹和 <strong>tensorflow/models/</strong> 和添加到 <strong>PATHONPATH</strong> 中。但是之前我的python目录是直接添加在path中的，于是先创建一个名为PAYTHONPATH的变量：</p><p>&lt;% asset_img pythonpath.png %&gt;</p><p>把关于python的统统添加到这个变量中，以及刚才所说的 slim 和 models：</p><p>&lt;% asset_img pythonpath.png %&gt;</p><p>再把 <strong>%PYTHONPATH%</strong> 添加到 <strong>PATH</strong> 中：</p><p>&lt;% asset_img path.png %&gt;</p></li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>​    运行<code>python object_detection/builders/model_builder_test.py</code></p><p>​    <img src="/2017/06/20/Tensorflow-test/2017/06/20/Tensorflow-test/ok.png" alt="ok.png" title=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;谷歌开源了物体识别系统的API，开源地址：&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/object_detection&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tensor Fl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex8</title>
    <link href="http://yoursite.com/2017/06/16/machine-learning-ex8/"/>
    <id>http://yoursite.com/2017/06/16/machine-learning-ex8/</id>
    <published>2017-06-16T11:46:41.000Z</published>
    <updated>2017-10-21T09:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。</p><p>这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。</p><a id="more"></a><h3 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h3><p>首先计算出 <strong>μ</strong> 和 <strong>σ^2^ </strong>：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mu = mean(X)';</div><div class="line">sigma2 = var(X)'*(m<span class="number">-1</span>)/m;</div></pre></td></tr></table></figure><p>在这里要注意函数 var() 除以的是m-1所以我们要修改一下函数。然后我们要利用交叉验证样本，计算 F~1~ Score 并挑选临界值 <strong>ε</strong>：</p> <figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 获取验证集异常值坐标</span></div><div class="line">cvPredictions = (pval &lt; epsilon);</div><div class="line">tp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line">fp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">0</span>) );</div><div class="line">fn = sum( (cvPredictions == <span class="number">0</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line"><span class="comment">% 计算精确率</span></div><div class="line">prec = tp / (tp + fp);</div><div class="line"><span class="comment">% 计算召回率</span></div><div class="line">rec = tp / (tp + fn);</div><div class="line">F1 = <span class="number">2</span> * prec * rec / (prec + rec);</div></pre></td></tr></table></figure><p>然后可以看到红红的圈出的异常值：</p><img src="/2017/06/16/machine-learning-ex8/2017/06/16/machine-learning-ex8/detected.png" alt="detected.png" title=""><h3 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h3><p>说到推荐系统，以电影为例，一方面要预测用户对于某电影的评分，另一方面要寻找相似的电影，我们经常使用的算法是协同过滤算法。进一步了解这个算法，查了一些中文资料。原本的线性回归，我们只需要根据特征值计算出参数 <strong>θ</strong>，但是现在变态了，我们不光要预测用户的喜好，还要查找相似的特征向量，俩参数（都用矩阵表示）一起学习。</p><p>首先我们完成未正规化的梯度和代价函数的计算：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_grad = (X * Theta' - Y) .* R * Theta;</div><div class="line">Theta_grad = (X * Theta' - Y)' .* R' * X;</div><div class="line"></div><div class="line">J = sum(( (X * Theta' - Y).^<span class="number">2</span> .* R )(:)) / <span class="number">2</span>;</div></pre></td></tr></table></figure><p>期间完全忘记梯度是什么鬼。。回顾一下，是代价函数对变量求偏导~ 计算公式完全按照矩阵的大小来判断。接下来我们正规化代价函数，按照公式加上<code>J += lambda / 2 _ sum(Theta.^2(:)) + lambda / 2 _ sum(X.^2(:));</code>，但发现J变成了1x3的向量，发现问题在于sum中应该在平方的时候添加括号，修改代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J += lambda / <span class="number">2</span> * sum((Theta.^<span class="number">2</span>)(:)) + lambda / <span class="number">2</span> * sum((X.^<span class="number">2</span>)(:));</div></pre></td></tr></table></figure><p>然后继续完成梯度的正规化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。&lt;/p&gt;
&lt;p&gt;这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Mmachine Learning ex7</title>
    <link href="http://yoursite.com/2017/06/13/machine-learning-ex7/"/>
    <id>http://yoursite.com/2017/06/13/machine-learning-ex7/</id>
    <published>2017-06-13T01:46:37.000Z</published>
    <updated>2017-06-15T04:38:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章主要学习了 <strong>K-均值算法</strong> 和 <strong>PCA 算法</strong>，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。</p><a id="more"></a><h3 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h3><p>我们先使用2维的数据集来感受一下K均值算法。接着我们要将写好的函数运用到图像压缩上。K-均值算法最核心的步骤如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Initialize centroids</span></div><div class="line">centroids = kMeansInitCentroids(X, K);</div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:iterations</div><div class="line"><span class="comment">% Cluster assignment step: Assign each data point to the</span></div><div class="line"><span class="comment">% closest centroid. idx(i) corresponds to cˆ(i), the index</span></div><div class="line"><span class="comment">% of the centroid assigned to example i</span></div><div class="line">idx = findClosestCentroids(X, centroids);</div><div class="line"><span class="comment">% Move centroid step: Compute means based on centroid</span></div><div class="line"><span class="comment">% assignments</span></div><div class="line">centroids = computeMeans(X, idx, K);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>第一步完成 <strong>findClosesCentroids</strong> 函数，计算每个样本到中心点的距离，用数值表示其所属类，返回聚类后的向量：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(X,<span class="number">1</span>)</div><div class="line">  min = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="number">1</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">  min_idx = <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">2</span> : K</div><div class="line">    cur = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="built_in">j</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">    <span class="keyword">if</span>(cur &lt; min)</div><div class="line">      min = cur;</div><div class="line">      min_idx = <span class="built_in">j</span>;</div><div class="line">    <span class="keyword">end</span></div><div class="line">  <span class="keyword">end</span></div><div class="line">  idx(<span class="built_in">i</span>) = min_idx;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>题解里说用一个for循环完成，但我先使用了两个，后面再进行优化好了。这其中出现了一个小问题，计算norm的时候，<code>norm(X(i,:) - centroids(j,:), 2).^2;</code> 主要包含所有列，否则只包含了单个数字。</p><p>接下来完成 <strong>computeMeans</strong> 函数，通过同个类里的样本计算新的中心：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : K</div><div class="line">  <span class="comment">% 查找聚类样本</span></div><div class="line">  examples_idx = <span class="built_in">find</span>(idx == <span class="built_in">i</span>);</div><div class="line">  <span class="comment">% 计算中心值</span></div><div class="line">  X(examples_idx, :);</div><div class="line">  centroids(<span class="built_in">i</span>,:) = sum( X(examples_idx, :) ) / <span class="built_in">size</span>(examples_idx, <span class="number">1</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>这里出现的问题在<code>centroids(i,:) = sum( X(examples_idx, :) ) / size(examples_idx, 1);</code> 。注意包含centroids的所有列，和在计算szie()的时候，选择所有行的大小。</p><p>然后我们就可以运行我们的K-均值算法来聚类了。可以看到初始化的时候三个中心点分别为（3,3）、（6,2）、（8,5）。</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter1.png" alt="iter1.png" title=""><p>经过6次迭代可以看到中心点逐渐往好的方向移动：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter6.png" alt="iter6.png" title=""><p>经过10次迭代基本到达有模有样的聚类中心了：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter10.png" alt="iter10.png" title=""><p>接下来我们要使用K均值算法来压缩图像了。每个像素用 <strong>24-bit</strong> 来表示颜色，即3个 <strong>8-bit</strong> 的无符号整形来表示RGB的值，目标压缩成用<strong>16-bit</strong> 来表示。</p><p>我们使用 <strong>imread(A)</strong> 来读入图像，因为我们原始图像的大小为128x128，因此 A 为一个三维矩阵，<strong>size(A) = 128x128x3</strong> ，然后reshape图像变为 <strong>mX3</strong>（m = 16384 = 128x128） 的矩阵，用于后面执行K-均值算法。</p><p>我们将K设置为16，对每个像素点进行聚类。获得最后的图像。原始图像需要 <strong>128 x 128 x 24 = 393,216</strong> bits。而压缩后我们使用 24bits存储16种颜色，但每个像素点只需要4bits 来进行定位。因此压缩后的图像为 <strong>16 x 24+128x128x4 = 65,920</strong> bits。（从pdf里理解是这样，然而真实的情况似乎并没有进行压缩，看了助教在论坛里的回答，完整的压缩过程还要创建一个新的图像，只使用4bit来表示，因为本课程只是关于聚类，而不是关于压缩图像的细节。）因此压缩后的图片如下：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress.png" alt="compress.png" title=""><p>然后我们换一张自己的图片来看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress2.png" alt="compress2.png" title=""><p>然后修改 <strong>K=5</strong> 再看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress3.png" alt="compress3.png" title=""><h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h3><p>我们使用PCA来减少数据集的维度。先实验2D的数据集来了解一下PCA的运作，再将其运用更高的维度上。</p><p>PCA的步骤为：1、正规化数据集。2、计算协方差矩阵。3、利用SVD函数计算主成分（U、S）。</p><p>2、3步骤实现</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sigma = X' * X / m;</div><div class="line">[U, S, V] = svd(Sigma);</div></pre></td></tr></table></figure><p>可以看到特征向量如下图：</p><p><strong>特征向量1</strong></p><p><strong>特征向量1 &amp; 2</strong></p><p>因为我们要降低到一维，即 <strong>K=1</strong> 因此在这次只会使用到U(:, 1)</p><p>通过SVD函数计算出了主成分后，我们可以就利用特征向量U来将每个样本映射为更低的维度，x^(i)^ -&gt; z^(i)^ 。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">U_reduce = U(:, <span class="number">1</span>:K);</div><div class="line">Z = X * U_reduce;</div></pre></td></tr></table></figure><p>重构出通过PCA映射后的点（红色）：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/reconstruction.png" alt="reconstruction.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章主要学习了 &lt;strong&gt;K-均值算法&lt;/strong&gt; 和 &lt;strong&gt;PCA 算法&lt;/strong&gt;，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex6</title>
    <link href="http://yoursite.com/2017/06/10/machine-learning-ex6/"/>
    <id>http://yoursite.com/2017/06/10/machine-learning-ex6/</id>
    <published>2017-06-10T01:58:07.000Z</published>
    <updated>2017-10-21T09:38:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>终于来到支持向量机（SVM）了！这一章我们要学习如何使用高斯核SVM来建立一个垃圾邮件分类器。</p><a id="more"></a><h3 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h3><p>首先显示出我们的数据，很明显地可以看出 <strong>C=1</strong> 时两个分类的决策边界是中间的一条线，但是要注意到左边有一个 <strong>+</strong> 在（0.1,4.1）。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c1.png" alt="dataset1_c1.png" title=""><p>增大C，修改 <strong>C=100</strong> 可以看到决策边界的偏差减小了。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c100.png" alt="dataset1_c100.png" title=""><p>继续增大C，此时 <strong>C=1000</strong> ，</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c1000.png" alt="dataset1_c1000.png" title=""><p>根据公式即可完成高斯核的代码，没有出现什么大问题。接下来出现了一个样本较大且是非线性的决策边界的数据集，我们利用已完成的高斯核SVM来显示决策边界。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset2.png" alt="dataset2.png" title=""><p>通过训练集3我们要通过验证集来找到最佳的 C 和 σ，最初的决策分界如下图：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset3_base.png" alt="dataset3_base.png" title=""><p>经过8864的循环终于得到了最优参数 :-)</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">min = <span class="number">1</span>;</div><div class="line">step = [<span class="number">0.01</span>; <span class="number">0.03</span>; <span class="number">0.1</span>; <span class="number">0.3</span>; <span class="number">1</span>; <span class="number">3</span>; <span class="number">10</span>; <span class="number">30</span>];</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: <span class="built_in">size</span>(step)</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>: <span class="built_in">size</span>(step)</div><div class="line">    model= svmTrain(X, y, step(<span class="built_in">i</span>), @(x1, x2) gaussianKernel(x1, x2, step(<span class="built_in">j</span>)));</div><div class="line">    predictions = svmPredict(model, Xval);</div><div class="line">    cur = mean(double(predictions ~= yval));</div><div class="line">    <span class="keyword">if</span>(cur &lt; min) </div><div class="line">      min = cur;</div><div class="line">      C = step(<span class="built_in">i</span>);</div><div class="line">      sigma = step(<span class="built_in">j</span>);</div><div class="line">    <span class="keyword">end</span> </div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset3_over.png" alt="dataset3_over.png" title=""><h3 id="Spam-Classification"><a href="#Spam-Classification" class="headerlink" title="Spam Classification"></a>Spam Classification</h3><p>在处理垃圾邮件的过程中，需要对邮件进行标准化。包括全部转换为小写字母、移除HTML标签、以及一些表示具体含义的内容（如链接、邮件、符号、数字等）替换为固定的字符串。</p><p>接下来我们的任务是将邮件中出现的单词映射为词典中的索引：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">length</span>(vocabList)</div><div class="line">  <span class="keyword">if</span>(strcmp(str, vocabList&#123;i&#125;))</div><div class="line">      word_indices = [word_indices; i];</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>后来在查看练习资料的时候发现，可以使用<code>ismember()</code>函数来实现同样功能：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[tf, idx] = ismember(str, vocabList);</div><div class="line"><span class="keyword">if</span>(idx)</div><div class="line">  word_indices = [word_indices; idx];</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>然后将邮件转化成一个向量x，如果单词出现过则设特征值 x~i~ 为1，否则为0，i 为词典中的索引。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(word_indices)</div><div class="line">  x(word_indices(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>接下来我们就是利用上面完成的两个函数，通过SVM来进行邮件垃圾分类了。测试一下自己收到过的垃圾邮件：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/evernote.png" alt="evernote.png" title=""><p>果然是垃圾邮件呢：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/result.png" alt="result.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;终于来到支持向量机（SVM）了！这一章我们要学习如何使用高斯核SVM来建立一个垃圾邮件分类器。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex5</title>
    <link href="http://yoursite.com/2017/06/05/machine-learning-ex5/"/>
    <id>http://yoursite.com/2017/06/05/machine-learning-ex5/</id>
    <published>2017-06-05T02:33:50.000Z</published>
    <updated>2017-06-06T15:32:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次练习是利用正则化的线性回归来学习不同的偏差-方差（bias-variance）模型，并且学会判断其误差类型，能够对算法进行改进。</p><p>根据公式完成线性回归的代价函数和梯度的函数。接下来会根据我们算出来的theta值显示一个模型，如下图</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/line.png" alt="line.png" title=""><p>可以看到模型不太适合我们的训练值，具有高偏差（high-bias），并且可以看出欠拟合（underfitting）。因此接下来我们要通过显示训练和测试的误差学习曲线来诊断偏差-方差问题。</p><a id="more"></a><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>我们可以利用之前写好的函数来计算训练集的θ，并通过这个θ来计算训练集误差和交叉验证误差，显示学习曲线。需要注意的是训练集误差计算时要包括不同的训练集大小，并且两者都不需要正则化。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[theta_train] = trainLinearReg(X, y, lambda);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>运行完图像如下：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross1.png" alt="cross1.png" title=""><p>和pdf中的示例还差挺多的，我应该是哪里理解错了。修改for循环中交叉验证误差<code>error_val(i) = linearRegCostFunction(Xval(1:i, :), yval(1:i), theta_train, lambda);</code>，图像如下</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross2.png" alt="cross2.png" title=""><p>结果还是不正确，交叉验证误差的error值不应该这么小。而且pdf里面也说了</p><blockquote><p>When you are computing the training set error, make sure you compute it on the training subset (i.e., X(1:n,:) and y(1:n))(instead of the entire training set). However, for the cross validation error,you should compute it over the entire cross validation set.  </p></blockquote><p>也就是说交叉验证误差要用到全部的交叉验证训练集，而不是X(1:n,:) and y(1:n)。看了一下论坛，应该不同大小的训练集计算不同的θ，因此才有正确的结果：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [theta_train] = trainLinearReg(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), lambda);</div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross3.png" alt="cross3.png" title=""><p>但是submit以后发现并没有获得分数，上论坛找找答案：</p><div class="tip"><br><br>我们不需要在学习曲线的函数中自己添加<code>lambda = 0</code>，λ用于正则化θ，当我们在计算 Jtrain, Jcv以及 Jtest的时候，我们需要的是真正的误差，而不需要额外的惩罚项。<br><br></div><p>因此修改4句为<code>  [error_train(i)] = linearRegCostFunction(X(1:i,:), y(1:i), theta_train, 0);</code>，第5句同理。</p><h3 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h3><p>本来以为这个函数很简单，简单的用一个循环次方计算出每一列就可以了：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : p</div><div class="line">  X_poly(:, <span class="built_in">i</span>) = X .^ p</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>但是结果却不尽如人意，把计算的结果显示出来，发现每一行的值都是一样的，并且看上去与X好像毫无关系</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly.png" alt="poly.png" title=""><p>发现问题了！mdzz…无视了循环，就一句代码也会错。。应该改成<code>X_poly(:, i) = X .^ i</code>就对了！</p><p>在利用多项式回归函数来计算theta值之前，我们必须对训练集进行特征缩放，不然后面的数据会变得炒鸡大。</p><p>在 λ= 0的 情况下，我们发现多项式回归函数可以很好的拟合我们的训练集（下图1），并且可以看到多项式回归的学习曲线（下图2）</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot.png" alt="poly_plot.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning.png" alt="poly_learning.png" title=""><p>接下来我们通过修改不同的λ值来看一下正则化对偏差-方差的影响。</p><p>λ = 1 时：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda1.png" alt="poly_plot_lambda1.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda1.png" alt="poly_learning_lambda1.png" title=""><p>λ = 100 时：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda100.png" alt="poly_plot_lambda100.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda100.png" alt="poly_learning_lambda100.png" title=""><p>通过上面两个例子可以看出，λ = 1 的时候最好的适应了训练集，并且训练集误差和验证误差都挺小的，比 λ = 0 的时候来得优秀，而λ= 100 的时候就严重失控了，不但欠拟合，而且误差也高的要死。</p><h3 id="Selecting-λ-using-a-cross-validation-set"><a href="#Selecting-λ-using-a-cross-validation-set" class="headerlink" title="Selecting λ using a cross validation set"></a>Selecting λ using a cross validation set</h3><p>前面我们通过训练集大小的变化来训练θ，并显示学习曲线，现在我们要通过改变λ来显示学习曲线。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">length</span>(lambda_vec)</div><div class="line">  lambda = lambda_vec(<span class="built_in">i</span>);</div><div class="line">  [theta] = trainLinearReg(X, y, lambda);</div><div class="line">  error_train(<span class="built_in">i</span>) = linearRegCostFunction(X, y, theta, <span class="number">0</span>);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta, <span class="number">0</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>在使用每个函数的时候应该注意参数的带入，在使用_linearRegCostFunction_漏了θ参数，活生生的报错了。显示的学习曲线如下：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/lambda_learning.png" alt="lambda_learning.png" title=""><p>可以看出随着λ的逐渐增加，交叉验证误差先减小后增大，有一个极值点，这个极值点便是我们应该选择的λ。</p><h3 id="Computing-test-set-error"><a href="#Computing-test-set-error" class="headerlink" title="Computing test set error"></a>Computing test set error</h3><p>测试集误差的计算对于评估最终模型来说很重要，我们使用前面看出来的最优λ = 3来计算测试集误差。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">best_lambda = <span class="number">3</span>;</div><div class="line">[best_theta] = trainLinearReg(X, y, best_lambda);</div><div class="line">error_test = linearRegCostFunction(Xtest, ytest, best_theta, <span class="number">0</span>)</div></pre></td></tr></table></figure><p>在这里要注意主函数里已经对Xtest进行了多项式化，在传参的时候要传<strong>X_poly_test</strong>。</p><h3 id="Plotting-learning-curves-with-randomly-selected-examples"><a href="#Plotting-learning-curves-with-randomly-selected-examples" class="headerlink" title="Plotting learning curves with randomly selected examples"></a>Plotting learning curves with randomly selected examples</h3><p>随机选择样本来计算θ并显示学习曲线。我使用了双重循环，外面一重i定义为样本的数量error(i)返回样本为i时的误差，内层j为误差迭代次数，每次计算便存入一个error(j)中，循环完成后用这个向量来计算平均值。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[aver_error_train, aver_error_val]</span> = ...</span></div><div class="line">   randlyExamplesCurve(X, y, Xval, yval, lambda)</div><div class="line"></div><div class="line">mx = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">mxal = <span class="built_in">size</span>(Xval, <span class="number">1</span>);</div><div class="line">   </div><div class="line">aver_error_train = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line">aver_error_val = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  <span class="comment">% 随机选择i个样本</span></div><div class="line">  rndIDX1= randperm(mx);</div><div class="line">  rndIDX2 = randperm(mxal);</div><div class="line">  rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">  <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">  theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line"></div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X, rand_y, theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval, rand_yval, theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>最后显示出来的学习曲线。。。五花八门。。。</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly1.png" alt="randly1.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly2.png" alt="randly2.png" title=""><p>再仔细看一下论坛，计算误差值时theta如果不改变是没有意义的，并且无需使用全部的样本，i个就使用前i个样本，因此修改代码</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    <span class="comment">% 随机选择i个样本</span></div><div class="line">    rndIDX1= randperm(mx);</div><div class="line">    rndIDX2 = randperm(mxal);</div><div class="line">    rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">    <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">    theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_y(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_yval(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly3.png" alt="randly3.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly4.png" alt="randly4.png" title=""><p>要运行老长的时间了，试了两次，这下好像比较像样咯~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次练习是利用正则化的线性回归来学习不同的偏差-方差（bias-variance）模型，并且学会判断其误差类型，能够对算法进行改进。&lt;/p&gt;
&lt;p&gt;根据公式完成线性回归的代价函数和梯度的函数。接下来会根据我们算出来的theta值显示一个模型，如下图&lt;/p&gt;
&lt;img src=&quot;/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/line.png&quot; alt=&quot;line.png&quot; title=&quot;&quot;&gt;
&lt;p&gt;可以看到模型不太适合我们的训练值，具有高偏差（high-bias），并且可以看出欠拟合（underfitting）。因此接下来我们要通过显示训练和测试的误差学习曲线来诊断偏差-方差问题。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex4</title>
    <link href="http://yoursite.com/2017/05/31/machine-learning-ex4/"/>
    <id>http://yoursite.com/2017/05/31/machine-learning-ex4/</id>
    <published>2017-05-31T03:06:22.000Z</published>
    <updated>2017-06-01T15:06:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章主要学习了神经网络中的反向传播算法（Backpropagation algorithm ）。相对于线性回归（Linear Regression）、逻辑回归（Logistic Regression）BP确实很复杂了。但是今天刚看的《Don’t make me think》里面作者所言</p><blockquote><p><strong>我们不是追根究底，而是勉强应付。 因为这对我们来说并不重要。</strong>对于我们中大多数人来说，只要我们能够正常使用，是否明白事物背后的运行机制并没有关系。这并不是智力低下的表现，而是我们并不关心。总之，它对我们来说没那么重要。</p></blockquote><p>那么现阶段，对于BP算法有一个整体的概念，懂得如何使用它便是关键。上一章我们用已经给定的theta矩阵来进行预测，这一章我们将通过BP算法来学习参数。这一章的练习主要分为两个部分，第一部分：我们通过给定的神经网络权重，前馈计算正则化的代价函数。第二部分：我们自己训练神经网络的权重。</p><a id="more"></a><p>此次用于训练的神经网络如下图：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/model.png" alt="model.png" title=""><h3 id="Feedforward-and-cost-function"><a href="#Feedforward-and-cost-function" class="headerlink" title="Feedforward and cost function"></a>Feedforward and cost function</h3><p>首先我们需要通过给定的Theta1、Theta2，用以下公式来计算代价函数（cost function）。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/cost.png" alt="cost.png" title=""><ul><li><p>第一次尝试</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute the output value</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = X * Theta1';</div><div class="line">a2 = [ones(m, <span class="number">1</span>), sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the labels</span></div><div class="line">recode_y = <span class="built_in">zeros</span>(m, num_labels);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="built_in">j</span> = y(<span class="built_in">i</span>);</div><div class="line">  recode_y(<span class="built_in">i</span>, <span class="built_in">j</span>) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="comment">% compute cost</span></div><div class="line">J  = sum(-y' * <span class="built_in">log</span>(a3) - (<span class="number">1</span> - y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3));</div></pre></td></tr></table></figure><p>算出来的J可有1523995.662855那么大。。。第一次尝试失败。。。</p></li><li><p>第二次尝试：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% compute the output value</span></div><div class="line">  z2 = X(<span class="built_in">i</span>, :) * Theta1';</div><div class="line">  a2 = [ones(<span class="number">1</span>,<span class="number">1</span>) sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the labels as vectors</span></div><div class="line">  recode_y = <span class="built_in">zeros</span>(num_labels, <span class="number">1</span>);</div><div class="line">  recode_y(y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line">  J += -recode_y' * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line"><span class="keyword">end</span></div><div class="line">J /= m;</div></pre></td></tr></table></figure><p>对公式进行分解：for循环是用来执行公式最外层的m的累加，通过矩阵的相乘来执行内层的k的累加。</p><p>不过我不死心认为第一次尝试的思路并没有错。。用大矩阵来完成两个相加。。我要再回去尝试一下。</p></li><li><p>第三次尝试</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute the output value</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = X * Theta1';</div><div class="line">a2 = [ones(m, <span class="number">1</span>), sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the labels</span></div><div class="line">recode_y = <span class="built_in">zeros</span>(m, num_labels);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  recode_y(<span class="built_in">i</span>, y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="comment">% compute cost</span></div><div class="line">J  = sum(sum(-recode_y' * <span class="built_in">log</span>(a3) - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)));</div><div class="line">J /= m;</div></pre></td></tr></table></figure><p>修改了一下第一次尝试中错误的变量（比如最后一步忘记代入recode_y等），但是发现结果还是不对。把矩阵画出来自己试着相乘了一下，发现这样不能实现内层的 y~k~ 和 a~k~ 的一一对应相乘。而是一个y~k~会和另外a~k~都相乘然后累加。事实证明，目前还是用第二次尝试的办法实现吧。</p><p>​</p></li></ul><h3 id="Regularized-cost-function"><a href="#Regularized-cost-function" class="headerlink" title="Regularized cost function"></a>Regularized cost function</h3><p>接着我们要正则化代价函数。这个公式看起来更可怕了：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/regularized.png" alt="regularized.png" title=""><p>这里注意到在累加的时候k是从1到400而不是401，那么就不能单纯的用 sum() 把所有数通通加一起了。因此尝试代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Theta1_t = Theta1(<span class="number">1</span>:hidden_layer_size, <span class="number">1</span>:input_layer_size);</div><div class="line">Theta2_t = Theta2(<span class="number">1</span>:num_labels, <span class="number">1</span>:hidden_layer_size);</div><div class="line"></div><div class="line">J += lambda * ((sum((Theta1_t.^<span class="number">2</span>)(:)) + sum((Theta2_t.^<span class="number">2</span>)(:)))) / (<span class="number">2</span> * m);</div></pre></td></tr></table></figure><p>当然。。不会第一次就那么顺利的。。啊。。。想不通哪里错啊。。。。再仔细看一下文档，</p><blockquote><p>For the matrices Theta1 and Theta2, this corresponds to the first column of each matrix. </p></blockquote><p>那么应该把<code>Theta1_t = Theta1(1:hidden_layer_size, 1:input_layer_size);</code>改为 <code>Theta1_t = Theta1(1:hidden_layer_size, 2:input_layer_size+1);</code>Theta2_2同理。</p><h3 id="Sigmoid-gradient"><a href="#Sigmoid-gradient" class="headerlink" title="Sigmoid gradient"></a>Sigmoid gradient</h3><p>我们从前往后算出来了最终输出的值，接下来要通过后向传播来计算delta了。那么首先我们需要根据下面这个方程来完成sigmoidGradient函数。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/sigmoid.png" alt="sigmoid.png" title=""><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(z, <span class="number">1</span>)</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">size</span>(z, <span class="number">2</span>)</div><div class="line">    g(<span class="built_in">i</span>, <span class="built_in">j</span>) = sigmoid(z(<span class="built_in">i</span>,<span class="built_in">j</span>)) * (<span class="number">1</span> - sigmoid(z(<span class="built_in">i</span>,<span class="built_in">j</span>)));</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>按照4个步骤，在每个循环里计算从后向前的delta值。论坛里面说这是最具有挑战的一次练习，果然调了好久，我们从修改for循环开始：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% compute the output value</span></div><div class="line">  a1 = X(<span class="built_in">i</span>, :);</div><div class="line">  z2 = a1 * Theta1';</div><div class="line">  a2 = [<span class="number">1</span> sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the labels as vectors</span></div><div class="line">  recode_y = <span class="built_in">zeros</span>(num_labels, <span class="number">1</span>);</div><div class="line">  recode_y(y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line">  J += -recode_y' * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line">  </div><div class="line">  <span class="comment">% step2</span></div><div class="line">  delta3 = a3' - recode_y;</div><div class="line">  </div><div class="line">  <span class="comment">% step3</span></div><div class="line"></div><div class="line">  delta2 = Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>)'*delta3.*sigmoidGradient(z2)';</div><div class="line">  </div><div class="line">  <span class="comment">% step4</span></div><div class="line">  Theta2_grad += delta3 * a2;</div><div class="line">  Theta1_grad += delta2 * a1;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">grad = [Theta1_grad(:); Theta2_grad(:)] / m;</div></pre></td></tr></table></figure><p>但是上面的答案和给出的还差挺多的。。。然后发现问题竟然出现在最后。。同时也修改了一下recode函数。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% Feedforward</span></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% step1.compute the output value</span></div><div class="line">  a1 = X(<span class="built_in">i</span>, :);</div><div class="line">  z2 = a1 * Theta1';</div><div class="line">  a2 = [<span class="number">1</span> sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the y</span></div><div class="line">  recode_y = [<span class="number">1</span>:num_labels] == y(<span class="built_in">i</span>);</div><div class="line">  <span class="comment">% compute cost function</span></div><div class="line">  J += -recode_y * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y) * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line">  </div><div class="line">  <span class="comment">% step2.compute error term delta3</span></div><div class="line">  delta3 = a3 - recode_y;</div><div class="line">  <span class="comment">% size(delta3) = 1 x 10</span></div><div class="line">  </div><div class="line">  <span class="comment">% step3.compute error term delta2</span></div><div class="line">  delta2 = delta3 * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) .* sigmoidGradient(z2);</div><div class="line">  <span class="comment">% size(delta2) = 1 x 25</span></div><div class="line">  </div><div class="line">  <span class="comment">% step4.accumulate the gradient</span></div><div class="line">  Theta1_grad += delta2' * a1;</div><div class="line">  <span class="comment">% size(Theta1_grad) = 25 x 401</span></div><div class="line">  Theta2_grad += delta3' * a2;</div><div class="line">  <span class="comment">% size(Theta2_grad) = 10 x 26</span></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">J /= m; </div><div class="line"></div><div class="line"><span class="comment">% regularized the cost function</span></div><div class="line">Theta1_t = Theta1(<span class="number">1</span>:hidden_layer_size, <span class="number">2</span>:input_layer_size+<span class="number">1</span>);</div><div class="line">Theta2_t = Theta2(<span class="number">1</span>:num_labels, <span class="number">2</span>:hidden_layer_size+<span class="number">1</span>);</div><div class="line"></div><div class="line">J += lambda * ((sum((Theta1_t.^<span class="number">2</span>)(:)) + sum((Theta2_t.^<span class="number">2</span>)(:)))) / (<span class="number">2</span> * m);</div><div class="line"></div><div class="line">Theta1_grad /= m;</div><div class="line">Theta2_grad /= m;</div><div class="line">grad = [Theta1_grad(:); Theta2_grad(:)];</div></pre></td></tr></table></figure><p>完美！</p><p>并且通过查资料，get了trace（）函数，可以不用for循环完成函数了~~</p><p>修改Feedfoorward</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Feedforward</span></div><div class="line">a1 = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = a1 * Theta1';</div><div class="line">a2 = [ones(m,<span class="number">1</span>) sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the y</span></div><div class="line"><span class="comment">%recode_y = zeros(m, num_labels);</span></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</div><div class="line">  recode_y(<span class="built_in">i</span>, :) = [<span class="number">1</span>:num_labels] == y(<span class="built_in">i</span>);</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="comment">% compute cost function</span></div><div class="line">J = trace(-recode_y * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y) * <span class="built_in">log</span>(<span class="number">1</span> - a3)');</div><div class="line">J /= m;</div></pre></td></tr></table></figure><h3 id="Regularized-Neural-Networks"><a href="#Regularized-Neural-Networks" class="headerlink" title="Regularized Neural Networks"></a>Regularized Neural Networks</h3><p>接下来是正则化神经网络，根据公式则可以敲出代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% rregularized neural networks</span></div><div class="line">Theta1_grad(:, <span class="number">2</span>:<span class="keyword">end</span>) += lambda * Theta1(:, <span class="number">2</span>:<span class="keyword">end</span>) / m;</div><div class="line">Theta2_grad(:, <span class="number">2</span>:<span class="keyword">end</span>) += lambda * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) / m;</div></pre></td></tr></table></figure><p>lambda = 1，Maxiter = 50时隐藏层为下图，准确率94.6%：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda1_iter50.png" alt="lambda1_iter50.png" title=""><p>接下来我们改变lambda的数值和迭代次数来看一下隐藏层的变化。首先修改lambda为0，迭代次数不变，准确率96.4%。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda0_iter50.png" alt="lambda0_iter50.png" title=""><p>lambda为0，迭代次数为100，准确率可高了99.38%：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda0_iter100.png" alt="lambda0_iter100.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章主要学习了神经网络中的反向传播算法（Backpropagation algorithm ）。相对于线性回归（Linear Regression）、逻辑回归（Logistic Regression）BP确实很复杂了。但是今天刚看的《Don’t make me think》里面作者所言&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;我们不是追根究底，而是勉强应付。 因为这对我们来说并不重要。&lt;/strong&gt;对于我们中大多数人来说，只要我们能够正常使用，是否明白事物背后的运行机制并没有关系。这并不是智力低下的表现，而是我们并不关心。总之，它对我们来说没那么重要。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那么现阶段，对于BP算法有一个整体的概念，懂得如何使用它便是关键。上一章我们用已经给定的theta矩阵来进行预测，这一章我们将通过BP算法来学习参数。这一章的练习主要分为两个部分，第一部分：我们通过给定的神经网络权重，前馈计算正则化的代价函数。第二部分：我们自己训练神经网络的权重。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex3</title>
    <link href="http://yoursite.com/2017/05/29/machine-learning-ex3/"/>
    <id>http://yoursite.com/2017/05/29/machine-learning-ex3/</id>
    <published>2017-05-29T07:43:51.000Z</published>
    <updated>2017-05-30T02:20:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章练习也有两大部分，第一部分是正则化逻辑回归方程的多类分类，识别图像中的数字。觉得逻辑回归应该也是神经网络的一部分吧，只是没有hidden layer，直接就final了。第二部分需要解决的问题是一样的，识别图像中的文字，只通过是神经网络来达到这个目的。</p><a id="more"></a><h1 id="1-Multi-Class-Classification"><a href="#1-Multi-Class-Classification" class="headerlink" title="1/ Multi-Class Classification"></a>1/ Multi-Class Classification</h1><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>按照pdf的步骤，先写出不正则化的逻辑回归方程，打印了一下这时函数参数的X </p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/Xt.png" alt="Xt.png" title=""><p>发现这一节要求不使用循环，全部向量化，那我上一章就基本实现了耶，我真棒~ 稍微修改了一下方程，用一下hint的.*和sum，终于懂星号前面有一点是什么鬼了，是不使用矩阵乘法，而是对应的元素相乘。方程如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算unregulized cost function</span></div><div class="line">J = sum(-y .* <span class="built_in">log</span>(sigmoid(X*theta)) - (<span class="number">1</span>-y) .* <span class="built_in">log</span>(<span class="number">1</span>-sigmoid(X*theta))) / m;</div></pre></td></tr></table></figure><p>此时输出J的值为0.734819，但是参考值是2.534819，等正则化了看看结果。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算gradient</span></div><div class="line">grad = X' * (sigmoid(X * theta) - y);</div></pre></td></tr></table></figure><p> 接下来就要开始正则化cost function和gradient了。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 正则化cost function</span></div><div class="line">J += lambda * sum(theta(<span class="number">2</span>:<span class="keyword">end</span>).^<span class="number">2</span>) / (<span class="number">2</span> * m);</div></pre></td></tr></table></figure><p>上一章傻傻的用循环来避开对theta~0~的处理，现在get了用冒号来限制处理范围。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 正则化gradient</span></div><div class="line">temp = theta;</div><div class="line">temp(<span class="number">1</span>) = <span class="number">0</span>;</div><div class="line">grad += lambda * temp / m;</div></pre></td></tr></table></figure><p>提交发现咋不对啊。对照了一下ex2发现gradient忘记 / m了，我这粗心的小脑袋啊。。。</p><h3 id="oneVsAll"><a href="#oneVsAll" class="headerlink" title="oneVsAll"></a>oneVsAll</h3><p>看完毫无头绪要怎么训练啊。。我回顾一下one-vs-all是什么鬼。嗯。。再回来理解一下题目，把每个训练集分成K个类，返回一个Kx(N+1)的矩阵theta，矩阵的每一行是通过逻辑回归方程学习到的theta值。继续仔细看，没让你直接用一个函数就完成预测，这个函数只是用来生训练分类器tehta矩阵的，并且还提示使用一个给定函数fmincg来生成theta，只是循环K次，每次生成一个向量。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: num_labels</div><div class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="built_in">i</span> </div><div class="line">  c = <span class="number">10</span>;</div><div class="line">  <span class="keyword">else</span> </div><div class="line">  c = <span class="built_in">i</span>;</div><div class="line">  initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">  options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</div><div class="line">  [theta] = ...</div><div class="line">    fmincg(@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</div><div class="line">                initial_theta, options);</div><div class="line">   all_theta(c, :) = theta';</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>一开始对于c的概念有一些模糊，在经过测试以后发现 一个向量==一个数值 时，返回一个向量，其中与这个值相等的位置为1，其他位置均为0。再回顾一下上课的笔记，那么根据这个函数的意思，在num_labels个循环中y == c 就生成了num_labels个1位置不同的向量，这个向量作为判断结果，对于每一个结果通过fmincg函数来生成一个theta向量，但是这个向量是(n+1)x1的，因此要把theta转置成all_theta对应的那一行。</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/multiy.png" alt="multiy.png" title=""><p>修改代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> c = <span class="number">1</span>: num_labels</div><div class="line">  initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">  options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</div><div class="line">  [theta] = ...</div><div class="line">    fmincg(@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</div><div class="line">                initial_theta, options);</div><div class="line">   all_theta(c, :) = theta';</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>理解了c的值以后，就知道0为什么要用10来表示啦。</p><p>接下来完成进行预测的函数。要对所有训练集进行训练，也就是h = X * theta’，然后与y进行比较，判断所属类型。不过我们已经知道了y就是一个向量，1在不同的位置为不同的类别。那么就是每一个训练集得到的 h 向量中数值最大的那一个就是我们的类别。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算hypothesis</span></div><div class="line">h = X * all_theta';</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(h(<span class="built_in">i</span>));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>但是结果又不对，思路应该是没错的，检查了一下size(theta)，是一个10x401的矩阵没错，h计算出来的结果肯定也没错。那么问题肯定就出现在查找最大值上。输出了一下h(1)发现只是一个数值，我们要查找的应该是一行中的最大值，把max(h(i))改成max(h(i, :))就对啦。</p><h1 id="2-Neural-Networks"><a href="#2-Neural-Networks" class="headerlink" title="2/ Neural Networks"></a>2/ Neural Networks</h1><p>上一部分我们通过多类分类的逻辑回归方程来识别手写的数字。但是由于逻辑回归仅仅是一个线性分类器，对于更复杂的行为很难处理，因此在这个部分我们实践一下神经网络。这个神经网络有3层，输入层，隐藏层，输出层。我们使用已经训练完成的权重来进行预测。</p><h3 id="Feedforward-Propagation-and-Prediction"><a href="#Feedforward-Propagation-and-Prediction" class="headerlink" title="Feedforward Propagation and Prediction"></a>Feedforward Propagation and Prediction</h3><p>这个预测函数通过给定的Theta1、Theta2来对输入层进行训练，判断结果和one-vs-all通过最大值来分类是一样的。</p><p>我们先来看下训练集X、Theta1、Theta2的大小</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/size.png" alt="size.png" title=""><p>如上图所示，也就是说训练集有5000个，400个项（不包含x~0~ = 1），因此加上x~0~以后，Theta1把401个项训练成25个，再加上x~0~ = 1，最后训练出10个分类结果。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% add x0 = 1</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% compute hidden layer</span></div><div class="line">layer2 = [ones(m, <span class="number">1</span>), X * Theta1<span class="string">'];</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">% compute output layer</span></div><div class="line"><span class="string">layer3 = layer2 * Theta2'</span>;</div><div class="line"></div><div class="line">% predict</div><div class="line">for i = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(layer3(<span class="built_in">i</span>, :));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>自信满满写完submit发现还是错的 orz…准确率不对，回去看一下训练模型，好像每一层训练的时候都要用到sigmodi函数？</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% add x0 = 1</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% compute hidden layer</span></div><div class="line">z2 = [ones(m, <span class="number">1</span>) X * Theta1<span class="string">'];</span></div><div class="line"><span class="string">a2 = sigmoid(z2);</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">% compute output layer</span></div><div class="line"><span class="string">z3 = a2 * Theta2'</span>;</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line">% predict</div><div class="line">for i = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(z3(<span class="built_in">i</span>, :));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>啊。。。明明准确率对，结果也对为什么submit以后还是没有分数呢。。。再回去认真看模型。。</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/model.png" alt="model.png" title=""><p>好像隐藏层是先逻辑回归，再添加a~0~嗯。。。got it。。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute output layer</span></div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div></pre></td></tr></table></figure><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/nice.png" alt="nice.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章练习也有两大部分，第一部分是正则化逻辑回归方程的多类分类，识别图像中的数字。觉得逻辑回归应该也是神经网络的一部分吧，只是没有hidden layer，直接就final了。第二部分需要解决的问题是一样的，识别图像中的文字，只通过是神经网络来达到这个目的。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex2</title>
    <link href="http://yoursite.com/2017/05/28/machine-learning-ex2/"/>
    <id>http://yoursite.com/2017/05/28/machine-learning-ex2/</id>
    <published>2017-05-28T07:08:36.000Z</published>
    <updated>2017-05-30T02:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章是实践逻辑回归。两个大题目，一个就是正常的逻辑回归，另外一个是正则化的逻辑回归。记录下做作业的心路历程。</p><a id="more"></a><h3 id="plotData"><a href="#plotData" class="headerlink" title="plotData"></a>plotData</h3><p>一开始就感觉被难住，X是mx2的矩阵，y是m行向量。对于如何来显示图像示毫无头绪，然后发现pdf里给出了答案。get了两个函数，一个是find，另一个是plot。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/find.png" alt="find.png" title=""><p>简单的看一下find根据查找条件返回查找矩阵的下标</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pos = <span class="built_in">find</span>(y == <span class="number">1</span>);</div><div class="line">neg = <span class="built_in">find</span>(y == <span class="number">0</span>);</div></pre></td></tr></table></figure><p>因此这两句结果分类返回对应的训练集所在行数。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/pos.png" alt="pos.png" title=""><p>看一下pos~就是返回一个向量，包括结果为1的行数。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/plot.png" alt="plot.png" title=""><p>接下来就是显示啦，第一个参数是x轴的数值，第二个参数是y坐标的数值，分别取自对应分类的第一项成绩和第二项成绩。然后显示结果。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/plot_result.png" alt="plot_result.png" title=""><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>这个用下图这个公式变换一下函数</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/sigmoid.png" alt="sigmoid.png" title=""><p>一步一步来，先实现一个数字的转换：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g = <span class="number">1</span> / (<span class="number">1</span> + <span class="built_in">exp</span>(x));</div></pre></td></tr></table></figure><p>然后发现结果不正确，发现x还有可能是矩阵，也就是说矩阵的每一位都要做相应的变换</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:r</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:c</div><div class="line">  g(<span class="built_in">i</span>, <span class="built_in">j</span>) = <span class="number">1</span> / (<span class="number">1</span> + <span class="built_in">exp</span>(-z(<span class="built_in">i</span>,<span class="built_in">j</span>)));</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h3 id="costFunction"><a href="#costFunction" class="headerlink" title="costFunction"></a>costFunction</h3><p>首先公式很复杂，但是其实实现很简单</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/cost.png" alt="cost.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/grad.png" alt="grad.png" title=""><p>我一开始弄了两层循环来遍历i和m，后面意识到，我们用了矩阵啊，然后变成了一层只对m的循环，然后发现还是不对，其实就。。。非常简单</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算hypothesis</span></div><div class="line">para =  X * theta;</div><div class="line"><span class="comment">% 计算cost</span></div><div class="line">J = (-y' * <span class="built_in">log</span>(sigmoid(para)) - (<span class="number">1</span> - y)' * (<span class="built_in">log</span>(<span class="number">1</span> - sigmoid(para)))) / m;</div><div class="line"><span class="comment">% 计算gradient</span></div><div class="line">grad = X' * (sigmoid(para) - y)/ m;</div></pre></td></tr></table></figure><h3 id="costFunctionReg"><a href="#costFunctionReg" class="headerlink" title="costFunctionReg"></a>costFunctionReg</h3><p>正则化逻辑回归的函数，这里要注意的是，计算cost和gradient的时候都不能把theta~0~ 算进去，复杂的公式用矩阵带入就好简单呀，虽然花了一个下午完成练习，主要都花在纠结矩阵谁乘以谁，但是完成了就成就感满满~~</p><p>修改一下lambda的值来测试一下整体化到底是什么鬼</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda0.png" alt="lamda0.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda1.png" alt="lamda1.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda50.png" alt="lamda50.png" title=""><p>从图形的变化可以看出lamda太小会overfitting，1的时候切好是最正确的分界线，50就开始under fitting了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章是实践逻辑回归。两个大题目，一个就是正常的逻辑回归，另外一个是正则化的逻辑回归。记录下做作业的心路历程。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>复习一下单片机吧</title>
    <link href="http://yoursite.com/2017/05/24/Microcontrollermd/"/>
    <id>http://yoursite.com/2017/05/24/Microcontrollermd/</id>
    <published>2017-05-23T16:00:00.000Z</published>
    <updated>2017-05-28T10:44:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解无线键盘监听装置的实验原理中，还是先理解一下一直没搞懂的单片机吧。顺便小小复习一下以前计算机组成原理的内容。</p><a id="more"></a><h1 id="单片机"><a href="#单片机" class="headerlink" title="单片机"></a>单片机</h1><p>全称单片微型计算机，又称微控制器，是把中央处理器、存储器、定时/计数器、各种输入输出接口等都集成在一块集成电路芯片上的微型计算机。与电脑中的通用型微处理器相比，它更强调自供应（不用外接硬件）和节约成本，最大优势是体积小，可放在仪表内部，但存储量小，输入输出接口简单，功能较低。</p><p>大多数使用冯诺依曼结构，即定义了嵌入式系统所必须的四个基本部分：1、中央处理器核心，程序存储器（只读存储器或闪存）。2、数据存储器（随机存储器）。3、一个或者多个定时/计算器。4、用来与外围设备以及扩展资源进行通信的输入/输出端口。</p><h3 id="与CPU的区别"><a href="#与CPU的区别" class="headerlink" title="与CPU的区别"></a>与CPU的区别</h3><p>CPU是一个运算处理器，主要负责计算、处理数据。单片机包括运算单元、存储单元、输入输出单元。单片机是一种微处理器，可以执行某些功能。但是CPU还要有主板、内存、硬盘等辅助。</p><h3 id="常用单片机"><a href="#常用单片机" class="headerlink" title="常用单片机"></a>常用单片机</h3><p>微芯（Microchip）公司的PIC系列出货量居于业界领导者地位；Atmel的51系列AVR系列种类众多，受支持面广；Arduino就是基于Atmel的AVR系列；德州仪器的MSP430系列以低功耗闻名，常用于医疗电子产品以及仪器仪表中；瑞萨单片机在日本使用广泛。还有比如Intel、飞思、卡尔半导体，NEC，NXP等等厂牌。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解无线键盘监听装置的实验原理中，还是先理解一下一直没搞懂的单片机吧。顺便小小复习一下以前计算机组成原理的内容。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
