<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-10T15:00:45.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>cs231n assignment1 学习笔记</title>
    <link href="http://yoursite.com/2017/11/10/cs231n-assignment1/"/>
    <id>http://yoursite.com/2017/11/10/cs231n-assignment1/</id>
    <published>2017-11-10T13:43:02.000Z</published>
    <updated>2017-11-10T15:00:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 <a href="https://matplotlib.org/faq/osx_framework.html#osxframework-faq" target="_blank" rel="external">Working with Matplotlib on OSX</a>，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。</p><h3 id="k-Nearest-Neighbor-kNN-exercise"><a href="#k-Nearest-Neighbor-kNN-exercise" class="headerlink" title="k-Nearest Neighbor (kNN) exercise"></a>k-Nearest Neighbor (kNN) exercise</h3><p>点击「run」执行每个框框，<code>dists = classifier.compute_distances_two_loops(X_test)</code> 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。</p><a id="more"></a><p>进行预测的时候发现准确度无论如何都是0.14，查了一下发现 <code>predict_labels</code> 函数没有修改😂 </p><p>二次循环的函数比较简单，一次循环边洗澡边思考，然后头脑风暴矩阵变换，开心地想出来了☺️ 核心代码<code>dists[i, :] = np.sqrt(np.sum((X[i, :] - self.X_train) ** 2, axis = 1)).T</code></p><p>无循环函数估摸着应该是要运用数学公式来解决。假设测试集(2x3)：<br>$$<br>testX = \begin{pmatrix}x_{11}&amp;x_{12}&amp;x_{13} \\x_{21}&amp;x_{22}&amp;x_{23}\ \end{pmatrix}<br>$$<br>训练集(4x3)：<br>$$<br>trainX = \begin{pmatrix}y_{11}&amp;y_{12}&amp;y_{13} \\y_{21}&amp;y_{22}&amp;y_{23}\\y_{31}&amp;y_{32}&amp;y_{33}\\y_{41}&amp;y_{42}&amp;y_{43}\ \end{pmatrix}<br>$$<br>最后生成 dist (2x4) 的距离矩阵，自然联想到矩阵乘法 (2x4) := (2x3) x (3x4)。刚好代码中也有提示矩阵乘法，那么 \(trainX\) 必然是转置一下的。<br>$$<br>trainX^T = \begin{pmatrix}y_{11}&amp;y_{21}&amp;y_{31}&amp;y_{41} \\y_{12}&amp;y_{22}&amp;y_{32}&amp;y_{42}\\y_{13}&amp;y_{23}&amp;y_{33}&amp;y_{43}\\ \end{pmatrix}<br>$$<br>我们取出一个测试集和一个训练集，计算一下它们的欧氏距离：<br>$$<br>dist[1, 1] = \sqrt{(x_{11}-y_{11})^2 + (x_{12}-y_{12})^2 + (x_{13}-y_{13})^2 }<br>$$<br>我们试一下把每个平方拆开：<br>$$<br>dist[1, 1] = \sqrt{x_{11}^2 - 2x_{11}y_{11}+y_{11}^2 + x_{12}^2 - 2x_{12}y_{12}+y_{12}^2+x_{13}^2 - 2x_{13}y_{13}+y_{13}^2}<br>$$<br>整理出根号里头的家伙：<br>$$<br>x_{11}^2+ x_{12}^2+x_{13}^2 +y_{11}^2+y_{12}^2 +y_{13}^2 - 2(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13})<br>$$<br>嗯。。。再一次感叹数学之美。。。于是「根号里头的家伙」就被分成了三个部分</p><ul><li>\(x_{11}^2+ x_{12}^2x_{13}^2\) 相当于 \(testX\) 元素平方，再按列相加，即把矩阵压缩成一列。</li><li>\(y_{11}^2+y_{12}^2 +y_{13}^2\) 相当于 \(trainX^T\) 元素平方，再按行相加 ，即把矩阵压缩成一行。</li><li>\(x_{11}y_{11}   +x_{12}y_{12}+ x_{13}y_{13}\) 相当于\(testX * trainX\)</li></ul><p>于是就可以 <strong>broadcasting</strong> 啦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">transXtrain = self.X_train.T <span class="comment"># 转置训练集</span></div><div class="line">sumX = np.sum(X ** <span class="number">2</span>, axis = <span class="number">1</span>)[:, np.newaxis] <span class="comment">#（500x1）</span></div><div class="line">sumtransXtrain = np.sum(transXtrain ** <span class="number">2</span>, axis = <span class="number">0</span>)[np.newaxis] <span class="comment">#（1x5000）</span></div><div class="line">dists = np.sqrt( sumX + sumtransXtrain - <span class="number">2</span> * np.dot(X, transXtrain)) <span class="comment"># broadcasting &amp; matrix multiplication</span></div></pre></td></tr></table></figure><p>最后一个部分是关于k-折交叉验证，根据作业提示，循环每个可能的k值，运行k-nn算法 num_folds 次，每次选择一个子集作为训练集，最后一个子集为验证集。但参考了网上关于10折交叉验证的说法，是轮流将其中9份作为训练数据，1份作为测试数据的。所以我就循环了num+folds-1次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">size = X_train_folds[<span class="number">0</span>].shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</div><div class="line">    k_to_accuracies[k] = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds<span class="number">-1</span>): <span class="comment"># each case </span></div><div class="line">        classifier.train(X_train_folds[i], y_train_folds[i]) <span class="comment"># use one as training data</span></div><div class="line">        predict_labels = classifier.predict(X_train_folds[<span class="number">-1</span>], k, num_loops = <span class="number">0</span>)</div><div class="line">        accuracy = sum(predict_labels == y_train_folds[<span class="number">-1</span>]) / size <span class="comment"># last fold as validation set</span></div><div class="line">        k_to_accuracies[k].append(accuracy)</div></pre></td></tr></table></figure><p><img src="/2017/11/10/cs231n-assignment1/knn.png" alt="cross-validation"></p><p>最后根据交叉验证的结果选择k=8最佳。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作业的运行环境我选择「Working locally」。在配置虚拟环境过程中，果真遇到 matplotlib 运行不了的问题，参考 &lt;a href=&quot;https://matplotlib.org/faq/osx_framework.html#osxframework-faq&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Working with Matplotlib on OSX&lt;/a&gt;，除了最后一个，几乎每种方法设置一遍，重启了好几次虚拟环境，最后可以用 jupyter notebook 打开。&lt;/p&gt;
&lt;h3 id=&quot;k-Nearest-Neighbor-kNN-exercise&quot;&gt;&lt;a href=&quot;#k-Nearest-Neighbor-kNN-exercise&quot; class=&quot;headerlink&quot; title=&quot;k-Nearest Neighbor (kNN) exercise&quot;&gt;&lt;/a&gt;k-Nearest Neighbor (kNN) exercise&lt;/h3&gt;&lt;p&gt;点击「run」执行每个框框，&lt;code&gt;dists = classifier.compute_distances_two_loops(X_test)&lt;/code&gt; 真的要运行非常非常非常久，同时框框左边会左边变成 In [*]，然后我傻傻的以为卡了，重启了好多次，后来才醒悟这是正在运行的意思。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记五：自适应增强算法</title>
    <link href="http://yoursite.com/2017/11/05/machine-learning-in-action-note5/"/>
    <id>http://yoursite.com/2017/11/05/machine-learning-in-action-note5/</id>
    <published>2017-11-05T01:14:40.000Z</published>
    <updated>2017-11-09T04:39:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：</p><ul><li><p>Bagging（bootstrap aggregating）</p><p>Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。</p></li><li><p>Boosting</p><p>Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。</p></li></ul><p>Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。</p><a id="more"></a><p>AdaBoost的每个样本都有一个权重，构成向量D。首先初始化每个样本的权重相等，在训练集上训练出一个弱分类器，然后计算出错误率 𝝐，通过错误率计算该分类器的 alpha 值，通过这个 alpha 值计算该分类器的权重。<br>$$<br>𝜶 = \frac{1}{2}\ln\frac{1-𝝐}{𝝐}<br>$$<br>接着对权重做出调整，降低分对的样本权重，<br>$$<br>D_{i+1} = \frac{D_i^{i}e^{-𝜶}}{Sum(D)}<br>$$</p><p>提高分错的样本权重。</p><p>$$<br>D_{i+1} = \frac{D_i^{i}e^{𝜶}}{Sum(D)}<br>$$<br>不断迭代达到一定数量或错误率为0，最后输出集成的弱分类器，通过加权来进行预测。算法过程如下图所示：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/adaboost.png" alt="adaboost"></p><p>###训练：决策树桩</p><p>算法的关键就在于如何训练弱分类器，并把它们集成一个强分类器。本章我们使用决策树桩（decision stump）来实现AdaBoost。决策树桩的树桩意味着我们只用单个特征来进行决策。首先我们设定一个阈值 _threshVal_ 与比较规则 _threshIneq_ ，在规则下判断特征值与阈值的大小。当 _threshIneq == ‘lt’_，将所有比 _threshVal_ 小的归为 -1；当 _threshIneq == ‘gt’_ 时，将所有比 _threshVal_ 大的归为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(data, dimen, threshVal, threshIneq)</span>:</span></div><div class="line">    <span class="string">"""测试是否某个值大于或小于阈值"""</span></div><div class="line">    retArr = ones((shape(data)[<span class="number">0</span>], <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</div><div class="line">        retArr[data[:, dimen] &lt;= threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        retArr[data[:, dimen] &gt; threshVal] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> retArr</div></pre></td></tr></table></figure><p>这里利用数组过滤来对比阈值大小 <code>retArr[data[:, dimen] &lt;= threshVal]</code> 可以很简单滴获取到返回数组，这个返回数组就是一个弱分类器预测的结果。建立决策树桩的过程就是找出错误率最低的弱分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(data, labels, D)</span>:</span></div><div class="line">    <span class="string">"""遍历样本的每一列，返回错误率最小的弱分类器"""</span></div><div class="line">    dataMat = mat(data); labelMat = mat(labels).T</div><div class="line">    m, n = shape(dataMat)</div><div class="line">    numSteps = <span class="number">10</span>; bestStump = &#123;&#125;; bestClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    minError = inf</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n): <span class="comment"># 每个特征</span></div><div class="line">        rangeMin = dataMat[:, i].min(); rangeMax = dataMat[:, i].max()</div><div class="line">        stepSize = (rangeMax - rangeMin) / numSteps</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">-1</span>, int(numSteps + <span class="number">1</span>)): <span class="comment"># 每个步长</span></div><div class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment"># 每个不等号</span></div><div class="line">                threshVal = (rangeMin + float(j) * stepSize)</div><div class="line">                predictedVals = stumpClassify(dataMat, i, threshVal, inequal) <span class="comment"># 建立一棵决策树桩</span></div><div class="line">                errArr = mat(ones((m, <span class="number">1</span>)))</div><div class="line">                errArr[predictedVals ==  labelMat] = <span class="number">0</span></div><div class="line"></div><div class="line">                <span class="comment"># 计算错误率</span></div><div class="line">                <span class="comment"># AdaBoost和分类器交互的地方</span></div><div class="line">                weightedError = D.T * errArr</div><div class="line">                <span class="comment">#print("split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f" %(i, threshVal, inequal, weightedError))</span></div><div class="line">                <span class="keyword">if</span> weightedError &lt; minError: <span class="comment"># 保存错误率较小的弱分类器</span></div><div class="line">                    minError = weightedError</div><div class="line">                    bestClassEst = predictedVals.copy() <span class="comment"># 预测结果</span></div><div class="line">                    bestStump[<span class="string">'dim'</span>] = i</div><div class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</div><div class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</div><div class="line">    <span class="keyword">return</span> bestStump, minError, bestClassEst</div></pre></td></tr></table></figure><p>我们知道了如何建立一个弱分类器，那么接下来就到了集成弱分类器的部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(data, labels, numIt = <span class="number">40</span>)</span>:</span></div><div class="line">    weakClassArr = []</div><div class="line">    m = shape(data)[<span class="number">0</span>]</div><div class="line">    <span class="comment"># D保存每个样本的权重</span></div><div class="line">    <span class="comment"># AdaBoost算法会降低分对的样本权重，提高分错的样本权重</span></div><div class="line">    D = mat(ones((m, <span class="number">1</span>))/m)</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</div><div class="line">        bestStump, error, classEst = buildStump(data, labels, D)</div><div class="line">        <span class="comment"># 计算该分类器的权重</span></div><div class="line">        alpha = float(<span class="number">.5</span> * log((<span class="number">1</span> - error) / max(error, <span class="number">1e-16</span>))) <span class="comment"># 确保没有错误时不发生除零溢出</span></div><div class="line">        bestStump[<span class="string">'alpha'</span>] = alpha</div><div class="line">        weakClassArr.append(bestStump)</div><div class="line"></div><div class="line">        <span class="comment"># 1 / 计算下一次迭代的权重向量D</span></div><div class="line">        expon = multiply(<span class="number">-1</span> * alpha * mat(labels).T, classEst)</div><div class="line">        D = multiply(D, exp(expon))</div><div class="line">        D = D / D.sum() <span class="comment"># D是一个概率分布, sum = 1</span></div><div class="line"></div><div class="line">        <span class="comment"># 2 / 计算总错误率</span></div><div class="line">        aggClassEst += alpha * classEst</div><div class="line">        aggErrors = multiply(sign(aggClassEst) != mat(labels).T, ones((m, <span class="number">1</span>)))</div><div class="line">        errorRate = aggErrors.sum() / m</div><div class="line">        <span class="keyword">if</span> errorRate == <span class="number">0</span>: <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> weakClassArr</div></pre></td></tr></table></figure><p>###分类：分类器加权</p><p>将每个弱分类器的结果进行加权，输出最终预测的分类结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass, classifierArr)</span>:</span></div><div class="line">    dataMatrix = mat(datToClass)</div><div class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</div><div class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)): <span class="comment"># 每个弱分类器</span></div><div class="line">        classEst = stumpClassify(dataMatrix, classifierArray[i][<span class="string">'dim'</span>],classifierArray[i][<span class="string">'thresh'</span>], classifierArray[i][<span class="string">'ineq'</span>])</div><div class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>] * classEst <span class="comment"># 加权</span></div><div class="line">    <span class="comment"># 返回预测结果</span></div><div class="line">    <span class="keyword">return</span> sign(aggClassEst)</div></pre></td></tr></table></figure><h3 id="非均衡分类问题：ROC曲线"><a href="#非均衡分类问题：ROC曲线" class="headerlink" title="非均衡分类问题：ROC曲线"></a>非均衡分类问题：ROC曲线</h3><p>之前我们都只用错误率来判断分类器的好坏，假设分类问题的结果是不均衡的，比如预测一个人一个人得癌症／没有得癌症，这个分类的代价是不同的，只看错误率是没什么意义的。因此我们引入一些别的性能指标来判断分类器</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">+1</th><th style="text-align:center">-1</th></tr></thead><tbody><tr><td style="text-align:center">+1</td><td style="text-align:center">真正例（TP）</td><td style="text-align:center">伪反例（FN）</td></tr><tr><td style="text-align:center">-1</td><td style="text-align:center">伪正例（FP）</td><td style="text-align:center">真反例（TN）</td></tr></tbody></table><ul><li>正确率（Precision） = TP /（TP + FP）</li><li>召回率（Recall） = TP /（TP + FN）</li></ul><p>ROC代表接受者特征（reciver operating characteristic）曲线。横轴是伪正例的比例 （假阳率 = FP /（FP+TN）），纵轴是真正例的比例（真阳率 = TP / （TP + FN））。</p><blockquote><p>为了创建ROC曲线，首先将分类样例按照其测试强度排序。先从排名最低的样例开始，所有排名更低的样例都被视为反例，而所有排名更高的样例都被判为正例。该情况对应点为(1, 1)。然后将其移到排名次低的样例中去，如果该样例属于正例，那么对真阳率进行修改；如果该样例属于反例，那么对假阴率进行修改。</p></blockquote><p>虽然没看懂上面这段话，但看懂了原代码，感觉循环 <code>for index in sortedIndicies.tolist()[0]</code> 超迷的，tolist() 是什么鬼，[0] 又是什么鬼。对原函数进行分析，首先是对传入参数 _aggClassEst_ 进行 _argsort_ 排序。_aggClassEst_ 是一个矩阵， shape ==  (298,1)，如果直接排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort1.png" alt=""></p><p>如果将其转置再进行排序结果如下：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/argsort2.png" alt=""></p><p>根据 <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argsort.html" target="_blank" rel="external">numpy.argsort</a> 关于一维数组的排序，我们先将其转换为一维数组，并将其<strong>shape</strong>转换 (298 ,)。转换有两种方法：</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aggClassEst.getA().reshape((-1, ))</div></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">squeeze(aggClassEst.getA())</div></pre></td></tr></table></figure></li></ul><p>在这里简单说明一下 (R，1)和 (R，) 的区别。NumPy数组的<strong>shape</strong>为 (R， ) 意味着这个数组只有一个索引，以一个有12个元素的数组为例：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/narray1.png" alt=""></p><p>当我们将其 <code>reshape((3, 4))</code> 后，它有了两个索引：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape34.png" alt=""></p><p>当我们 <code>reshape((12, 1))</code>，它其实也是有两个索引，只是其中一个恒为0：</p><p><img src="/2017/11/05/machine-learning-in-action-note5/reshape1.png" alt=""></p><p>于是修改代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></div><div class="line">    cur = (<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 保留绘制光标的位置</span></div><div class="line">    ySum = <span class="number">0</span> <span class="comment"># 计算AUC</span></div><div class="line">    numPosClas = sum(array(classLabels) == <span class="number">1</span>)</div><div class="line">    yStep = <span class="number">1</span> / float(numPosClas)</div><div class="line">    xStep = <span class="number">1</span> / float(len(classLabels) - numPosClas)</div><div class="line">    <span class="comment"># 获取排序后的索引</span></div><div class="line">    <span class="comment"># 从 (1, 1) 绘制到 (0, 0)</span></div><div class="line">    sortedIndicies = predStrengths.argsort()</div><div class="line">    <span class="comment"># fig = plt.figure();  fig.clf();ax = plt.subplot(111)</span></div><div class="line">    fig, ax = plt.subplots()</div><div class="line">    <span class="comment"># for index in sortedIndicies.tolist()[0]:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sortedIndicies:</div><div class="line">        print(predStrengths[i])</div><div class="line">        <span class="keyword">if</span> classLabels[i] == <span class="number">1</span>:</div><div class="line">            delX = <span class="number">0</span>; delY = yStep</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            delX = xStep; delY = <span class="number">0</span></div><div class="line">            ySum += cur[<span class="number">1</span>]</div><div class="line">        ax.plot([cur[<span class="number">0</span>], cur[<span class="number">0</span>]-delX], [cur[<span class="number">1</span>], cur[<span class="number">1</span>]-delY], c= <span class="string">'b'</span>)</div><div class="line">        cur = (cur[<span class="number">0</span>] - delX, cur[<span class="number">1</span>] - delY)</div><div class="line">    ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'b--'</span>)</div><div class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>)</div><div class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>)</div><div class="line">    ax.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    plt.show()</div><div class="line">    print(<span class="string">"the Area Under the Curve is: "</span>, ySum * xStep)</div><div class="line"></div><div class="line">predStrengths = aggClassEst.getA().reshape((<span class="number">-1</span>, ))</div><div class="line">plotROC(predStrengths, labels)</div></pre></td></tr></table></figure><p><img src="/2017/11/05/machine-learning-in-action-note5/roc.png" alt=""></p><p><strong>参考链接</strong></p><p>1) <a href="https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r" target="_blank" rel="external">Difference between numpy.array shape (R, 1) and (R,)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AdaBoost算法是一种元算法。元算法（meta-algorithm）也叫集成方法（ensemble method），通过将其他算法进行组合而形成更优的算法，组合方式包括：不同算法的集成，数据集不同部分采用不同算法分类后的集成或者同一算法在不同设置下的集成。下面我们讨论两种使用弱分类器和多个实例构建一个强分类器的技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bagging（bootstrap aggregating）&lt;/p&gt;
&lt;p&gt;Bagging即套袋法，从原始数据集中随机抽取n个训练样本，抽取S次后，得到S个新数据集（其中的随机意味着可能会抽取到重复样本）。将某个学习算法分别作用于每个数据集得到S个分类器。当对新数据进行分类时，运用S个分类器进行分类，选择投票结果中最高的类别作为分类结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Boosting&lt;/p&gt;
&lt;p&gt;Boosting最常见的算法是AdaBoost（Adaptive Boosting）。不同的分类器是通过串行训练获得，每个新分类器根据已训练出的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bagging中分类器权重是相等的。而Boosting中分类器的权重是不相等的，分类的结果是基于所有分类器加权求和的结果，每个权重代表的是其分类器在上一轮迭代的成功度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记四：支持向量机</title>
    <link href="http://yoursite.com/2017/10/22/machine-learning-in-action-note4/"/>
    <id>http://yoursite.com/2017/10/22/machine-learning-in-action-note4/</id>
    <published>2017-10-22T00:50:53.000Z</published>
    <updated>2017-11-07T11:54:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。</p><p>上一章学习的「对数几率函数」中，我们提到了</p><blockquote><p>利用线性回归模型的预测结果去逼近真实标记的对数几率</p></blockquote><p>标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。</p><p>上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svmpic.jpg" alt="svmpic"></p><a id="more"></a><p>👆🏻有一条线把苹果和香蕉分开了（在二维空间中就是一条线），这样分开不同训练样本的线就称为「划分超平面」。距离这条线最近的几个点就是「支持向量」，它们到这条线的距离就为「间隔」。那么我们再来回顾一下刚说的一句话：</p><blockquote><p>找出具有“最大间隔”（maximum margin）的「划分超平面」</p></blockquote><p>能够划分训练样本的超平面可能有很多，我们应该要找的，是位于“正中间”的那个划分超平面，也就是距离不同类别都尽可能远的那个超平面。这样进行预测时误差才会尽可能小。</p><p>说到这里，我们可以明确一下SVM算法的设计问题了。因为越接近超平面的点越“难”分割，找到这些点就万事大吉了。因此SVM学习分类器最重要的是找到哪些样本作为「支持向量」。其本质是一个最优化问题。</p><p>一个最优化问题通常有两个最基本的因素：1）目标函数：希望什么东西的指标达到最好；2）优化对象：你希望通过改变哪些因素使目标函数达到最优。在SVM中，目标函数是「最大间隔」，优化对象就是「划分超平面」。下面我们就需要对这两个基本因素进行数学描述。</p><p>###SVM的数学建模</p><p><img src="/2017/10/22/machine-learning-in-action-note4/svm_model.png" alt="svmpic"></p><p>划分超平面的线性方程：<br>$$<br>w^Tx + b = 0<br>$$<br>其中，<strong>w</strong> = （w~1~； w~2~；… ； w~d~）为法向量。样本空间中任意点 <strong>x</strong> 到超平面的距离（几何间隔）可写为：<br>$$<br>d = \frac{|w^Tx + b|}{||w||}<br>$$<br>其中，$||w||$ 为向量的模。前面我们说到，SVM的分类结果是+1／-1，令：<br>$$<br>\begin{cases}w^Tx + b≥+1, y_i = +1 \\ w^Tx + b≤-1, y_i = -1<br>\end{cases}<br>$$<br>支持向量使得等号成立。两个异类支持向量到超平面的距离之和为<br>$$<br>𝜸 = \frac{2}{||w||}<br>$$<br>显然，为了最大化间隔，仅需最大化$||w||^{-1}$，等价于最小化$||w||^2$。</p><p>到这里，我们可以给出SVM的数学描述：<br>$$<br>\min_{w,b}\frac{1}{2}||w||^2 \\  s.t. \quad y_i(w^Tx+b) ≥1,i = 1,2,…,m<br>$$</p><p>缩写s. t. 表示“Subject to”，是“服从某某条件”的意思。根据参考链接[3]解释一下这个条件的含义。我们定义「函数间隔」为<br>$$<br>y(w^Tx+b)=yf(x)≥𝜸<br>$$<br>前面乘上类别 y 之后保证间隔的非负性（因为 f(x)&lt;0 对应于 y=−1 的那些点）。</p><p>###对偶问题</p><p>上面SVM的数学描述其实是一个<strong>二次优化问题</strong>——目标函数是二次的，约束条件是线性的。引入「拉格朗日乘子法」求解，对每条约束添加拉格朗日乘子𝜶~i~ ≥ 0，则该问题的拉格朗日函数可写为<br>$$<br>L（w,b,𝜶) =\frac{1}{2}||w||^2 + 𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))<br>$$<br>其中，\(𝜶 = （𝛼_1;𝛼_2;…;𝛼_m)\)。我们令<br>$$<br>𝜃(w) =\max _{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$</p><p>则上式的最优值为 $𝜃(w) = \frac{1}{2}||w||^2$, 即我们需要优化的SVM数学模型。具体公式为</p><p>$$<br>\min_{w,b} 𝜃(w)  =\min_{w,b}\max_{𝜶_i≥ 0}\quad L（w,b,𝜶)<br>$$<br>将min和max交换位置得到原始问题的对偶问题</p><p>$$<br>\max_{𝜶_i≥ 0}\min_{w,b}\quad L（w,b,𝜶)<br>$$<br><strong>为什么可以转化呢？</strong>因为瘦死的骆驼比马大，「最大值中的最小值」也比「最小值中的最大值」来得大。</p><p><strong>那么先求最大值和先求最小值有什么区别呢？</strong>因为这样更容易求解。我们通过偏导先求 <strong>L</strong> 关于 𝒘 和 𝑏 极小，再求 <strong>L</strong> 的极大。分别令 ∂/∂w 和 ∂/∂b 为零可得<br>$$<br>\frac{∂L}{∂w} = 0 ⇒ w = \sum_{i=1}^m𝜶_iy_ix_i<br>$$</p><p>$$<br>\frac{∂L}{∂b} = 0 ⇒ \sum_{i=1}^m𝜶_iy_i = 0<br>$$</p><p>代回 <strong>L</strong> 得到SVM数学描述的对偶问题<br>$$<br>max_𝜶\quad \sum_{i=1}^m𝜶_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m𝜶_i𝜶_jy_iy_jx_i^Tx_j^T<br>$$</p><p>$$<br>s.t\quad\sum_{i=1}^m𝜶_iy_i = 0,\quad𝜶_i≥ 0,i =1,2,…,m<br>$$</p><p>解出𝜶后，求出w与b即可得到模型<br>$$<br>\begin {align_} f(x) &amp; = w^T+b \\ &amp; = \sum_{i=1}^m𝜶_iy_ix_i^Tx + b\end {align_}<br>$$<br>所以只要求出了w和b，将测试数据带入上面这个模型，即可得出预测值。</p><p>实际上，所有非支持向量的𝜶值都为零，<strong>最终模型只与支持向量有关</strong>。回忆一下我们的拉格朗日函数<br>$$<br>\max _{𝜶_i≥ 0}\quad L（w,b,𝜶) =\max_{𝜶_i≥ 0}\quad \frac{1}{2}||w||^2 + \color{red}{𝜶_i\sum_{i=1}^m(1-y_i(w^Tx_i+b))}<br>$$<br>如果 x~i~ 是支持向量，那么红色标出部分为0（因为支持向量函数间隔为1）。对于非支持向量，函数间隔大于1，那么红色标出部分将小于0，为满足整个式子最大，只能𝜶~i~ 为0。因此我们预测的过程只需要计算少量向量的内积，速度是很快的。</p><h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>好的，接下来问题就转换为<strong>如何求解对偶问题</strong>。1996年（竟然在我出生这年搞事情🙂）John Platt发布了SMO（Sequntial Minimal Optimization）算法，将大优化问题分解为多个小优化问题求解。SMO的基本思路是先固定𝜶~i~之外的所有参数，然后求𝜶~i~上的极值。由于存在约束 $sum_{i=1}^m𝜶_iy_i = 0$ ，若固定 𝜶~i~ 之外的其他变量，则 𝜶~i~ 可由其他变量导出。于是SMO每次选择两个变量𝜶~i~ 和 𝜶~j~ ，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li>选取一对需要更新的变量 𝜶~i~ 和 𝜶~j~ ；</li><li>固定 𝜶~i~ 和 𝜶~j~ 以外参数，求解获得更新后的 𝜶~i~ 和 𝜶~j~ </li></ul><p>好的，理解SMO实在是无能为力了，我们进入下一个话题「核函数」。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>上面我们解决了线性分类问题，但SVM还可以解决非线性的分类问题，这就需要引入「核函数」，来将数据从一个特征空间转换到另一个特征空间，在新空间下再利用模型对数据进行处理。</p><p><img src="/2017/10/22/machine-learning-in-action-note4/kernel.gif" alt="kernel"></p><p>径向基函数（radial bias function）是SVM常用的一个核函数，具体公式为<br>$$<br>k(x,y) = exp(\frac{-||x-y||^2}{2𝜎^2})<br>$$<br>其中 𝜎 是用户定义的函数值跌落到0的速度参数。</p><h3 id="SVM的一般流程"><a href="#SVM的一般流程" class="headerlink" title="SVM的一般流程"></a>SVM的一般流程</h3><ol><li>收集数据：可以使用任意方法</li><li>准备数据：需要数值型数据</li><li>分析数据：可视化分割超平面是很有帮助的</li><li>训练算法：SVM算法最耗时的地方。该过程主要实现两个参数调优</li><li>测试算法：计算十分简单</li><li>使用算法：几乎所有分类问题都可以用SVM来解决，值得一提的是，SVM本身是一个二类分类器，你需要修改一些代码来适应多分类问题</li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在作者给的源码中，有可视化数据的部分，修改了一下用于测试不同k值rbf选择的支持向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">xcord0 = []; ycord0 = []; xcord1 = []; ycord1 = []</div><div class="line">fr = open(<span class="string">'testSetRBF.txt'</span>) <span class="comment"># generate data</span></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</div><div class="line">    lineSplit = line.strip().split(<span class="string">'\t'</span>)</div><div class="line">    xPt = float(lineSplit[<span class="number">0</span>])</div><div class="line">    yPt = float(lineSplit[<span class="number">1</span>])</div><div class="line">    label = float(lineSplit[<span class="number">2</span>])</div><div class="line">    <span class="keyword">if</span> (label &lt; <span class="number">0</span>):</div><div class="line">        xcord0.append(xPt)</div><div class="line">        ycord0.append(yPt)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        xcord1.append(xPt)</div><div class="line">        ycord1.append(yPt)</div><div class="line">k1 = <span class="number">1.3</span> <span class="comment"># 修改测试</span></div><div class="line">sVs = svm.testRbf(k1)</div><div class="line">m = shape(sVs)[<span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">    x = sVs.A[i][<span class="number">0</span>]</div><div class="line">    y = sVs.A[i][<span class="number">1</span>]</div><div class="line">    circle = Circle((x,y), <span class="number">0.05</span>, facecolor=<span class="string">'none'</span>, edgecolor=(<span class="number">0</span>, <span class="number">0.8</span>, <span class="number">0.8</span>), linewidth=<span class="number">3</span>, alpha=<span class="number">0.5</span>)</div><div class="line">    ax.add_patch(circle)</div><div class="line">ax.scatter(xcord0, ycord0, marker=<span class="string">'s'</span>, s=<span class="number">30</span>)</div><div class="line">ax.scatter(xcord1, ycord1, marker=<span class="string">'o'</span>, s=<span class="number">30</span>, c=<span class="string">'red'</span>)</div><div class="line">plt.title(<span class="string">'RBF k1 = %f, %d Support Vectors'</span> %(k1, m))</div><div class="line">plt.show()</div><div class="line">fr.close()</div></pre></td></tr></table></figure><p><img src="/2017/10/22/machine-learning-in-action-note4/k_1.3.png" alt="k_1.3"></p><p><img src="/2017/10/22/machine-learning-in-action-note4/k_0.1.png" alt="k_0.1"></p><p>可以看出参数值越小支持向量的范围越模糊，选择合适的参数还是很重要的。</p><p><strong>参考链接</strong></p><ul><li><a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">https://www.zhihu.com/question/21094489</a></li><li><a href="https://zhuanlan.zhihu.com/p/24638007" target="_blank" rel="external">零基础学SVM—Support Vector Machine(一)</a> </li><li><a href="http://blog.pluskid.org/?p=632" target="_blank" rel="external">http://blog.pluskid.org/?p=632</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机（Support Vector Machine）真的不是很好理解啊，虽然作者给出了代码，emmmmm…..还是稍微记录一下吧。&lt;/p&gt;
&lt;p&gt;上一章学习的「对数几率函数」中，我们提到了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;利用线性回归模型的预测结果去逼近真实标记的对数几率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;标记结果为1／0。但SVM中，我们目标是找出具有“最大间隔”（maximum margin）的「划分超平面」，标记结果为-1／1。&lt;/p&gt;
&lt;p&gt;上面一句话出现了两个好像似懂非懂的名词：最大间隔和划分超平面。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2017/10/22/machine-learning-in-action-note4/svmpic.jpg&quot; alt=&quot;svmpic&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记三：对数几率回归</title>
    <link href="http://yoursite.com/2017/10/20/machine-learning-in-action-note3/"/>
    <id>http://yoursite.com/2017/10/20/machine-learning-in-action-note3/</id>
    <published>2017-10-20T08:24:26.000Z</published>
    <updated>2017-11-07T11:53:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。</p><p>「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：<br>$$<br>f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i<br>$$<br>有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即<br>$$<br>\ln y = w^Tx + b<br>$$<br>这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）<br>$$<br>y = g^{-1}(w^Tx + b)<br>$$<br>其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。</p><a id="more"></a><p>其次，我们想用回归分析来做<strong>分类任务</strong>怎么办？那么我们可以找一个单调可微函数，将分类任务的真实标记y与线性回归模型的预测值 <strong>z=w^T^x+b</strong> 联系起来。考虑最简单的二分类任务，我们想到了“单位阶跃函数”（unit-step function）<br>$$<br>y = \begin{cases}0, z<0\\0.5, z="0\\1,z">0<br>\end{cases}<br>$$<br>但单位阶跃函数不连续，因此不能作为g(･)。于是聪明的人们就找到了Sigmoid函数，也称对数几率函数（logistic function）：<br>$$<br>y = \frac{1}{1+e^{-z}}<br>$$<br>类比上面的线性回归我们可以得到：<br>$$<br>\ln\frac{y}{1-y} = w^Tx + b<br>$$<br>由此就引发了关于 Logistic Regression 中文名的思考。大多数看到的翻译都是「逻辑回归」，其中逻辑一词代表什么一直不懂。但在西瓜书中作者称之为「对数几率回归」，其言语是让我信服的。</0\\0.5,></p><p>将 y 视为样本x作为正例的可能性，则 1-y 时其反例的可能性，两者比值称为“对率”（odds）：<br>$$<br>\frac{y}{1-y}<br>$$<br>对率反映了x作为正例的相对可能性。对几率取对数则得到“对数几率”（log odds，亦称logit）：<br>$$<br>\ln\frac{y}{1-y}<br>$$<br>由此可看出，实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。</p><p>不管怎么说，在理解一个回归模型的时候至少先把它的名字念对吧233</p><h3 id="分析：可视化分析数据"><a href="#分析：可视化分析数据" class="headerlink" title="分析：可视化分析数据"></a>分析：可视化分析数据</h3><p>在调用plotBestFit时，有一个<code>weights.getA())</code>这个getA()是啥玩意儿？根据参考链接[1]-11楼的小伙伴给出了答案，矩阵通过这个getA()这个方法可以将自身返回成一个n维数组对象</p><p><img src="/2017/10/20/machine-learning-in-action-note3/geta_test.png" alt=""></p><p>从上图输出类型可以看出，使用getA函数将矩阵转化为了数组</p><p><img src="/2017/10/20/machine-learning-in-action-note3/matnarr.png" alt=""></p><p>当矩阵只有一行的时候输出 mat[1] 是会报错的</p><p><img src="/2017/10/20/machine-learning-in-action-note3/mat_trans.png" alt=""></p><p>将 mat 转制后 mat[1] 输出的是一个1x1的矩阵而不是一个数值</p><p>###训练：梯度下降（Gradient Descent）</p><p>在对数几率函数的训练过程中，最重要的就是如何训练权值w。为了使其公式化，我们定义了一个“代价函数”（cost function），来衡量预测值和实例标记的差距：<br>$$<br>J(w) = \frac{1}{2}\sum_{i=1}^m(h_w(x^{(i)})-y^{i})^2<br>$$<br>我们希望这个值很小，于是就有了梯度下降算法的迭代公式：<br>$$<br>w_j := w_j - 𝛼\frac{\partial }{\partial w_j}J(w)<br>$$<br>让我们针对一个训练样本 (x, y) 算算这个偏导是啥：<br>$$<br>\begin{align}\frac{\partial }{\partial w_j}J(w)   &amp; =   \frac{\partial }{\partial w_j}\frac{1}{2}(h_w(x)-y)^2\\<br>&amp; =  2･\frac{1}{2}(h_w(x)-y)･\frac{𝜕}{𝜕w_j}(h_w(x)-y)\\<br>&amp; = (h_w(x)-y)･\frac{𝜕}{𝜕w_j}(\sum_{i=0}^nw_ix_i-y)\\<br>&amp; = (h_w(x)-y)x_j\end{align}<br>$$<br>因此对于一个训练样本有：<br>$$<br>w_j := w_j - 𝛼(y^{i}-h_w(x^{i}))x_j^{(i)}<br>$$<br>运用在多于一个样本的训练集上时，我们有两种选择：</p><ol><li><p><strong>批量梯度下降（batch gradient descent）</strong></p><p>将所有权值初始化为1</p><p>循环R次: </p><p>​    计算整个训练集的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataMatrix = mat(dataMatIn) <span class="comment"># m x n</span></div><div class="line">    labelMat = mat(classLabels).transpose() <span class="comment">#  1x100 to 100x1</span></div><div class="line">    m, n = shape(dataMatrix)</div><div class="line">    alpha = <span class="number">0.001</span></div><div class="line">    maxCycles = <span class="number">500</span></div><div class="line">    weights = ones((n,<span class="number">1</span>)) <span class="comment"># n x 1</span></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</div><div class="line">        h = sigmoid(dataMatrix * weights) <span class="comment"># m x 1, 整个训练集</span></div><div class="line">        error = labelMat - h</div><div class="line">        weights = weights + alpha * dataMatrix.transpose()*error</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure></li><li><p><strong>随机梯度下降（stochastic gradient descent／incremental gradient descent）</strong></p><p>将所有权值初始化为1</p><p>对每一个训练样本：</p><p>​    计算训练样本的梯度</p><p>​    通过 alpha * gradient 更新权值向量</p><p>​    返回权值向量</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatIn, classLabels)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">        h = sigmoid(sum(dataArr[i] * weights)) <span class="comment"># single value，一个训练样本</span></div><div class="line">        error = classLabels[i] - h <span class="comment"># single value</span></div><div class="line">        weights = weights + alpha * error * dataArr[i]</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><p>两种方法的区别在于计算 <strong>weights</strong> 时，「批量梯度下降」每次都用整个训练集来更新权值。假设我们有m个实例和n个特征，每次就要做 <strong>m*n</strong> 次乘法，当 <strong>m</strong> 非常大时，计算代价是很高的。而「随机梯度下降」每次只用一个训练样本，它同时也是一种在线学习算法。虽然它有时候很难安全等于最优值，但也差不多了。因此，我们一般使用「随机梯度下降」来训练权值。</p><p>接下来我们对随机梯度下降函数做一些修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatIn, classLabels, numIter=<span class="number">150</span>)</span>:</span></div><div class="line">    dataArr = array(dataMatIn)</div><div class="line">    m, n = shape(dataArr)</div><div class="line">    alpha = <span class="number">0.01</span></div><div class="line">    weights = ones(n)</div><div class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(numIter): <span class="comment"># 迭代次数</span></div><div class="line">        dataIndex = list(range(m))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">            <span class="comment"># 1/ Alpha changes with each iteration</span></div><div class="line">            alpha = <span class="number">4</span>/(<span class="number">1</span>+iter+i) + <span class="number">0.01</span></div><div class="line">            <span class="comment"># 2/ Update vectors are randomly selected</span></div><div class="line">            randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</div><div class="line">            h = sigmoid(sum(dataArr[randIndex] * weights)) <span class="comment"># single value</span></div><div class="line">            error = classLabels[randIndex] - h <span class="comment"># single value</span></div><div class="line">            weights = weights + alpha * error * dataArr[randIndex]</div><div class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</div><div class="line">    <span class="keyword">return</span> weights</div></pre></td></tr></table></figure><h3 id="预测：从疝气病预测病马的死亡率"><a href="#预测：从疝气病预测病马的死亡率" class="headerlink" title="预测：从疝气病预测病马的死亡率"></a>预测：从疝气病预测病马的死亡率</h3><p>这里插播一下如何处理数据的缺失值。面对缺失的一些特征我们有如下选择：</p><ol><li>使用可用特征的均值来填补缺失值</li><li>使用特殊值来填补缺失值，如-1</li><li>忽略有缺失值的样本</li><li>使用相似样本的均值填补缺失值</li><li>使用另外的机器学习算法预测缺失值</li></ol><p>在「对数几率回归」中，特征的缺失值可以用0来填补。因为 <code>weights = weights + alpha _ error _ dataArr[randIndex]</code>，当对应特征值为0时，该特征的系数值将不变，则 <code>weights = weights</code>。由于 <code>sigmoid（0）=0.5</code>，对于结果的预测不具有任何倾向性。但是标记缺失的样本我们就不得不丢弃了啊（都不知道你有病没病要你何用</p><p>好的，说了这么多，作者已经帮我们处理完数据了。</p><p>终于我们要开始预测了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></div><div class="line">    prob = sigmoid(sum(inX*weights))</div><div class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>: <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div></pre></td></tr></table></figure><p><img src="/2017/10/20/machine-learning-in-action-note3/test_result.png" alt=""></p><p>我的妈呀训练快一小时呢。。。</p><p><strong>参考链接：</strong></p><p>[1] <a href="http://tieba.baidu.com/p/2905471495" target="_blank" rel="external">http://tieba.baidu.com/p/2905471495</a></p><p>[2] 周志华《机器学习》</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先我们先来说说「回归」。wiki告诉我们，回归指的是研究变量关系的一种统计分析方法。回归分析是一种数学模型，而我们最熟悉的非「线性模型」莫属了。以下引入的理论来源于西瓜书。&lt;/p&gt;
&lt;p&gt;「线性回归」试图通过给定数据集学得一个「线性模型」以尽可能准确地预测输出标记。用公式来表示：&lt;br&gt;$$&lt;br&gt;f(x_i) = wx_i+b，\\ 使得f(x_i) \approx y_i&lt;br&gt;$$&lt;br&gt;有时我们示例对应的输出标记是在指数尺度上变化，那就可将输出标记的对数作为线性模型逼近的目标，即&lt;br&gt;$$&lt;br&gt;\ln y = w^Tx + b&lt;br&gt;$$&lt;br&gt;这就是”对数线性回归“（log-linear regression）。从而引出了一个”广义线性模型“（generalized linear model）&lt;br&gt;$$&lt;br&gt;y = g^{-1}(w^Tx + b)&lt;br&gt;$$&lt;br&gt;其中函数g(･)称为“联系函数”（link function）。对数回归是广义线性模型在g(･) = ln(･) 时的特例。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>image-classification-note</title>
    <link href="http://yoursite.com/2017/10/19/image-classification-note/"/>
    <id>http://yoursite.com/2017/10/19/image-classification-note/</id>
    <published>2017-10-19T07:41:20.000Z</published>
    <updated>2017-10-21T09:39:25.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems:"></a>Problems:</h3><ol><li>Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.</li><li>Viewpoint variation: All pixels change when the camera moves</li><li>Illumination: There can be lighting conditions going on in the scene</li><li>Deformation: Cats can assume a lot of different, varied poses and positions.</li><li>Occlusion: You might only see a part of a cat.</li><li>Background Clutter: The foreground of the cat look similar in appearance</li><li>Intraclass variation: Cats can come in different shapes and sizes and colors and ages</li></ol><a id="more"></a><h3 id="An-image-classifier"><a href="#An-image-classifier" class="headerlink" title="An image classifier"></a>An image classifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify_image</span><span class="params">(image)</span>:</span></div><div class="line"><span class="comment"># Some magic here?</span></div><div class="line">    <span class="keyword">return</span> class_label</div></pre></td></tr></table></figure><p> <strong>no obvious way</strong> to hard-code the algorithm for recognizing a cat, or other classes.</p><h3 id="Data-Driven-Approach"><a href="#Data-Driven-Approach" class="headerlink" title="Data-Driven Approach"></a>Data-Driven Approach</h3><ol><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(images, labels)</span>:</span></div><div class="line"><span class="comment"># Machine learning</span></div><div class="line"><span class="keyword">return</span> model</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, test_images)</span>:</span></div><div class="line"><span class="comment"># Use model to predict labels</span></div><div class="line">    <span class="keyword">return</span> test_labels</div></pre></td></tr></table></figure><p>Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One called <strong>train</strong>, that’s going to input images and labels and then output a model, another function called <strong>predict</strong>, which will input the model and make predictions for images.</p><p>#Nearest Neighbor classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span>:</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line"><span class="keyword">pass</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></div><div class="line"><span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></div><div class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></div><div class="line">    self.Xtr = X</div><div class="line">    self.ytr = y</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">     <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></div><div class="line">    num_test = X.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></div><div class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</div><div class="line">    </div><div class="line">    <span class="comment"># loop over all test rows</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</div><div class="line"><span class="comment"># find the nearest training image to the i'th test image</span></div><div class="line">     <span class="comment"># using the L1 distance (sum of absolute alue differences)</span></div><div class="line">        distances = np.sum(np.abs(self.Xtr - X[i, :]), axis = <span class="number">1</span>)</div><div class="line">        min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></div><div class="line">Ypred[i] = self.ytr[min_index] <span class="comment">#predict the label of the nearest example</span></div><div class="line">     <span class="keyword">return</span> Ypred</div></pre></td></tr></table></figure><p>Q: With N examples. how fast are training and prediction?</p><p>A: Train O(1), predict O(N)</p><p>This is bad: we want classifiers that are <strong>fast</strong> at prediciton; <strong>slow</strong> for training is ok.</p><h3 id="k-Nearest-Neighbors"><a href="#k-Nearest-Neighbors" class="headerlink" title="k-Nearest Neighbors"></a>k-Nearest Neighbors</h3><p>Instead of copying label from nearest neighbor, thake <strong>majority vote</strong> form K closest points.</p><p>###Hyperparameters</p><ul><li>What is the best value of <strong>k</strong> to use?</li><li>What is the best <strong>distance</strong> to use?</li></ul><p>These are <strong>hyperparameters</strong>: choices about the algorithm that we set rather than learn</p><p>_Very problem-dependent._</p><p>_Must try them all out and see what works best._</p><h3 id="Setting-Hyperparameters"><a href="#Setting-Hyperparameters" class="headerlink" title="Setting Hyperparameters"></a>Setting Hyperparameters</h3><ul><li>Split data into <strong>train</strong>, <strong>val</strong>, and <strong>test</strong>; choose hyperparameters on val and evaluate on test</li><li><strong>Cross-Validation</strong>: Split data into **folds, try each fold as validation and average the results. Useful for small datasets but not used too frequently in deep learning.</li></ul><h3 id="k-Nearest-Neighbor-on-images-never-used"><a href="#k-Nearest-Neighbor-on-images-never-used" class="headerlink" title="k-Nearest Neighbor on images never used"></a>k-Nearest Neighbor on images never used</h3><ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li><li>Curse of dimensionality</li></ul><h3 id="k-Nearest-Neighbors-Summary"><a href="#k-Nearest-Neighbors-Summary" class="headerlink" title="k-Nearest Neighbors: Summary"></a>k-Nearest Neighbors: Summary</h3><ul><li>In <strong>Image classification</strong> we start with a <strong>training set</strong> of images and labels, and must predict labels on the <strong>test set</strong></li><li>The *K-Nearest Neighbors classifier predicts labels based on nearest training examples</li><li>Distance metric and K are <strong>hyperparameters</strong></li><li>Choose hyperparameters using the <strong>validation set</strong>; only run on the test set once at the very end!</li></ul><p>#Linear Classification</p><p>These deep neural networks are kind of like Legos and this linear classifier is kind of like the most basic building blocks of these giant networks.</p><p>f(x, W) = Wx + b</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Problems&quot;&gt;&lt;a href=&quot;#Problems&quot; class=&quot;headerlink&quot; title=&quot;Problems:&quot;&gt;&lt;/a&gt;Problems:&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Semantic Gap: There’s a huge gap between the semantic idea of a cat, and these pixel values that the computer is actually seeing.&lt;/li&gt;
&lt;li&gt;Viewpoint variation: All pixels change when the camera moves&lt;/li&gt;
&lt;li&gt;Illumination: There can be lighting conditions going on in the scene&lt;/li&gt;
&lt;li&gt;Deformation: Cats can assume a lot of different, varied poses and positions.&lt;/li&gt;
&lt;li&gt;Occlusion: You might only see a part of a cat.&lt;/li&gt;
&lt;li&gt;Background Clutter: The foreground of the cat look similar in appearance&lt;/li&gt;
&lt;li&gt;Intraclass variation: Cats can come in different shapes and sizes and colors and ages&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记二：朴素贝叶斯</title>
    <link href="http://yoursite.com/2017/10/18/machine-learning-in-action-note2/"/>
    <id>http://yoursite.com/2017/10/18/machine-learning-in-action-note2/</id>
    <published>2017-10-18T00:52:21.000Z</published>
    <updated>2017-11-06T14:30:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。</p><p>假设有i个分类，我们需要比较的其实是后验概率 <strong>P(Y=c~k~|X=x)</strong> 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。</p><p>那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：<br>$$<br>P(A|B) = P(A)\frac{P(B|A)}{P(B)}<br>$$<br>让我们来代入一下：</p><p>$$<br>P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}<br>$$<br>给一个训练集，<strong>P(Y=c~i~)</strong>是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：<br>$$<br>P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)<br>$$<br>由于分母 <strong>P(X=x)</strong> 对所有c~i~都没差，那我们大可不必计算出这个值。</p><p>###朴素贝叶斯学习与分类的算法过程：</p><p>输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x</p><p>输出：实例x的分类</p><p>1) 计算先验概率和条件概率</p><p>$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$</p><p>$$<br>P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}<br>$$</p><p>$$<br>j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K<br>$$</p><p>2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算<br>$$<br>P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><p>3) 确定实例x的类<br>$$<br>y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K<br>$$</p><a id="more"></a><h3 id="一个栗子：邮件分类问题"><a href="#一个栗子：邮件分类问题" class="headerlink" title="一个栗子：邮件分类问题"></a>一个栗子：邮件分类问题</h3><p>公式都有了，到底要如何实现呢？我们通过邮件分类问题来引入解决办法。</p><p>问题：假设有很多个邮件，标记为<strong>1-垃圾邮件</strong>，<strong>0-正常邮件</strong>，给定一个实例判断它属于哪一类。</p><p>思路：首先在啥都没有的情况下，我们要处理邮件，将其转化为python可以理解的list，并为出现的所有单词构建一个词汇表，每封邮件对应为词汇表上的一个向量，所有的邮件构成一个矩阵，利用矩阵计算出先验概率和条件概率，将给定实例转化为向量，通过比较大小确定实例的类。</p><ol><li><p>处理邮件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</div><div class="line">        wordList = textParse(open(<span class="string">'email/spam/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>,<span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList) <span class="comment"># 文档合集</span></div><div class="line">        fullText.extend(wordList) <span class="comment"># 单词合集</span></div><div class="line">        classList.append(<span class="number">1</span>)</div><div class="line">        wordList = textParse(open(<span class="string">'email/ham/%d.txt'</span> % i, <span class="string">"rb"</span>).read().decode(<span class="string">'GBK'</span>, <span class="string">'ignore'</span>))</div><div class="line">        docList.append(wordList)</div><div class="line">        fullText.extend(wordList)</div><div class="line">        classList.append(<span class="number">0</span>)</div></pre></td></tr></table></figure></li><li><p>构建词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></div><div class="line">    vocabSet = set([])</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</div><div class="line">        vocabSet = vocabSet | set(document) <span class="comment"># 不重复的单词</span></div><div class="line">    <span class="keyword">return</span> list(vocabSet)</div><div class="line"> </div><div class="line">vocabList = vocabSet(docList)</div></pre></td></tr></table></figure></li><li><p>将所有邮件分为训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">trainIndex = list(range(<span class="number">50</span>)); testIndex = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    randIndex = int(random.uniform(<span class="number">0</span>, len(trainIndex)))</div><div class="line">    testIndex.append(trainIndex[randIndex])</div><div class="line">    <span class="keyword">del</span>(trainIndex[randIndex])</div></pre></td></tr></table></figure></li><li><p>将所有邮件转换为矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></div><div class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabList:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</div><div class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span> <span class="comment"># 转换为词汇表对应向量</span></div><div class="line">    <span class="keyword">return</span>  returnVec</div><div class="line"></div><div class="line">trainMat = []; trainClasses = []</div><div class="line"><span class="keyword">for</span> docIndex <span class="keyword">in</span> trainIndex:</div><div class="line">trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) </div><div class="line">    trainClasses.append(classList[docIndex])</div></pre></td></tr></table></figure></li><li><p>计算先验概率和条件概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></div><div class="line">    numTrainDocs = len(trainMatrix)</div><div class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</div><div class="line">    pAbusive = sum(trainCategory) / float(numTrainDocs) <span class="comment"># 先验概率 p(class=1)</span></div><div class="line"></div><div class="line">    <span class="comment"># 1/ Initialize probabilities</span></div><div class="line">    <span class="comment"># 拉普拉斯平滑处理</span></div><div class="line">    p0Num = ones(numWords) <span class="comment"># p(xi|c0)</span></div><div class="line">    p1Num = ones(numWords) <span class="comment"># p(xi|c1)</span></div><div class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</div><div class="line"></div><div class="line">        <span class="comment"># 2 / Vector addition</span></div><div class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</div><div class="line">            p1Num += trainMatrix[i]</div><div class="line">            p1Denom += sum(trainMatrix[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            p0Num += trainMatrix[i]</div><div class="line">            p0Denom += sum(trainMatrix[i])</div><div class="line"></div><div class="line">    <span class="comment"># 3/ Element-wise division</span></div><div class="line">    <span class="comment"># 条件概率</span></div><div class="line">    p1Vect = log(p1Num / p1Denom)</div><div class="line">    p0Vect = log(p0Num / p0Denom)</div><div class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</div><div class="line"></div><div class="line">p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</div></pre></td></tr></table></figure><p>让我们把条件概率用普通话稍微翻译一下：<br>$$<br>P(X=x_i|c_k) = \frac{单词x_i出现次数}{c_k类文档总单词数}<br>$$<br>把所有的 \(P(X=x_i|c_k)\) 捋到一起就是一个向量 piVcet。</p><p>在上面的代码中我们其实处理了两种极端情况：</p><ol><li>早些时候这个代码是<code>p0Num = zeros(numWords); p0Denom =0</code>（p1同理）。如果某个单词没有出现，当计算乘积 ΠP(X=x~i~|c~k~)时，结果就会为0，这显然就没有办法进行预测了嘛。拉普拉斯就提出用加1的方法估计没有出现过的现象的概率。</li><li>还有一个是下溢出问题。在计算乘积  \(\prod P(X=x_i|c_k)\)时，由于概率都是很小的数值，程序有可能下溢出而得不到正确答案。解决办法就是对乘积取自然对数。</li></ol></li><li><p>交叉验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></div><div class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</div><div class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1</span>-pClass1)</div><div class="line">    <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    </div><div class="line">    errorCount = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testIndex:</div><div class="line">        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])</div><div class="line">classifyResult = classifyNB(array(wordVector), p0V, p1V, pSpam)</div><div class="line">    <span class="keyword">if</span>  classifyResult != classList[docIndex]:</div><div class="line">   errorCount += <span class="number">1</span></div><div class="line">print(<span class="string">"the error rate is: "</span>, float(errorCount)/len(testIndex))</div></pre></td></tr></table></figure></li></ol><p>这里我们终于要计算\(\prod P(X=x_i|c_k)\)了。<code>vec2Classify _ piVec</code> 表示先找出实例有的单词，由于\(\ln(a_b) = ln(a)+ln(b)\)，条件概率的相乘便转换为矩阵元素相加。</p><p>🙂 适用于量少的数据集／可以处理多类别问题</p><p>🙁 对输入数据的准备方式较敏感</p><p>🛠 标称型数据</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;朴素贝叶斯是贝叶斯决策理论的一部分，所以我们来先来看看贝叶斯决策理论。贝叶斯决策理论的核心是选择具有最高概率的决策。&lt;/p&gt;
&lt;p&gt;假设有i个分类，我们需要比较的其实是后验概率 &lt;strong&gt;P(Y=c~k~|X=x)&lt;/strong&gt; 的大小。那么后验概率是什么鬼？wiki说一个随机事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。简而言之，后验概率就是条件概率。换算成我们需要解决的问题就是，在已知一堆特征的情况下，计算出它属于各个类的概率，选择概率最大的那一类作为预测的类别。&lt;/p&gt;
&lt;p&gt;那么问题的关键就转向了如何计算这个后验概率。贝叶斯给出了一个公式：&lt;br&gt;$$&lt;br&gt;P(A|B) = P(A)\frac{P(B|A)}{P(B)}&lt;br&gt;$$&lt;br&gt;让我们来代入一下：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(Y=c_k|X=x) = P(Y=c_k)\frac{P(X=x|Y=c_k)}{P(X=x)}&lt;br&gt;$$&lt;br&gt;给一个训练集，&lt;strong&gt;P(Y=c~i~)&lt;/strong&gt;是很好求的，那它邻居那坨玩意儿怎么办？这里就引出了一个很重要的（敲黑板）关于「朴素」一词的含义。它给出了一个假设：假设所有条件都是相互独立的。意思是一个特征出现的概率并不依赖其他特征。因此我们不妨先考虑一下邻居的分子：&lt;br&gt;$$&lt;br&gt;P(X=x|Y=c_k) = P(X_1=x_1,…,X_n=x_n|Y=c_k) = \prod_{j=1}^{n}P(X_j=x_j|Y=c_k)&lt;br&gt;$$&lt;br&gt;由于分母 &lt;strong&gt;P(X=x)&lt;/strong&gt; 对所有c~i~都没差，那我们大可不必计算出这个值。&lt;/p&gt;
&lt;p&gt;###朴素贝叶斯学习与分类的算法过程：&lt;/p&gt;
&lt;p&gt;输入：数据集 \(T = {(x_1,y_1), (x_~2,y_2), … ,(x_n, y_n)}\)，其中\(x_i = (x_i^1, x_i^2, … , x_i^n)^T，x_i^j \)是第 i 个样本的第j个特征；实例x&lt;/p&gt;
&lt;p&gt;输出：实例x的分类&lt;/p&gt;
&lt;p&gt;1) 计算先验概率和条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}，k=1,2,…K$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;P(X^j = a_{jl} | Y=c_k) = \frac{\sum_{i=1}^NI(x_i^j = a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i)=c_k}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;2) 对于给定实例x=(x^1^, x^2^, … , x^n^)，计算&lt;br&gt;$$&lt;br&gt;P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;3) 确定实例x的类&lt;br&gt;$$&lt;br&gt;y=\max P(Y=c_k)\prod_{j=1}^{n}P(X^j = x^j|Y=c_k) ,  k=1,2,…K&lt;br&gt;$$&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Machine Learning in Action》学习笔记一：kNN和决策树</title>
    <link href="http://yoursite.com/2017/10/15/machine-learning-in-action-note1/"/>
    <id>http://yoursite.com/2017/10/15/machine-learning-in-action-note1/</id>
    <published>2017-10-15T13:58:12.000Z</published>
    <updated>2017-11-06T14:02:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。</p><p>算法的一般流程为：收集数据 -&gt; 准备数据 -&gt; 分析数据 -&gt; 训练算法 -&gt; 测试算法 -&gt; 使用算法</p><p>第一个笔记本包括kNN分类算法和决策树算法。</p><h1 id="kNN分类算法"><a href="#kNN分类算法" class="headerlink" title="kNN分类算法"></a>kNN分类算法</h1><p>kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。</p><h3 id="kNN的一般流程"><a href="#kNN的一般流程" class="headerlink" title="kNN的一般流程"></a>kNN的一般流程</h3><ol><li>收集数据</li><li>准备数据：最好使用结构化数据格式，因为计算距离需要数值。</li><li>分析数据</li><li>训练算法：此步骤不适用于kNN算法</li><li>测试算法：计算错误率</li><li>使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。</li></ol><a id="more"></a><p>我们一般使用欧氏距离公式来计算距离（以两点为例）：</p><p>$$<br>d = \sqrt{(x_0-y_0)^2+(x_1-y_1)^2+…+(x_n-y_n)^2)}<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div></pre></td></tr></table></figure><p>对距离进行排序，并找出最小的k个距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line">classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k): </div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div></pre></td></tr></table></figure><p>最后对出现次数进行排序，预测结果即为出现次数最多的类别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line"><span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>原书中sorted的第一个参数为<code>classCount.iteritems</code>，根据python3作出修改。</p><p>完整的分类函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></div><div class="line">    <span class="string">"""k-Nearest Neighbors algorithm</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    :param inX: A row vector under test</span></div><div class="line"><span class="string">    :param dataSet: The training data</span></div><div class="line"><span class="string">    :param labels: Labels of training data</span></div><div class="line"><span class="string">    :return: Prediction for the class of inX</span></div><div class="line"><span class="string">    """</span></div><div class="line"></div><div class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Distance calculation</span></div><div class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment"># 计算预测向量inX与每个样本的差值矩阵(mxn)</span></div><div class="line">sqDiffMat = diffMat ** <span class="number">2</span> <span class="comment"># 将矩阵的每个元素都平方(mxn)</span></div><div class="line">sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment"># 矩阵每一行向量相加（mx1）</span></div><div class="line">distances  = sqDistances ** <span class="number">0.5</span> <span class="comment"># 将矩阵的每个元素开根号(mx1)</span></div><div class="line">    sortedDistIndicies = distances.argsort() <span class="comment"># 返回数组从小到大排序后的索引值</span></div><div class="line"></div><div class="line">    <span class="comment"># 2 Voting with lowest k distances</span></div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]] <span class="comment"># 获得距离第i小的样本类别</span></div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># 记录该类别的出现次数</span></div><div class="line"></div><div class="line">    <span class="comment"># 3 Sort dictionary</span></div><div class="line">    sortedClassCount = sorted(classCount.items(),</div><div class="line">                              key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure><p>🙂：准确度高／对异常值不敏感／不假设数据</p><p>🙁：计算复杂度高／占用大量内存</p><p>🛠：数值型／标称型</p><h1 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h1><p>决策树算法（Decision trees）也是一种常见的分类算法。我们每次用一个特征来对数据集进行分类，迭代直到分出的数据集都属于一个类别时，训练完成，训练模型即为一个树结构。</p><h3 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h3><ol><li>数据收集</li><li>准备数据：这个构造树的过程只适用于标称型数据，因此需要离散化连续的数值</li><li>分析数据</li><li>训练算法：构造一个树的数据结构</li><li>测试算法：使用经验树计算错误率</li><li>使用算法：这可以应用于任何监督学习任务。通常，决策树树可以更好的理解数据的内在含义。</li></ol><p>判断使用哪一个特征来进行分类即为训练算法的关键。在这里，我们使用特征的「信息增益」来判断。好了，我们得来复习一下「信息增益」。</p><p>说到信息增益就有一个不得不提的家伙叫「熵」，它是随机变量的平均量，代表了随机变量的不确定性。啥玩意儿？？？一个事件发生的概率越小，所含的信息量就越大，如果所有事件发生的概率都很小，平均信息量就比较大。讲人话。「熵」越大，数据集越混乱。</p><p>用公式来表示：<br>$$<br>H = -\sum_{i=1}^np(x_i)log_2p(x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></div><div class="line">    numEntries = len(dataSet)</div><div class="line"></div><div class="line">    labelCounts = &#123;&#125; <span class="comment"># 用一个字典记录一个类别的出现次数</span></div><div class="line">    <span class="keyword">for</span> featVect <span class="keyword">in</span> dataSet:</div><div class="line">        currentLabel = featVect[<span class="number">-1</span>]</div><div class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</div><div class="line">            labelCounts[currentLabel] = <span class="number">0</span></div><div class="line">        labelCounts[currentLabel] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># 计算熵</span></div><div class="line">    shannonEnt = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">        prob = float(labelCounts[key])/numEntries <span class="comment"># 概率</span></div><div class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> shannonEnt</div></pre></td></tr></table></figure><p>还有一个家伙叫「条件熵」，是已知条件下，随机变量的不确定性。即当我们固定了一个特征时，整个系统的信息量。然而这个特征的取值也不止一个，因此我们就需要求出它的平均值。</p><p>用公式来表示：<br>$$<br>H(C|X) = p_1H(C|x_1)+p_2(C|x_2)+…p_n(C|x_n) = \sum_{i=1}^np_iH(C|x_i)<br>$$<br>用代码来表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line">featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">newEntropy = <span class="number">0</span></div><div class="line">    </div><div class="line">    <span class="comment"># 计算条件熵</span></div><div class="line"><span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">      subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">   prob = len(subDataSet)/float(len(dataSet))</div><div class="line">   newEntropy += prob * calcShannonEnt(subDataSet)</div></pre></td></tr></table></figure><p>接着又来了一个家伙，它就是「信息增益」。<strong>信息增益 = 熵 - 条件熵</strong>。这个信息增益代表着，系统固定一个特征后，<strong>不确定性的减少程度</strong>。我们当然希望这个减少程度尽可能大，使得系统更加有组织纪律，那么信息增益很大就表明这个特征很关键！！</p><p>我们将上面的内容整理进一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></div><div class="line">    </div><div class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment"># 获取特征的数量</span></div><div class="line">    baseEntropy = calcShannonEnt(dataSet)</div><div class="line">    bestInfoGain = <span class="number">0</span>; bestFeature = <span class="number">-1</span></div><div class="line">    </div><div class="line">    <span class="comment"># 分别计算每一个特征的信息增益</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line"></div><div class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">        uniqueVals = set(featList) <span class="comment"># 找出第i个特征的所有取值</span></div><div class="line">        newEntropy = <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># 计算条件熵</span></div><div class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">            subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">            prob = len(subDataSet)/float(len(dataSet))</div><div class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line">        <span class="comment"># 计算信息增益</span></div><div class="line">        infoGain = baseEntropy - newEntropy</div><div class="line">        </div><div class="line">        <span class="comment"># 找到最大的信息增益</span></div><div class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line">    <span class="keyword">return</span>  bestFeature <span class="comment"># 返回最佳分类特征</span></div></pre></td></tr></table></figure><p>整理到这里，其实有一个疑惑，我觉得信息增益并没有什么卵用，每次找出条件熵最小的不就行了？？？经实验是可以的。</p><p>在treePlotter.py中发现一个神奇的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></div><div class="line">    <span class="comment"># 2 Draws annotations with arrows</span></div><div class="line">    createPlot.ax.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           xytext = centerPt, textcoords=<span class="string">'axes fraction'</span>,</div><div class="line">                           va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodeType, arrowprops=arrow_args)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></div><div class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</div><div class="line">    fig.clf()</div><div class="line">    createPlot.ax = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>)</div><div class="line">    plotNode(<span class="string">'a decision node'</span>, (<span class="number">.5</span>, <span class="number">.1</span>), (<span class="number">.1</span>, <span class="number">.5</span>), decisionNode)</div><div class="line">    plotNode(<span class="string">'a leaf node'</span>, (<span class="number">.8</span>, <span class="number">.1</span>), (<span class="number">.3</span>, <span class="number">.8</span>), leafNode)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Python中一切皆为对象，在createPlot函数中，为这个函数对象绑定了一个属性ax，变成了一个全局的变量，可以在plotNode函数中调用。</p><p>书中作者利用treePlotter中写好的树结构来进行预测，但tree.py里头不有一个createDataSet函数和createTree函数吗？尝试一下利用这两个函数来生成并预测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels)</div></pre></td></tr></table></figure><p>Buuuuuuuut……….</p><p><img src="/2017/10/15/machine-learning-in-action-note1/no_surfacing_error.png" alt=""></p><p>检查了一下调用createTree函数前后labels的值，发现调用前后labels的值发生了变化，因此对原函数createTree稍作修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></div><div class="line"></div><div class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line"></div><div class="line">    <span class="comment"># 1 Stop when all classes are equal</span></div><div class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</div><div class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 2 When just one feature, return majority</span></div><div class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> majorityCnt(classList)</div><div class="line"></div><div class="line">    subLabels = labels[:] <span class="comment"># 将labels全部复制到subLabels，进行接下来的处理</span></div><div class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">    bestFeatLabel = subLabels[bestFeat] <span class="comment"># 利用subLabels来获取最佳分类特征</span></div><div class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</div><div class="line">    <span class="keyword">del</span>(subLabels[bestFeat])</div><div class="line"></div><div class="line">    <span class="comment"># 3 Get list of unique values</span></div><div class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">    uniqueVals = set(featValues)</div><div class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">        <span class="comment">#subLabels = labels[:] 作者在这里才进行参数复制，导致labels的值发生改变</span></div><div class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</div><div class="line">    <span class="keyword">return</span> myTree</div></pre></td></tr></table></figure><p>接下来就可以愉快利用该函数进行分类了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> treePlotter</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">省略一堆作者源代码</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line">myDat, labels = createDataSet()</div><div class="line">myTree = createTree(myDat, labels) <span class="comment"># 训练数据</span></div><div class="line">treePlotter.createPlot(myTree) <span class="comment"># 显示模型</span></div><div class="line">print(classify(myTree, labels, [<span class="number">1</span>, <span class="number">1</span>])) <span class="comment"># 预测数据</span></div></pre></td></tr></table></figure><p>看一看生成的决策树模型：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/tree1.png" alt=""></p><p>我们可以引入pickle模块来将训练出的模型序列化，保存在磁盘中，以便后续的调用。因为书本作者是使用Python2的，我打算用Python3来完成，在下面的代码中就遇到了问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></div><div class="line">    <span class="keyword">import</span> pickle</div><div class="line">    <span class="comment">#Python2用法：fw = open(filename,'w')</span></div><div class="line">    <span class="comment">#改为python3:</span></div><div class="line">    <span class="keyword">with</span> open(filename,<span class="string">'wb'</span>) <span class="keyword">as</span> fw:</div><div class="line">        pickle.dump(inputTree, fw)</div><div class="line">    fw.close()</div></pre></td></tr></table></figure><p>接下来就要引入稍微大一点的数据集来进行训练了，然鹅….</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fr = open(<span class="string">'lenses.txt'</span>)</div><div class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readline()]</div><div class="line">lensesLables = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</div><div class="line">print(lenses)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/readlines_error.png" alt=""></p><p>这输出的啥玩意儿？仔细一看，<strong>fr.readlines()</strong>函数写错了，少了一个<strong>sssssssss</strong>。修改好以后我们就来看看训练完的决策树吧：</p><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree1.png" alt=""></p><p>？？？这又啥玩意儿？？这看得下去？？？？那就只能修改一下plotMidText函数了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></div><div class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>])/<span class="number">2</span> + cntrPt[<span class="number">0</span>]</div><div class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>])/<span class="number">2</span> + cntrPt[<span class="number">1</span>]</div><div class="line">    createPlot.ax.text(xMid, yMid, txtString, fontsize=<span class="number">8</span>, horizontalalignment=<span class="string">'center'</span>,verticalalignment=<span class="string">'center'</span>, rotation=<span class="number">30</span>)</div></pre></td></tr></table></figure><p><img src="/2017/10/15/machine-learning-in-action-note1/lenses_tree2.png" alt=""></p><p>完美！</p><p>🙂：计算复杂度不高／便于人们理解学习结果／对中间的缺失值不敏感／可以处理无关的特征值</p><p>🙁：可能会过拟合</p><p>🛠：数值型／标称型</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正在学习《Machine Learning in Action》，随笔记录一下，代码基本上都是跟着书本敲的，就不大贴全部的代码了，仅写一些自己的理解并对出现的问题进行整理和归纳。&lt;/p&gt;
&lt;p&gt;算法的一般流程为：收集数据 -&amp;gt; 准备数据 -&amp;gt; 分析数据 -&amp;gt; 训练算法 -&amp;gt; 测试算法 -&amp;gt; 使用算法&lt;/p&gt;
&lt;p&gt;第一个笔记本包括kNN分类算法和决策树算法。&lt;/p&gt;
&lt;h1 id=&quot;kNN分类算法&quot;&gt;&lt;a href=&quot;#kNN分类算法&quot; class=&quot;headerlink&quot; title=&quot;kNN分类算法&quot;&gt;&lt;/a&gt;kNN分类算法&lt;/h1&gt;&lt;p&gt;kNN分类算法（k-Nearest Neighbors classification algorithm）是比较简单的一种分类算法。原理是通过计算距离找出与待预测数据最接近的k个类别，这k个类别中出现最多的类即为预测结果。&lt;/p&gt;
&lt;h3 id=&quot;kNN的一般流程&quot;&gt;&lt;a href=&quot;#kNN的一般流程&quot; class=&quot;headerlink&quot; title=&quot;kNN的一般流程&quot;&gt;&lt;/a&gt;kNN的一般流程&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;收集数据&lt;/li&gt;
&lt;li&gt;准备数据：最好使用结构化数据格式，因为计算距离需要数值。&lt;/li&gt;
&lt;li&gt;分析数据&lt;/li&gt;
&lt;li&gt;训练算法：此步骤不适用于kNN算法&lt;/li&gt;
&lt;li&gt;测试算法：计算错误率&lt;/li&gt;
&lt;li&gt;使用算法：这首先需要获取一些输入数据和结构化的输出数据，然后在输入数据上运行kNN算法并判断它属于哪一类，最后对计算出的分类执行后续处理。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>来生成一个中国地图吧</title>
    <link href="http://yoursite.com/2017/07/25/china-map/"/>
    <id>http://yoursite.com/2017/07/25/china-map/</id>
    <published>2017-07-25T02:49:22.000Z</published>
    <updated>2017-11-06T14:07:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>第一部分应该是根据作者2012年的文章进行的，第二部分根据作者2016年新的文章，在命令行显示人口密度。</p><a id="more"></a><h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><h3 id="1-获取地图文件并转换格式"><a href="#1-获取地图文件并转换格式" class="headerlink" title="1/ 获取地图文件并转换格式"></a>1/ 获取地图文件并转换格式</h3><p>首先据教程<a href="https://sandbox.idre.ucla.edu/sandbox/tutorials/installing-gdal-for-windows" target="_blank" rel="external">https://sandbox.idre.ucla.edu/sandbox/tutorials/installing-gdal-for-windows</a> 安装GDAL，并添加环境变量，要利用GDAL的org2org来转换文件。根据参考链接1，下载好全球地图以后，命令行运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ogr2ogr -f GeoJSON -where &quot;SU_A3 = &apos;CHN&apos; OR SU_A3 = &apos;TWN&apos;&quot; countries.json ne_10m_admin_0_countries.shp</div></pre></td></tr></table></figure><p>获取我们需要的城市，包括大陆和台湾。-where后面是我们需要筛选的条件，根据参考链接中国家的代码，中国是CHN台湾是TWN。</p><p>接下来是获取省份，命令行运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ogr2ogr -f GeoJSON -where &quot;gu_a3 = &apos;CHN&apos;&quot; provinces_china.json ne_10m_admin_1_states_provinces.shp</div></pre></td></tr></table></figure><h3 id="2-压缩文件"><a href="#2-压缩文件" class="headerlink" title="2/ 压缩文件"></a>2/ 压缩文件</h3><p>链接2给出了一个topojson的方法，不过我npm安装完里没有这玩意儿，仔细看了一下文章说是把GeoJSON文件格式转化为TopoJSON。有geo2topo这个方法，于是运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">geo2topo --id-propety SU_A3 -p name=NAME -p name -o countries_china_topo.json countries.json</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">geo2topo --id-propety SU_A3 -p name=NAME -p name -o provinces_china_topo.json provinces_china.json</div></pre></td></tr></table></figure><p>这个文件格式还是挺大的，继续上<a href="http://mapshaper.org/" target="_blank" rel="external">http://mapshaper.org/</a> 网站压缩，导出后可以看到结尾mini是最小的。</p><p><img src="/2017/07/25/china-map/cmp_countreis.png" alt=""></p><p><img src="/2017/07/25/china-map/cmp_provinces.png" alt=""></p><h3 id="3-敲代码"><a href="#3-敲代码" class="headerlink" title="3/ 敲代码"></a>3/ 敲代码</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&lt;<span class="selector-tag">style</span>&gt;</div><div class="line">  <span class="selector-id">#china</span> &#123;</div><div class="line">    <span class="attribute">stroke</span>: <span class="number">#fff</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="selector-class">.province</span> &#123;</div><div class="line">    <span class="attribute">fill</span>: <span class="number">#cde</span>;</div><div class="line">    <span class="comment">/* stroke-width: 1; */</span></div><div class="line">    <span class="attribute">stroke-width</span>: <span class="number">2px</span>;</div><div class="line">    <span class="attribute">cursor</span>: pointer;</div><div class="line">  &#125;</div><div class="line">  <span class="selector-class">.province</span><span class="selector-pseudo">:hover</span> &#123;</div><div class="line">    <span class="attribute">fill</span>: lightblue;</div><div class="line">  &#125;</div><div class="line">&lt;/style&gt;</div></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> width = <span class="built_in">window</span>.innerWidth,</div><div class="line">    height = <span class="built_in">window</span>.innerHeight,</div><div class="line">    countreis,</div><div class="line">    state</div><div class="line"></div><div class="line"><span class="keyword">var</span> projection = d3.geoMercator()</div><div class="line">  .center([<span class="number">104</span>, <span class="number">36</span>])</div><div class="line">  .scale(<span class="number">850</span>)</div><div class="line">  .translate([width / <span class="number">2</span>, height / <span class="number">2</span>])</div><div class="line"></div><div class="line"><span class="keyword">var</span> path = d3.geoPath()</div><div class="line">    .projection(projection)</div><div class="line"></div><div class="line"><span class="keyword">var</span> zoom = d3.zoom()</div><div class="line">    .scaleExtent([<span class="number">0.5</span>, <span class="number">10</span>])</div><div class="line">    .on(<span class="string">'zoom'</span>, zoomed)</div><div class="line"></div><div class="line"><span class="keyword">var</span> svg = d3.select(<span class="string">'.china-map'</span>)</div><div class="line">    .attr(<span class="string">"width"</span>, width)</div><div class="line">    .attr(<span class="string">"height"</span>, height)</div><div class="line">    .call(zoom)</div><div class="line"></div><div class="line"><span class="keyword">var</span> g = svg.append(<span class="string">'g'</span>)</div><div class="line"></div><div class="line">d3.json(<span class="string">'./provinces_china_mini.json'</span>, (err, chn) =&gt; &#123;</div><div class="line">  <span class="keyword">if</span>(err) <span class="built_in">console</span>.log(error(err))</div><div class="line">  <span class="built_in">console</span>.log(chn)</div><div class="line"></div><div class="line">  g.append(<span class="string">'g'</span>)</div><div class="line">    .attr(<span class="string">'id'</span>, <span class="string">'china'</span>)</div><div class="line">    <span class="comment">// .attr('id', 'provinces')</span></div><div class="line">    .selectAll(<span class="string">'path'</span>)</div><div class="line">    .data(topojson.feature(chn, chn.objects.provinces_china).features)</div><div class="line">    .enter()</div><div class="line">    .append(<span class="string">'path'</span>)</div><div class="line">    .attr(<span class="string">'class'</span>, <span class="string">'province'</span>)</div><div class="line">    .attr(<span class="string">'d'</span>, path)</div><div class="line">    .on(<span class="string">'click'</span>, clicked)</div><div class="line">    </div><div class="line">  g.selectAll(<span class="string">'.provinces-label'</span>)</div><div class="line">    .data(topojson.feature(chn, chn.objects.provinces_china).features)</div><div class="line">    .enter().append</div><div class="line">&#125;)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">zoomed</span>(<span class="params"></span>) </span>&#123;</div><div class="line">  g.attr(<span class="string">'transform'</span>, <span class="string">`translate(<span class="subst">$&#123;d3.event.transform.x&#125;</span>, <span class="subst">$&#123;d3.event.transform.y&#125;</span>) scale(<span class="subst">$&#123;d3.event.transform.k&#125;</span>)`</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>第一部分参考链接：</p><ol><li><a href="https://yukun.im/javascript/533" target="_blank" rel="external">https://yukun.im/javascript/533</a></li><li><a href="http://www.tnoda.com/blog/2013-12-07" target="_blank" rel="external">Interactive Map with d3.js</a></li><li><a href="https://msdn.microsoft.com/en-us/library/ee783932(v=cs.10" target="_blank" rel="external">Table of Country/Region Names and Codes</a>.aspx)</li></ol><h1 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h1><p>利用<strong>ndjson-reduce</strong>和<strong>ndjson-map</strong>把转化为GeoJSON格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ndjson-reduce &lt; ca-albers-density.ndjson | ndjson-map &quot;&#123;type: &apos;FeatureCollection&apos;, features: d&#125;&quot; &gt; ca-albers-densi ty.json</div></pre></td></tr></table></figure><p>（没想到windows这么看重单双引号</p><p>看看转换后的地图：</p><p><img src="/2017/07/25/china-map/ca-albers-density.png" alt=""></p><p>好像和原先的地图。。。倒了？？？</p><p>接下来安装d3<code>npm install -g d3</code>，通过<code>-r d3</code>引入d3模块，使用 Viridis 主体颜色来填充地区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ndjson-map -r d3 &quot;(d.properties.fill = d3.scaleSequential(d3.interpolateViridis).domain([0, 4000])(d.properties.de nsity), d)&quot; &lt; ca-albers-density.ndjson &gt; ca-albers-color.ndjson</div></pre></td></tr></table></figure><p>把上面生成的GeoJSON文件转化成SVG</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure><p>第二部分参考链接：</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一部分应该是根据作者2012年的文章进行的，第二部分根据作者2016年新的文章，在命令行显示人口密度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Tensorflow初体验</title>
    <link href="http://yoursite.com/2017/06/20/Tensorflow-test/"/>
    <id>http://yoursite.com/2017/06/20/Tensorflow-test/</id>
    <published>2017-06-20T02:49:03.000Z</published>
    <updated>2017-10-21T09:38:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>谷歌开源了物体识别系统的API，开源地址：<a href="https://github.com/tensorflow/models/tree/master/object_detection" target="_blank" rel="external">Tensor Flow Object Detectoin</a>，刚好最近也在学ML，学习了python，对sklearn的库也有一丢丢了解，来感受一下google的技术~</p><p>&lt;–more–&gt;</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>先抛出<a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/installation.md" target="_blank" rel="external">官方安装教程</a>，但是这个安装过程不太完整，比如 protobuf 的安装等，于是自己记录并做一些补充。</p><ol><li><p>安装 Tensorflow 包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow</div></pre></td></tr></table></figure></li><li><p>把 github 的 tensorflow/models 克隆下来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/tensorflow/models.git</div></pre></td></tr></table></figure></li><li><p>安装 google protobuf</p><p><strong>Protobuf</strong> 是 google 开发的一种数据结构。它提供了一种灵活、高效、自动序列化结构数据的机制，可以联想 XML，但是比 XML 更小、更快、更简单。</p><p>安装链接：<a href="https://github.com/google/protobuf/releases/tag/v3.3.0。我下载了" target="_blank" rel="external">https://github.com/google/protobuf/releases/tag/v3.3.0。我下载了</a> protoc-3.3.0-win32.zip，专供不想自己配置的懒逼下载，载完后直接将bin目录添加到环境变量PATH中即可。</p></li><li><p>编译 protobuf 库文件</p><p>在 <strong>tensorflow/models</strong> 文件夹下运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">protoc object_detection/protos/*.proto --python_out=.</div></pre></td></tr></table></figure></li><li><p>将目录添加到PYTHONPATH中</p><p>windows下没有export命令，根据文字描述，要把 <strong>slim</strong> 文件夹和 <strong>tensorflow/models/</strong> 和添加到 <strong>PATHONPATH</strong> 中。但是之前我的python目录是直接添加在path中的，于是先创建一个名为PAYTHONPATH的变量：</p><p>&lt;% asset_img pythonpath.png %&gt;</p><p>把关于python的统统添加到这个变量中，以及刚才所说的 slim 和 models：</p><p>&lt;% asset_img pythonpath.png %&gt;</p><p>再把 <strong>%PYTHONPATH%</strong> 添加到 <strong>PATH</strong> 中：</p><p>&lt;% asset_img path.png %&gt;</p></li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>​    运行<code>python object_detection/builders/model_builder_test.py</code></p><p>​    <img src="/2017/06/20/Tensorflow-test/2017/06/20/Tensorflow-test/ok.png" alt="ok.png" title=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;谷歌开源了物体识别系统的API，开源地址：&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/object_detection&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tensor Fl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex8</title>
    <link href="http://yoursite.com/2017/06/16/machine-learning-ex8/"/>
    <id>http://yoursite.com/2017/06/16/machine-learning-ex8/</id>
    <published>2017-06-16T11:46:41.000Z</published>
    <updated>2017-10-21T09:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。</p><p>这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。</p><a id="more"></a><h3 id="Anomaly-detection"><a href="#Anomaly-detection" class="headerlink" title="Anomaly detection"></a>Anomaly detection</h3><p>首先计算出 <strong>μ</strong> 和 <strong>σ^2^ </strong>：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mu = mean(X)';</div><div class="line">sigma2 = var(X)'*(m<span class="number">-1</span>)/m;</div></pre></td></tr></table></figure><p>在这里要注意函数 var() 除以的是m-1所以我们要修改一下函数。然后我们要利用交叉验证样本，计算 F~1~ Score 并挑选临界值 <strong>ε</strong>：</p> <figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 获取验证集异常值坐标</span></div><div class="line">cvPredictions = (pval &lt; epsilon);</div><div class="line">tp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line">fp = sum( (cvPredictions == <span class="number">1</span>) &amp; (yval == <span class="number">0</span>) );</div><div class="line">fn = sum( (cvPredictions == <span class="number">0</span>) &amp; (yval == <span class="number">1</span>) );</div><div class="line"><span class="comment">% 计算精确率</span></div><div class="line">prec = tp / (tp + fp);</div><div class="line"><span class="comment">% 计算召回率</span></div><div class="line">rec = tp / (tp + fn);</div><div class="line">F1 = <span class="number">2</span> * prec * rec / (prec + rec);</div></pre></td></tr></table></figure><p>然后可以看到红红的圈出的异常值：</p><img src="/2017/06/16/machine-learning-ex8/2017/06/16/machine-learning-ex8/detected.png" alt="detected.png" title=""><h3 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h3><p>说到推荐系统，以电影为例，一方面要预测用户对于某电影的评分，另一方面要寻找相似的电影，我们经常使用的算法是协同过滤算法。进一步了解这个算法，查了一些中文资料。原本的线性回归，我们只需要根据特征值计算出参数 <strong>θ</strong>，但是现在变态了，我们不光要预测用户的喜好，还要查找相似的特征向量，俩参数（都用矩阵表示）一起学习。</p><p>首先我们完成未正规化的梯度和代价函数的计算：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_grad = (X * Theta' - Y) .* R * Theta;</div><div class="line">Theta_grad = (X * Theta' - Y)' .* R' * X;</div><div class="line"></div><div class="line">J = sum(( (X * Theta' - Y).^<span class="number">2</span> .* R )(:)) / <span class="number">2</span>;</div></pre></td></tr></table></figure><p>期间完全忘记梯度是什么鬼。。回顾一下，是代价函数对变量求偏导~ 计算公式完全按照矩阵的大小来判断。接下来我们正规化代价函数，按照公式加上<code>J += lambda / 2 _ sum(Theta.^2(:)) + lambda / 2 _ sum(X.^2(:));</code>，但发现J变成了1x3的向量，发现问题在于sum中应该在平方的时候添加括号，修改代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">J += lambda / <span class="number">2</span> * sum((Theta.^<span class="number">2</span>)(:)) + lambda / <span class="number">2</span> * sum((X.^<span class="number">2</span>)(:));</div></pre></td></tr></table></figure><p>然后继续完成梯度的正规化。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以光速刷课。。昨天一天刷完week9，今天又刷完week10，正在向week11进军。。还是要完成一下wee9的编程作业。&lt;/p&gt;
&lt;p&gt;这一章主要是讲了异常检测和推荐系统。异常检测算法使用的是以前概率论学过的正态（高斯）分布。只使用特征值X来计算出概率分布，根据临界值的大小再判断y是否异常。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Mmachine Learning ex7</title>
    <link href="http://yoursite.com/2017/06/13/machine-learning-ex7/"/>
    <id>http://yoursite.com/2017/06/13/machine-learning-ex7/</id>
    <published>2017-06-13T01:46:37.000Z</published>
    <updated>2017-06-15T04:38:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章主要学习了 <strong>K-均值算法</strong> 和 <strong>PCA 算法</strong>，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。</p><a id="more"></a><h3 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h3><p>我们先使用2维的数据集来感受一下K均值算法。接着我们要将写好的函数运用到图像压缩上。K-均值算法最核心的步骤如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Initialize centroids</span></div><div class="line">centroids = kMeansInitCentroids(X, K);</div><div class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:iterations</div><div class="line"><span class="comment">% Cluster assignment step: Assign each data point to the</span></div><div class="line"><span class="comment">% closest centroid. idx(i) corresponds to cˆ(i), the index</span></div><div class="line"><span class="comment">% of the centroid assigned to example i</span></div><div class="line">idx = findClosestCentroids(X, centroids);</div><div class="line"><span class="comment">% Move centroid step: Compute means based on centroid</span></div><div class="line"><span class="comment">% assignments</span></div><div class="line">centroids = computeMeans(X, idx, K);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>第一步完成 <strong>findClosesCentroids</strong> 函数，计算每个样本到中心点的距离，用数值表示其所属类，返回聚类后的向量：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(X,<span class="number">1</span>)</div><div class="line">  min = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="number">1</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">  min_idx = <span class="number">1</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">2</span> : K</div><div class="line">    cur = norm(X(<span class="built_in">i</span>,:) - centroids(<span class="built_in">j</span>,:), <span class="number">2</span>).^<span class="number">2</span>;</div><div class="line">    <span class="keyword">if</span>(cur &lt; min)</div><div class="line">      min = cur;</div><div class="line">      min_idx = <span class="built_in">j</span>;</div><div class="line">    <span class="keyword">end</span></div><div class="line">  <span class="keyword">end</span></div><div class="line">  idx(<span class="built_in">i</span>) = min_idx;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>题解里说用一个for循环完成，但我先使用了两个，后面再进行优化好了。这其中出现了一个小问题，计算norm的时候，<code>norm(X(i,:) - centroids(j,:), 2).^2;</code> 主要包含所有列，否则只包含了单个数字。</p><p>接下来完成 <strong>computeMeans</strong> 函数，通过同个类里的样本计算新的中心：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : K</div><div class="line">  <span class="comment">% 查找聚类样本</span></div><div class="line">  examples_idx = <span class="built_in">find</span>(idx == <span class="built_in">i</span>);</div><div class="line">  <span class="comment">% 计算中心值</span></div><div class="line">  X(examples_idx, :);</div><div class="line">  centroids(<span class="built_in">i</span>,:) = sum( X(examples_idx, :) ) / <span class="built_in">size</span>(examples_idx, <span class="number">1</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>这里出现的问题在<code>centroids(i,:) = sum( X(examples_idx, :) ) / size(examples_idx, 1);</code> 。注意包含centroids的所有列，和在计算szie()的时候，选择所有行的大小。</p><p>然后我们就可以运行我们的K-均值算法来聚类了。可以看到初始化的时候三个中心点分别为（3,3）、（6,2）、（8,5）。</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter1.png" alt="iter1.png" title=""><p>经过6次迭代可以看到中心点逐渐往好的方向移动：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter6.png" alt="iter6.png" title=""><p>经过10次迭代基本到达有模有样的聚类中心了：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/iter10.png" alt="iter10.png" title=""><p>接下来我们要使用K均值算法来压缩图像了。每个像素用 <strong>24-bit</strong> 来表示颜色，即3个 <strong>8-bit</strong> 的无符号整形来表示RGB的值，目标压缩成用<strong>16-bit</strong> 来表示。</p><p>我们使用 <strong>imread(A)</strong> 来读入图像，因为我们原始图像的大小为128x128，因此 A 为一个三维矩阵，<strong>size(A) = 128x128x3</strong> ，然后reshape图像变为 <strong>mX3</strong>（m = 16384 = 128x128） 的矩阵，用于后面执行K-均值算法。</p><p>我们将K设置为16，对每个像素点进行聚类。获得最后的图像。原始图像需要 <strong>128 x 128 x 24 = 393,216</strong> bits。而压缩后我们使用 24bits存储16种颜色，但每个像素点只需要4bits 来进行定位。因此压缩后的图像为 <strong>16 x 24+128x128x4 = 65,920</strong> bits。（从pdf里理解是这样，然而真实的情况似乎并没有进行压缩，看了助教在论坛里的回答，完整的压缩过程还要创建一个新的图像，只使用4bit来表示，因为本课程只是关于聚类，而不是关于压缩图像的细节。）因此压缩后的图片如下：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress.png" alt="compress.png" title=""><p>然后我们换一张自己的图片来看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress2.png" alt="compress2.png" title=""><p>然后修改 <strong>K=5</strong> 再看看效果：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/compress3.png" alt="compress3.png" title=""><h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a>Principal Component Analysis</h3><p>我们使用PCA来减少数据集的维度。先实验2D的数据集来了解一下PCA的运作，再将其运用更高的维度上。</p><p>PCA的步骤为：1、正规化数据集。2、计算协方差矩阵。3、利用SVD函数计算主成分（U、S）。</p><p>2、3步骤实现</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sigma = X' * X / m;</div><div class="line">[U, S, V] = svd(Sigma);</div></pre></td></tr></table></figure><p>可以看到特征向量如下图：</p><p><strong>特征向量1</strong></p><p><strong>特征向量1 &amp; 2</strong></p><p>因为我们要降低到一维，即 <strong>K=1</strong> 因此在这次只会使用到U(:, 1)</p><p>通过SVD函数计算出了主成分后，我们可以就利用特征向量U来将每个样本映射为更低的维度，x^(i)^ -&gt; z^(i)^ 。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">U_reduce = U(:, <span class="number">1</span>:K);</div><div class="line">Z = X * U_reduce;</div></pre></td></tr></table></figure><p>重构出通过PCA映射后的点（红色）：</p><img src="/2017/06/13/machine-learning-ex7/2017/06/13/machine-learning-ex7/reconstruction.png" alt="reconstruction.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章主要学习了 &lt;strong&gt;K-均值算法&lt;/strong&gt; 和 &lt;strong&gt;PCA 算法&lt;/strong&gt;，前者用于无监督学习中聚类的训练，后者用于压缩输入特征值的维度。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex6</title>
    <link href="http://yoursite.com/2017/06/10/machine-learning-ex6/"/>
    <id>http://yoursite.com/2017/06/10/machine-learning-ex6/</id>
    <published>2017-06-10T01:58:07.000Z</published>
    <updated>2017-10-21T09:38:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>终于来到支持向量机（SVM）了！这一章我们要学习如何使用高斯核SVM来建立一个垃圾邮件分类器。</p><a id="more"></a><h3 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h3><p>首先显示出我们的数据，很明显地可以看出 <strong>C=1</strong> 时两个分类的决策边界是中间的一条线，但是要注意到左边有一个 <strong>+</strong> 在（0.1,4.1）。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c1.png" alt="dataset1_c1.png" title=""><p>增大C，修改 <strong>C=100</strong> 可以看到决策边界的偏差减小了。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c100.png" alt="dataset1_c100.png" title=""><p>继续增大C，此时 <strong>C=1000</strong> ，</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset1_c1000.png" alt="dataset1_c1000.png" title=""><p>根据公式即可完成高斯核的代码，没有出现什么大问题。接下来出现了一个样本较大且是非线性的决策边界的数据集，我们利用已完成的高斯核SVM来显示决策边界。</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset2.png" alt="dataset2.png" title=""><p>通过训练集3我们要通过验证集来找到最佳的 C 和 σ，最初的决策分界如下图：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset3_base.png" alt="dataset3_base.png" title=""><p>经过8864的循环终于得到了最优参数 :-)</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">min = <span class="number">1</span>;</div><div class="line">step = [<span class="number">0.01</span>; <span class="number">0.03</span>; <span class="number">0.1</span>; <span class="number">0.3</span>; <span class="number">1</span>; <span class="number">3</span>; <span class="number">10</span>; <span class="number">30</span>];</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: <span class="built_in">size</span>(step)</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>: <span class="built_in">size</span>(step)</div><div class="line">    model= svmTrain(X, y, step(<span class="built_in">i</span>), @(x1, x2) gaussianKernel(x1, x2, step(<span class="built_in">j</span>)));</div><div class="line">    predictions = svmPredict(model, Xval);</div><div class="line">    cur = mean(double(predictions ~= yval));</div><div class="line">    <span class="keyword">if</span>(cur &lt; min) </div><div class="line">      min = cur;</div><div class="line">      C = step(<span class="built_in">i</span>);</div><div class="line">      sigma = step(<span class="built_in">j</span>);</div><div class="line">    <span class="keyword">end</span> </div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/dataset3_over.png" alt="dataset3_over.png" title=""><h3 id="Spam-Classification"><a href="#Spam-Classification" class="headerlink" title="Spam Classification"></a>Spam Classification</h3><p>在处理垃圾邮件的过程中，需要对邮件进行标准化。包括全部转换为小写字母、移除HTML标签、以及一些表示具体含义的内容（如链接、邮件、符号、数字等）替换为固定的字符串。</p><p>接下来我们的任务是将邮件中出现的单词映射为词典中的索引：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">length</span>(vocabList)</div><div class="line">  <span class="keyword">if</span>(strcmp(str, vocabList&#123;i&#125;))</div><div class="line">      word_indices = [word_indices; i];</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>后来在查看练习资料的时候发现，可以使用<code>ismember()</code>函数来实现同样功能：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[tf, idx] = ismember(str, vocabList);</div><div class="line"><span class="keyword">if</span>(idx)</div><div class="line">  word_indices = [word_indices; idx];</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>然后将邮件转化成一个向量x，如果单词出现过则设特征值 x~i~ 为1，否则为0，i 为词典中的索引。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(word_indices)</div><div class="line">  x(word_indices(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>接下来我们就是利用上面完成的两个函数，通过SVM来进行邮件垃圾分类了。测试一下自己收到过的垃圾邮件：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/evernote.png" alt="evernote.png" title=""><p>果然是垃圾邮件呢：</p><img src="/2017/06/10/machine-learning-ex6/2017/06/10/machine-learning-ex6/result.png" alt="result.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;终于来到支持向量机（SVM）了！这一章我们要学习如何使用高斯核SVM来建立一个垃圾邮件分类器。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex5</title>
    <link href="http://yoursite.com/2017/06/05/machine-learning-ex5/"/>
    <id>http://yoursite.com/2017/06/05/machine-learning-ex5/</id>
    <published>2017-06-05T02:33:50.000Z</published>
    <updated>2017-06-06T15:32:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次练习是利用正则化的线性回归来学习不同的偏差-方差（bias-variance）模型，并且学会判断其误差类型，能够对算法进行改进。</p><p>根据公式完成线性回归的代价函数和梯度的函数。接下来会根据我们算出来的theta值显示一个模型，如下图</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/line.png" alt="line.png" title=""><p>可以看到模型不太适合我们的训练值，具有高偏差（high-bias），并且可以看出欠拟合（underfitting）。因此接下来我们要通过显示训练和测试的误差学习曲线来诊断偏差-方差问题。</p><a id="more"></a><h3 id="Learning-curves"><a href="#Learning-curves" class="headerlink" title="Learning curves"></a>Learning curves</h3><p>我们可以利用之前写好的函数来计算训练集的θ，并通过这个θ来计算训练集误差和交叉验证误差，显示学习曲线。需要注意的是训练集误差计算时要包括不同的训练集大小，并且两者都不需要正则化。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[theta_train] = trainLinearReg(X, y, lambda);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>运行完图像如下：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross1.png" alt="cross1.png" title=""><p>和pdf中的示例还差挺多的，我应该是哪里理解错了。修改for循环中交叉验证误差<code>error_val(i) = linearRegCostFunction(Xval(1:i, :), yval(1:i), theta_train, lambda);</code>，图像如下</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross2.png" alt="cross2.png" title=""><p>结果还是不正确，交叉验证误差的error值不应该这么小。而且pdf里面也说了</p><blockquote><p>When you are computing the training set error, make sure you compute it on the training subset (i.e., X(1:n,:) and y(1:n))(instead of the entire training set). However, for the cross validation error,you should compute it over the entire cross validation set.  </p></blockquote><p>也就是说交叉验证误差要用到全部的交叉验证训练集，而不是X(1:n,:) and y(1:n)。看了一下论坛，应该不同大小的训练集计算不同的θ，因此才有正确的结果：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% use tranLinearReg function to find theta</span></div><div class="line">  [theta_train] = trainLinearReg(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), lambda);</div><div class="line">  [error_train(i)] = linearRegCostFunction(X(<span class="number">1</span>:<span class="built_in">i</span>,:), y(<span class="number">1</span>:<span class="built_in">i</span>), theta_train, lambda);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta_train, lambda);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/cross3.png" alt="cross3.png" title=""><p>但是submit以后发现并没有获得分数，上论坛找找答案：</p><div class="tip"><br><br>我们不需要在学习曲线的函数中自己添加<code>lambda = 0</code>，λ用于正则化θ，当我们在计算 Jtrain, Jcv以及 Jtest的时候，我们需要的是真正的误差，而不需要额外的惩罚项。<br><br></div><p>因此修改4句为<code>  [error_train(i)] = linearRegCostFunction(X(1:i,:), y(1:i), theta_train, 0);</code>，第5句同理。</p><h3 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h3><p>本来以为这个函数很简单，简单的用一个循环次方计算出每一列就可以了：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : p</div><div class="line">  X_poly(:, <span class="built_in">i</span>) = X .^ p</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>但是结果却不尽如人意，把计算的结果显示出来，发现每一行的值都是一样的，并且看上去与X好像毫无关系</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly.png" alt="poly.png" title=""><p>发现问题了！mdzz…无视了循环，就一句代码也会错。。应该改成<code>X_poly(:, i) = X .^ i</code>就对了！</p><p>在利用多项式回归函数来计算theta值之前，我们必须对训练集进行特征缩放，不然后面的数据会变得炒鸡大。</p><p>在 λ= 0的 情况下，我们发现多项式回归函数可以很好的拟合我们的训练集（下图1），并且可以看到多项式回归的学习曲线（下图2）</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot.png" alt="poly_plot.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning.png" alt="poly_learning.png" title=""><p>接下来我们通过修改不同的λ值来看一下正则化对偏差-方差的影响。</p><p>λ = 1 时：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda1.png" alt="poly_plot_lambda1.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda1.png" alt="poly_learning_lambda1.png" title=""><p>λ = 100 时：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_plot_lambda100.png" alt="poly_plot_lambda100.png" title=""><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/poly_learning_lambda100.png" alt="poly_learning_lambda100.png" title=""><p>通过上面两个例子可以看出，λ = 1 的时候最好的适应了训练集，并且训练集误差和验证误差都挺小的，比 λ = 0 的时候来得优秀，而λ= 100 的时候就严重失控了，不但欠拟合，而且误差也高的要死。</p><h3 id="Selecting-λ-using-a-cross-validation-set"><a href="#Selecting-λ-using-a-cross-validation-set" class="headerlink" title="Selecting λ using a cross validation set"></a>Selecting λ using a cross validation set</h3><p>前面我们通过训练集大小的变化来训练θ，并显示学习曲线，现在我们要通过改变λ来显示学习曲线。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">length</span>(lambda_vec)</div><div class="line">  lambda = lambda_vec(<span class="built_in">i</span>);</div><div class="line">  [theta] = trainLinearReg(X, y, lambda);</div><div class="line">  error_train(<span class="built_in">i</span>) = linearRegCostFunction(X, y, theta, <span class="number">0</span>);</div><div class="line">  error_val(<span class="built_in">i</span>) = linearRegCostFunction(Xval, yval, theta, <span class="number">0</span>);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>在使用每个函数的时候应该注意参数的带入，在使用_linearRegCostFunction_漏了θ参数，活生生的报错了。显示的学习曲线如下：</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/lambda_learning.png" alt="lambda_learning.png" title=""><p>可以看出随着λ的逐渐增加，交叉验证误差先减小后增大，有一个极值点，这个极值点便是我们应该选择的λ。</p><h3 id="Computing-test-set-error"><a href="#Computing-test-set-error" class="headerlink" title="Computing test set error"></a>Computing test set error</h3><p>测试集误差的计算对于评估最终模型来说很重要，我们使用前面看出来的最优λ = 3来计算测试集误差。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">best_lambda = <span class="number">3</span>;</div><div class="line">[best_theta] = trainLinearReg(X, y, best_lambda);</div><div class="line">error_test = linearRegCostFunction(Xtest, ytest, best_theta, <span class="number">0</span>)</div></pre></td></tr></table></figure><p>在这里要注意主函数里已经对Xtest进行了多项式化，在传参的时候要传<strong>X_poly_test</strong>。</p><h3 id="Plotting-learning-curves-with-randomly-selected-examples"><a href="#Plotting-learning-curves-with-randomly-selected-examples" class="headerlink" title="Plotting learning curves with randomly selected examples"></a>Plotting learning curves with randomly selected examples</h3><p>随机选择样本来计算θ并显示学习曲线。我使用了双重循环，外面一重i定义为样本的数量error(i)返回样本为i时的误差，内层j为误差迭代次数，每次计算便存入一个error(j)中，循环完成后用这个向量来计算平均值。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[aver_error_train, aver_error_val]</span> = ...</span></div><div class="line">   randlyExamplesCurve(X, y, Xval, yval, lambda)</div><div class="line"></div><div class="line">mx = <span class="built_in">size</span>(X, <span class="number">1</span>);</div><div class="line">mxal = <span class="built_in">size</span>(Xval, <span class="number">1</span>);</div><div class="line">   </div><div class="line">aver_error_train = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line">aver_error_val = <span class="built_in">zeros</span>(mx, <span class="number">1</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  <span class="comment">% 随机选择i个样本</span></div><div class="line">  rndIDX1= randperm(mx);</div><div class="line">  rndIDX2 = randperm(mxal);</div><div class="line">  rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">  rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">  <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">  theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line"></div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X, rand_y, theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval, rand_yval, theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>最后显示出来的学习曲线。。。五花八门。。。</p><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly1.png" alt="randly1.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly2.png" alt="randly2.png" title=""><p>再仔细看一下论坛，计算误差值时theta如果不改变是没有意义的，并且无需使用全部的样本，i个就使用前i个样本，因此修改代码</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : mx</div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差值</span></div><div class="line">  iter = <span class="number">50</span>;</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : iter</div><div class="line">    <span class="comment">% 随机选择i个样本</span></div><div class="line">    rndIDX1= randperm(mx);</div><div class="line">    rndIDX2 = randperm(mxal);</div><div class="line">    rand_X = X(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_y = y(rndIDX1(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_Xval = Xval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line">    rand_yval = yval(rndIDX2(<span class="number">1</span>:<span class="built_in">i</span>), :);</div><div class="line"></div><div class="line">    <span class="comment">% 使用随机生成的样本计算theta</span></div><div class="line">    theta = trainLinearReg(rand_X, rand_y, lambda);</div><div class="line">    error_train(<span class="built_in">j</span>) = linearRegCostFunction(rand_X(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_y(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">    error_val(<span class="built_in">j</span>) = linearRegCostFunction(rand_Xval(<span class="number">1</span>:<span class="built_in">i</span>, :), rand_yval(<span class="number">1</span>:<span class="built_in">i</span>, :), theta, <span class="number">0</span>);</div><div class="line">  <span class="keyword">end</span></div><div class="line">  </div><div class="line">  <span class="comment">% 计算误差平均值</span></div><div class="line">  aver_error_train(<span class="built_in">i</span>) = mean(error_train);</div><div class="line">  aver_error_val(<span class="built_in">i</span>) = mean(error_val);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly3.png" alt="randly3.png" title=""> <img src="/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/randly4.png" alt="randly4.png" title=""><p>要运行老长的时间了，试了两次，这下好像比较像样咯~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次练习是利用正则化的线性回归来学习不同的偏差-方差（bias-variance）模型，并且学会判断其误差类型，能够对算法进行改进。&lt;/p&gt;
&lt;p&gt;根据公式完成线性回归的代价函数和梯度的函数。接下来会根据我们算出来的theta值显示一个模型，如下图&lt;/p&gt;
&lt;img src=&quot;/2017/06/05/machine-learning-ex5/2017/06/05/machine-learning-ex5/line.png&quot; alt=&quot;line.png&quot; title=&quot;&quot;&gt;
&lt;p&gt;可以看到模型不太适合我们的训练值，具有高偏差（high-bias），并且可以看出欠拟合（underfitting）。因此接下来我们要通过显示训练和测试的误差学习曲线来诊断偏差-方差问题。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex4</title>
    <link href="http://yoursite.com/2017/05/31/machine-learning-ex4/"/>
    <id>http://yoursite.com/2017/05/31/machine-learning-ex4/</id>
    <published>2017-05-31T03:06:22.000Z</published>
    <updated>2017-06-01T15:06:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章主要学习了神经网络中的反向传播算法（Backpropagation algorithm ）。相对于线性回归（Linear Regression）、逻辑回归（Logistic Regression）BP确实很复杂了。但是今天刚看的《Don’t make me think》里面作者所言</p><blockquote><p><strong>我们不是追根究底，而是勉强应付。 因为这对我们来说并不重要。</strong>对于我们中大多数人来说，只要我们能够正常使用，是否明白事物背后的运行机制并没有关系。这并不是智力低下的表现，而是我们并不关心。总之，它对我们来说没那么重要。</p></blockquote><p>那么现阶段，对于BP算法有一个整体的概念，懂得如何使用它便是关键。上一章我们用已经给定的theta矩阵来进行预测，这一章我们将通过BP算法来学习参数。这一章的练习主要分为两个部分，第一部分：我们通过给定的神经网络权重，前馈计算正则化的代价函数。第二部分：我们自己训练神经网络的权重。</p><a id="more"></a><p>此次用于训练的神经网络如下图：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/model.png" alt="model.png" title=""><h3 id="Feedforward-and-cost-function"><a href="#Feedforward-and-cost-function" class="headerlink" title="Feedforward and cost function"></a>Feedforward and cost function</h3><p>首先我们需要通过给定的Theta1、Theta2，用以下公式来计算代价函数（cost function）。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/cost.png" alt="cost.png" title=""><ul><li><p>第一次尝试</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute the output value</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = X * Theta1';</div><div class="line">a2 = [ones(m, <span class="number">1</span>), sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the labels</span></div><div class="line">recode_y = <span class="built_in">zeros</span>(m, num_labels);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="built_in">j</span> = y(<span class="built_in">i</span>);</div><div class="line">  recode_y(<span class="built_in">i</span>, <span class="built_in">j</span>) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="comment">% compute cost</span></div><div class="line">J  = sum(-y' * <span class="built_in">log</span>(a3) - (<span class="number">1</span> - y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3));</div></pre></td></tr></table></figure><p>算出来的J可有1523995.662855那么大。。。第一次尝试失败。。。</p></li><li><p>第二次尝试：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% compute the output value</span></div><div class="line">  z2 = X(<span class="built_in">i</span>, :) * Theta1';</div><div class="line">  a2 = [ones(<span class="number">1</span>,<span class="number">1</span>) sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the labels as vectors</span></div><div class="line">  recode_y = <span class="built_in">zeros</span>(num_labels, <span class="number">1</span>);</div><div class="line">  recode_y(y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line">  J += -recode_y' * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line"><span class="keyword">end</span></div><div class="line">J /= m;</div></pre></td></tr></table></figure><p>对公式进行分解：for循环是用来执行公式最外层的m的累加，通过矩阵的相乘来执行内层的k的累加。</p><p>不过我不死心认为第一次尝试的思路并没有错。。用大矩阵来完成两个相加。。我要再回去尝试一下。</p></li><li><p>第三次尝试</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute the output value</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = X * Theta1';</div><div class="line">a2 = [ones(m, <span class="number">1</span>), sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the labels</span></div><div class="line">recode_y = <span class="built_in">zeros</span>(m, num_labels);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  recode_y(<span class="built_in">i</span>, y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="comment">% compute cost</span></div><div class="line">J  = sum(sum(-recode_y' * <span class="built_in">log</span>(a3) - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)));</div><div class="line">J /= m;</div></pre></td></tr></table></figure><p>修改了一下第一次尝试中错误的变量（比如最后一步忘记代入recode_y等），但是发现结果还是不对。把矩阵画出来自己试着相乘了一下，发现这样不能实现内层的 y~k~ 和 a~k~ 的一一对应相乘。而是一个y~k~会和另外a~k~都相乘然后累加。事实证明，目前还是用第二次尝试的办法实现吧。</p><p>​</p></li></ul><h3 id="Regularized-cost-function"><a href="#Regularized-cost-function" class="headerlink" title="Regularized cost function"></a>Regularized cost function</h3><p>接着我们要正则化代价函数。这个公式看起来更可怕了：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/regularized.png" alt="regularized.png" title=""><p>这里注意到在累加的时候k是从1到400而不是401，那么就不能单纯的用 sum() 把所有数通通加一起了。因此尝试代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Theta1_t = Theta1(<span class="number">1</span>:hidden_layer_size, <span class="number">1</span>:input_layer_size);</div><div class="line">Theta2_t = Theta2(<span class="number">1</span>:num_labels, <span class="number">1</span>:hidden_layer_size);</div><div class="line"></div><div class="line">J += lambda * ((sum((Theta1_t.^<span class="number">2</span>)(:)) + sum((Theta2_t.^<span class="number">2</span>)(:)))) / (<span class="number">2</span> * m);</div></pre></td></tr></table></figure><p>当然。。不会第一次就那么顺利的。。啊。。。想不通哪里错啊。。。。再仔细看一下文档，</p><blockquote><p>For the matrices Theta1 and Theta2, this corresponds to the first column of each matrix. </p></blockquote><p>那么应该把<code>Theta1_t = Theta1(1:hidden_layer_size, 1:input_layer_size);</code>改为 <code>Theta1_t = Theta1(1:hidden_layer_size, 2:input_layer_size+1);</code>Theta2_2同理。</p><h3 id="Sigmoid-gradient"><a href="#Sigmoid-gradient" class="headerlink" title="Sigmoid gradient"></a>Sigmoid gradient</h3><p>我们从前往后算出来了最终输出的值，接下来要通过后向传播来计算delta了。那么首先我们需要根据下面这个方程来完成sigmoidGradient函数。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/sigmoid.png" alt="sigmoid.png" title=""><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">size</span>(z, <span class="number">1</span>)</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">size</span>(z, <span class="number">2</span>)</div><div class="line">    g(<span class="built_in">i</span>, <span class="built_in">j</span>) = sigmoid(z(<span class="built_in">i</span>,<span class="built_in">j</span>)) * (<span class="number">1</span> - sigmoid(z(<span class="built_in">i</span>,<span class="built_in">j</span>)));</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>按照4个步骤，在每个循环里计算从后向前的delta值。论坛里面说这是最具有挑战的一次练习，果然调了好久，我们从修改for循环开始：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% compute the output value</span></div><div class="line">  a1 = X(<span class="built_in">i</span>, :);</div><div class="line">  z2 = a1 * Theta1';</div><div class="line">  a2 = [<span class="number">1</span> sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the labels as vectors</span></div><div class="line">  recode_y = <span class="built_in">zeros</span>(num_labels, <span class="number">1</span>);</div><div class="line">  recode_y(y(<span class="built_in">i</span>)) = <span class="number">1</span>;</div><div class="line">  J += -recode_y' * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y)' * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line">  </div><div class="line">  <span class="comment">% step2</span></div><div class="line">  delta3 = a3' - recode_y;</div><div class="line">  </div><div class="line">  <span class="comment">% step3</span></div><div class="line"></div><div class="line">  delta2 = Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>)'*delta3.*sigmoidGradient(z2)';</div><div class="line">  </div><div class="line">  <span class="comment">% step4</span></div><div class="line">  Theta2_grad += delta3 * a2;</div><div class="line">  Theta1_grad += delta2 * a1;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">grad = [Theta1_grad(:); Theta2_grad(:)] / m;</div></pre></td></tr></table></figure><p>但是上面的答案和给出的还差挺多的。。。然后发现问题竟然出现在最后。。同时也修改了一下recode函数。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% Feedforward</span></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  <span class="comment">% step1.compute the output value</span></div><div class="line">  a1 = X(<span class="built_in">i</span>, :);</div><div class="line">  z2 = a1 * Theta1';</div><div class="line">  a2 = [<span class="number">1</span> sigmoid(z2)];</div><div class="line">  z3 = a2 * Theta2';</div><div class="line">  a3 = sigmoid(z3);</div><div class="line">  </div><div class="line">  <span class="comment">% recode the y</span></div><div class="line">  recode_y = [<span class="number">1</span>:num_labels] == y(<span class="built_in">i</span>);</div><div class="line">  <span class="comment">% compute cost function</span></div><div class="line">  J += -recode_y * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y) * <span class="built_in">log</span>(<span class="number">1</span> - a3)';</div><div class="line">  </div><div class="line">  <span class="comment">% step2.compute error term delta3</span></div><div class="line">  delta3 = a3 - recode_y;</div><div class="line">  <span class="comment">% size(delta3) = 1 x 10</span></div><div class="line">  </div><div class="line">  <span class="comment">% step3.compute error term delta2</span></div><div class="line">  delta2 = delta3 * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) .* sigmoidGradient(z2);</div><div class="line">  <span class="comment">% size(delta2) = 1 x 25</span></div><div class="line">  </div><div class="line">  <span class="comment">% step4.accumulate the gradient</span></div><div class="line">  Theta1_grad += delta2' * a1;</div><div class="line">  <span class="comment">% size(Theta1_grad) = 25 x 401</span></div><div class="line">  Theta2_grad += delta3' * a2;</div><div class="line">  <span class="comment">% size(Theta2_grad) = 10 x 26</span></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">J /= m; </div><div class="line"></div><div class="line"><span class="comment">% regularized the cost function</span></div><div class="line">Theta1_t = Theta1(<span class="number">1</span>:hidden_layer_size, <span class="number">2</span>:input_layer_size+<span class="number">1</span>);</div><div class="line">Theta2_t = Theta2(<span class="number">1</span>:num_labels, <span class="number">2</span>:hidden_layer_size+<span class="number">1</span>);</div><div class="line"></div><div class="line">J += lambda * ((sum((Theta1_t.^<span class="number">2</span>)(:)) + sum((Theta2_t.^<span class="number">2</span>)(:)))) / (<span class="number">2</span> * m);</div><div class="line"></div><div class="line">Theta1_grad /= m;</div><div class="line">Theta2_grad /= m;</div><div class="line">grad = [Theta1_grad(:); Theta2_grad(:)];</div></pre></td></tr></table></figure><p>完美！</p><p>并且通过查资料，get了trace（）函数，可以不用for循环完成函数了~~</p><p>修改Feedfoorward</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Feedforward</span></div><div class="line">a1 = [ones(m, <span class="number">1</span>) X];</div><div class="line">z2 = a1 * Theta1';</div><div class="line">a2 = [ones(m,<span class="number">1</span>) sigmoid(z2)];</div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line"><span class="comment">% recode the y</span></div><div class="line"><span class="comment">%recode_y = zeros(m, num_labels);</span></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</div><div class="line">  recode_y(<span class="built_in">i</span>, :) = [<span class="number">1</span>:num_labels] == y(<span class="built_in">i</span>);</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="comment">% compute cost function</span></div><div class="line">J = trace(-recode_y * <span class="built_in">log</span>(a3)' - (<span class="number">1</span> - recode_y) * <span class="built_in">log</span>(<span class="number">1</span> - a3)');</div><div class="line">J /= m;</div></pre></td></tr></table></figure><h3 id="Regularized-Neural-Networks"><a href="#Regularized-Neural-Networks" class="headerlink" title="Regularized Neural Networks"></a>Regularized Neural Networks</h3><p>接下来是正则化神经网络，根据公式则可以敲出代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% rregularized neural networks</span></div><div class="line">Theta1_grad(:, <span class="number">2</span>:<span class="keyword">end</span>) += lambda * Theta1(:, <span class="number">2</span>:<span class="keyword">end</span>) / m;</div><div class="line">Theta2_grad(:, <span class="number">2</span>:<span class="keyword">end</span>) += lambda * Theta2(:, <span class="number">2</span>:<span class="keyword">end</span>) / m;</div></pre></td></tr></table></figure><p>lambda = 1，Maxiter = 50时隐藏层为下图，准确率94.6%：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda1_iter50.png" alt="lambda1_iter50.png" title=""><p>接下来我们改变lambda的数值和迭代次数来看一下隐藏层的变化。首先修改lambda为0，迭代次数不变，准确率96.4%。</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda0_iter50.png" alt="lambda0_iter50.png" title=""><p>lambda为0，迭代次数为100，准确率可高了99.38%：</p><img src="/2017/05/31/machine-learning-ex4/2017/05/31/machine-learning-ex4/lambda0_iter100.png" alt="lambda0_iter100.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章主要学习了神经网络中的反向传播算法（Backpropagation algorithm ）。相对于线性回归（Linear Regression）、逻辑回归（Logistic Regression）BP确实很复杂了。但是今天刚看的《Don’t make me think》里面作者所言&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;我们不是追根究底，而是勉强应付。 因为这对我们来说并不重要。&lt;/strong&gt;对于我们中大多数人来说，只要我们能够正常使用，是否明白事物背后的运行机制并没有关系。这并不是智力低下的表现，而是我们并不关心。总之，它对我们来说没那么重要。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那么现阶段，对于BP算法有一个整体的概念，懂得如何使用它便是关键。上一章我们用已经给定的theta矩阵来进行预测，这一章我们将通过BP算法来学习参数。这一章的练习主要分为两个部分，第一部分：我们通过给定的神经网络权重，前馈计算正则化的代价函数。第二部分：我们自己训练神经网络的权重。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex3</title>
    <link href="http://yoursite.com/2017/05/29/machine-learning-ex3/"/>
    <id>http://yoursite.com/2017/05/29/machine-learning-ex3/</id>
    <published>2017-05-29T07:43:51.000Z</published>
    <updated>2017-05-30T02:20:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章练习也有两大部分，第一部分是正则化逻辑回归方程的多类分类，识别图像中的数字。觉得逻辑回归应该也是神经网络的一部分吧，只是没有hidden layer，直接就final了。第二部分需要解决的问题是一样的，识别图像中的文字，只通过是神经网络来达到这个目的。</p><a id="more"></a><h1 id="1-Multi-Class-Classification"><a href="#1-Multi-Class-Classification" class="headerlink" title="1/ Multi-Class Classification"></a>1/ Multi-Class Classification</h1><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>按照pdf的步骤，先写出不正则化的逻辑回归方程，打印了一下这时函数参数的X </p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/Xt.png" alt="Xt.png" title=""><p>发现这一节要求不使用循环，全部向量化，那我上一章就基本实现了耶，我真棒~ 稍微修改了一下方程，用一下hint的.*和sum，终于懂星号前面有一点是什么鬼了，是不使用矩阵乘法，而是对应的元素相乘。方程如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算unregulized cost function</span></div><div class="line">J = sum(-y .* <span class="built_in">log</span>(sigmoid(X*theta)) - (<span class="number">1</span>-y) .* <span class="built_in">log</span>(<span class="number">1</span>-sigmoid(X*theta))) / m;</div></pre></td></tr></table></figure><p>此时输出J的值为0.734819，但是参考值是2.534819，等正则化了看看结果。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算gradient</span></div><div class="line">grad = X' * (sigmoid(X * theta) - y);</div></pre></td></tr></table></figure><p> 接下来就要开始正则化cost function和gradient了。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 正则化cost function</span></div><div class="line">J += lambda * sum(theta(<span class="number">2</span>:<span class="keyword">end</span>).^<span class="number">2</span>) / (<span class="number">2</span> * m);</div></pre></td></tr></table></figure><p>上一章傻傻的用循环来避开对theta~0~的处理，现在get了用冒号来限制处理范围。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 正则化gradient</span></div><div class="line">temp = theta;</div><div class="line">temp(<span class="number">1</span>) = <span class="number">0</span>;</div><div class="line">grad += lambda * temp / m;</div></pre></td></tr></table></figure><p>提交发现咋不对啊。对照了一下ex2发现gradient忘记 / m了，我这粗心的小脑袋啊。。。</p><h3 id="oneVsAll"><a href="#oneVsAll" class="headerlink" title="oneVsAll"></a>oneVsAll</h3><p>看完毫无头绪要怎么训练啊。。我回顾一下one-vs-all是什么鬼。嗯。。再回来理解一下题目，把每个训练集分成K个类，返回一个Kx(N+1)的矩阵theta，矩阵的每一行是通过逻辑回归方程学习到的theta值。继续仔细看，没让你直接用一个函数就完成预测，这个函数只是用来生训练分类器tehta矩阵的，并且还提示使用一个给定函数fmincg来生成theta，只是循环K次，每次生成一个向量。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: num_labels</div><div class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="built_in">i</span> </div><div class="line">  c = <span class="number">10</span>;</div><div class="line">  <span class="keyword">else</span> </div><div class="line">  c = <span class="built_in">i</span>;</div><div class="line">  initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">  options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</div><div class="line">  [theta] = ...</div><div class="line">    fmincg(@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</div><div class="line">                initial_theta, options);</div><div class="line">   all_theta(c, :) = theta';</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>一开始对于c的概念有一些模糊，在经过测试以后发现 一个向量==一个数值 时，返回一个向量，其中与这个值相等的位置为1，其他位置均为0。再回顾一下上课的笔记，那么根据这个函数的意思，在num_labels个循环中y == c 就生成了num_labels个1位置不同的向量，这个向量作为判断结果，对于每一个结果通过fmincg函数来生成一个theta向量，但是这个向量是(n+1)x1的，因此要把theta转置成all_theta对应的那一行。</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/multiy.png" alt="multiy.png" title=""><p>修改代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> c = <span class="number">1</span>: num_labels</div><div class="line">  initial_theta = <span class="built_in">zeros</span>(n + <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">  options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</div><div class="line">  [theta] = ...</div><div class="line">    fmincg(@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</div><div class="line">                initial_theta, options);</div><div class="line">   all_theta(c, :) = theta';</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>理解了c的值以后，就知道0为什么要用10来表示啦。</p><p>接下来完成进行预测的函数。要对所有训练集进行训练，也就是h = X * theta’，然后与y进行比较，判断所属类型。不过我们已经知道了y就是一个向量，1在不同的位置为不同的类别。那么就是每一个训练集得到的 h 向量中数值最大的那一个就是我们的类别。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算hypothesis</span></div><div class="line">h = X * all_theta';</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(h(<span class="built_in">i</span>));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>但是结果又不对，思路应该是没错的，检查了一下size(theta)，是一个10x401的矩阵没错，h计算出来的结果肯定也没错。那么问题肯定就出现在查找最大值上。输出了一下h(1)发现只是一个数值，我们要查找的应该是一行中的最大值，把max(h(i))改成max(h(i, :))就对啦。</p><h1 id="2-Neural-Networks"><a href="#2-Neural-Networks" class="headerlink" title="2/ Neural Networks"></a>2/ Neural Networks</h1><p>上一部分我们通过多类分类的逻辑回归方程来识别手写的数字。但是由于逻辑回归仅仅是一个线性分类器，对于更复杂的行为很难处理，因此在这个部分我们实践一下神经网络。这个神经网络有3层，输入层，隐藏层，输出层。我们使用已经训练完成的权重来进行预测。</p><h3 id="Feedforward-Propagation-and-Prediction"><a href="#Feedforward-Propagation-and-Prediction" class="headerlink" title="Feedforward Propagation and Prediction"></a>Feedforward Propagation and Prediction</h3><p>这个预测函数通过给定的Theta1、Theta2来对输入层进行训练，判断结果和one-vs-all通过最大值来分类是一样的。</p><p>我们先来看下训练集X、Theta1、Theta2的大小</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/size.png" alt="size.png" title=""><p>如上图所示，也就是说训练集有5000个，400个项（不包含x~0~ = 1），因此加上x~0~以后，Theta1把401个项训练成25个，再加上x~0~ = 1，最后训练出10个分类结果。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% add x0 = 1</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% compute hidden layer</span></div><div class="line">layer2 = [ones(m, <span class="number">1</span>), X * Theta1<span class="string">'];</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">% compute output layer</span></div><div class="line"><span class="string">layer3 = layer2 * Theta2'</span>;</div><div class="line"></div><div class="line">% predict</div><div class="line">for i = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(layer3(<span class="built_in">i</span>, :));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>自信满满写完submit发现还是错的 orz…准确率不对，回去看一下训练模型，好像每一层训练的时候都要用到sigmodi函数？</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% add x0 = 1</span></div><div class="line">X = [ones(m, <span class="number">1</span>) X];</div><div class="line"></div><div class="line"><span class="comment">% compute hidden layer</span></div><div class="line">z2 = [ones(m, <span class="number">1</span>) X * Theta1<span class="string">'];</span></div><div class="line"><span class="string">a2 = sigmoid(z2);</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">% compute output layer</span></div><div class="line"><span class="string">z3 = a2 * Theta2'</span>;</div><div class="line">a3 = sigmoid(z3);</div><div class="line"></div><div class="line">% predict</div><div class="line">for i = <span class="number">1</span> : m</div><div class="line">  [maxn, maxi] = max(z3(<span class="built_in">i</span>, :));</div><div class="line">  p(<span class="built_in">i</span>) = maxi;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>啊。。。明明准确率对，结果也对为什么submit以后还是没有分数呢。。。再回去认真看模型。。</p><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/model.png" alt="model.png" title=""><p>好像隐藏层是先逻辑回归，再添加a~0~嗯。。。got it。。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% compute output layer</span></div><div class="line">z3 = a2 * Theta2';</div><div class="line">a3 = sigmoid(z3);</div></pre></td></tr></table></figure><img src="/2017/05/29/machine-learning-ex3/2017/05/29/machine-learning-ex3/nice.png" alt="nice.png" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章练习也有两大部分，第一部分是正则化逻辑回归方程的多类分类，识别图像中的数字。觉得逻辑回归应该也是神经网络的一部分吧，只是没有hidden layer，直接就final了。第二部分需要解决的问题是一样的，识别图像中的文字，只通过是神经网络来达到这个目的。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning ex2</title>
    <link href="http://yoursite.com/2017/05/28/machine-learning-ex2/"/>
    <id>http://yoursite.com/2017/05/28/machine-learning-ex2/</id>
    <published>2017-05-28T07:08:36.000Z</published>
    <updated>2017-05-30T02:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一章是实践逻辑回归。两个大题目，一个就是正常的逻辑回归，另外一个是正则化的逻辑回归。记录下做作业的心路历程。</p><a id="more"></a><h3 id="plotData"><a href="#plotData" class="headerlink" title="plotData"></a>plotData</h3><p>一开始就感觉被难住，X是mx2的矩阵，y是m行向量。对于如何来显示图像示毫无头绪，然后发现pdf里给出了答案。get了两个函数，一个是find，另一个是plot。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/find.png" alt="find.png" title=""><p>简单的看一下find根据查找条件返回查找矩阵的下标</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pos = <span class="built_in">find</span>(y == <span class="number">1</span>);</div><div class="line">neg = <span class="built_in">find</span>(y == <span class="number">0</span>);</div></pre></td></tr></table></figure><p>因此这两句结果分类返回对应的训练集所在行数。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/pos.png" alt="pos.png" title=""><p>看一下pos~就是返回一个向量，包括结果为1的行数。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/plot.png" alt="plot.png" title=""><p>接下来就是显示啦，第一个参数是x轴的数值，第二个参数是y坐标的数值，分别取自对应分类的第一项成绩和第二项成绩。然后显示结果。</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/plot_result.png" alt="plot_result.png" title=""><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>这个用下图这个公式变换一下函数</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/sigmoid.png" alt="sigmoid.png" title=""><p>一步一步来，先实现一个数字的转换：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g = <span class="number">1</span> / (<span class="number">1</span> + <span class="built_in">exp</span>(x));</div></pre></td></tr></table></figure><p>然后发现结果不正确，发现x还有可能是矩阵，也就是说矩阵的每一位都要做相应的变换</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:r</div><div class="line">  <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:c</div><div class="line">  g(<span class="built_in">i</span>, <span class="built_in">j</span>) = <span class="number">1</span> / (<span class="number">1</span> + <span class="built_in">exp</span>(-z(<span class="built_in">i</span>,<span class="built_in">j</span>)));</div><div class="line">  <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h3 id="costFunction"><a href="#costFunction" class="headerlink" title="costFunction"></a>costFunction</h3><p>首先公式很复杂，但是其实实现很简单</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/cost.png" alt="cost.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/grad.png" alt="grad.png" title=""><p>我一开始弄了两层循环来遍历i和m，后面意识到，我们用了矩阵啊，然后变成了一层只对m的循环，然后发现还是不对，其实就。。。非常简单</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% 计算hypothesis</span></div><div class="line">para =  X * theta;</div><div class="line"><span class="comment">% 计算cost</span></div><div class="line">J = (-y' * <span class="built_in">log</span>(sigmoid(para)) - (<span class="number">1</span> - y)' * (<span class="built_in">log</span>(<span class="number">1</span> - sigmoid(para)))) / m;</div><div class="line"><span class="comment">% 计算gradient</span></div><div class="line">grad = X' * (sigmoid(para) - y)/ m;</div></pre></td></tr></table></figure><h3 id="costFunctionReg"><a href="#costFunctionReg" class="headerlink" title="costFunctionReg"></a>costFunctionReg</h3><p>正则化逻辑回归的函数，这里要注意的是，计算cost和gradient的时候都不能把theta~0~ 算进去，复杂的公式用矩阵带入就好简单呀，虽然花了一个下午完成练习，主要都花在纠结矩阵谁乘以谁，但是完成了就成就感满满~~</p><p>修改一下lambda的值来测试一下整体化到底是什么鬼</p><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda0.png" alt="lamda0.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda1.png" alt="lamda1.png" title=""><img src="/2017/05/28/machine-learning-ex2/2017/05/28/machine-learning-ex2/lamda50.png" alt="lamda50.png" title=""><p>从图形的变化可以看出lamda太小会overfitting，1的时候切好是最正确的分界线，50就开始under fitting了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一章是实践逻辑回归。两个大题目，一个就是正常的逻辑回归，另外一个是正则化的逻辑回归。记录下做作业的心路历程。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>复习一下单片机吧</title>
    <link href="http://yoursite.com/2017/05/24/Microcontrollermd/"/>
    <id>http://yoursite.com/2017/05/24/Microcontrollermd/</id>
    <published>2017-05-23T16:00:00.000Z</published>
    <updated>2017-05-28T10:44:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解无线键盘监听装置的实验原理中，还是先理解一下一直没搞懂的单片机吧。顺便小小复习一下以前计算机组成原理的内容。</p><a id="more"></a><h1 id="单片机"><a href="#单片机" class="headerlink" title="单片机"></a>单片机</h1><p>全称单片微型计算机，又称微控制器，是把中央处理器、存储器、定时/计数器、各种输入输出接口等都集成在一块集成电路芯片上的微型计算机。与电脑中的通用型微处理器相比，它更强调自供应（不用外接硬件）和节约成本，最大优势是体积小，可放在仪表内部，但存储量小，输入输出接口简单，功能较低。</p><p>大多数使用冯诺依曼结构，即定义了嵌入式系统所必须的四个基本部分：1、中央处理器核心，程序存储器（只读存储器或闪存）。2、数据存储器（随机存储器）。3、一个或者多个定时/计算器。4、用来与外围设备以及扩展资源进行通信的输入/输出端口。</p><h3 id="与CPU的区别"><a href="#与CPU的区别" class="headerlink" title="与CPU的区别"></a>与CPU的区别</h3><p>CPU是一个运算处理器，主要负责计算、处理数据。单片机包括运算单元、存储单元、输入输出单元。单片机是一种微处理器，可以执行某些功能。但是CPU还要有主板、内存、硬盘等辅助。</p><h3 id="常用单片机"><a href="#常用单片机" class="headerlink" title="常用单片机"></a>常用单片机</h3><p>微芯（Microchip）公司的PIC系列出货量居于业界领导者地位；Atmel的51系列AVR系列种类众多，受支持面广；Arduino就是基于Atmel的AVR系列；德州仪器的MSP430系列以低功耗闻名，常用于医疗电子产品以及仪器仪表中；瑞萨单片机在日本使用广泛。还有比如Intel、飞思、卡尔半导体，NEC，NXP等等厂牌。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解无线键盘监听装置的实验原理中，还是先理解一下一直没搞懂的单片机吧。顺便小小复习一下以前计算机组成原理的内容。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>为什么草图对于设计如此重要？</title>
    <link href="http://yoursite.com/2017/05/14/Daily-Translation-04/"/>
    <id>http://yoursite.com/2017/05/14/Daily-Translation-04/</id>
    <published>2017-05-14T07:31:19.000Z</published>
    <updated>2017-05-30T03:08:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>在知乎上看到这篇文章是是同济2015年的英语面试题，就翻译来看看。 原文链接： <a href="https://crowdfavorite.com/why-is-sketching-such-an-important-aspect-of-design/" target="_blank" rel="external">Why is sketching such an important aspect of design?</a></p><p>在大学的时候，我的设计教授鼓励我手绘，但我不想听。为什么当我拥有一台含所有设计可视化软件以及比我所知更多的字体的电脑，我还要在幼稚的小本子上画画呢？</p><p>多年来我一直与这个观点作斗争。每当一个新的设计项目到来，我会拿出我的手绘本，寻找最棒的笔，然后开始手绘。结果通常是一部分时间在画草图，接着是耗费无数小时在电脑上完善一个想法。</p><blockquote><p>当我匆忙地拿起电脑开始完善，我失去了草图提供的机会。</p></blockquote><p>随着时间的推移，我慢慢开始越来越多的理解了这个观点。没有人强迫我。我意识到为了使我的想法更加具体，在电脑上浪费了多少时间。如今，手绘是我工作中很的重要组成部分，不仅是对客户，也是对我自己。更好的设计现在来得也更快。</p><a id="more"></a><h3 id="为什么草图很棒："><a href="#为什么草图很棒：" class="headerlink" title="为什么草图很棒："></a>为什么草图很棒：</h3><p><strong>你的第一个想法很难是最佳的</strong></p><p>你有一个项目想法感觉很棒。这是开始。画出来！它只需要几秒钟。现在手绘出更多的想法。你不会知道你最初的想法是否是最好的直到你探索出其他的想法。如果你发现你更喜欢的东西，你会立马感激你没有浪费5个小时在Illustrator中，只是像调整一个不好的logo的字母间距上这样的事上。</p><p><strong>手绘是快速的、粗糙的、脏乱的。</strong></p><p>学会拥抱你的纸和笔，你将会对你产生想法的速度感到十分惊喜。你会想知道为什么你从未想到过。通过给你自己自由去潦草地画出想法，你会到达你从未想象过的充满创意的地方。</p><p><strong>你会节省时间。</strong></p><p>在四十年代，我们让客户参与进设计的的每一步。我们不喜欢“大透露”，因为当过程保持神秘时，结果经常会让人失望。我们给客户展示我们许多的草图以展示我们的意图。当一个过程需要更改，我们会用几秒钟手绘出一个修订版，而不是花费好几个小时在photoshop中来移动图层。</p><p><strong>草图从细节中分离出观点。</strong></p><p>当你第一次呈现一个概念时，人们会很自然地关注错误的细节。如果你直接把你的想法用一个设计程序展示，人们倾向于关注单一的蓝色，或是呈现的第一种排版风格。当你只需弄懂“我们的方向是否正确？”时，会让沟通交流产生障碍。</p><p><strong>手绘是属于所有人的。</strong></p><p>我经常听到人们对于手绘的担忧，因为他们对自己手绘能力缺乏信心。不要担心！可以看看这本Dan Roam](<a href="http://www.thebackofthenapkin.com/" target="_blank" rel="external">http://www.thebackofthenapkin.com/</a>) <a href="http://www.thebackofthenapkin.com/" target="_blank" rel="external">“The Back of the Napkin” </a>写的书。基础的形状（圆形，正方形，矩形），线条，箭头，以及粘贴的图像足以展示你的想法。与团队或者客户一起手绘是很有益的，因为你可以在手绘的同时阐述你的观点，让你可以把握团队的努力重点。</p><p>灵感来源，可以看看SXSW 2010的 <a href="http://sketchnotearmy.com/2010/05/13/visual-note-taking-101-at-sxsw-slides-audio/" target="_blank" rel="external">Visual Note-Taking 101</a> 。只想看图片的话，请跳到27页，惊奇于阴影使图片更有趣。</p><p><strong>最后，手绘很有意思！</strong></p><p>当你停止焦虑，让你的想法发展时，你会发现手绘多么有趣。特别是当你找到完美的笔和纸组合！我的完美组合是 black Pilot Precise V7 配上 方眼Moleskine.</p><p>接下来是我给你的挑战：下一次你接到任何形式的沟通任务，不管是一个logo，一个品牌概念，甚至是一个导向你喜爱的咖啡店的地图，尝试手绘草图。让手绘融入你每天的任务，你会发现它也符合你的职业惯例。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在知乎上看到这篇文章是是同济2015年的英语面试题，就翻译来看看。 原文链接： &lt;a href=&quot;https://crowdfavorite.com/why-is-sketching-such-an-important-aspect-of-design/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Why is sketching such an important aspect of design?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在大学的时候，我的设计教授鼓励我手绘，但我不想听。为什么当我拥有一台含所有设计可视化软件以及比我所知更多的字体的电脑，我还要在幼稚的小本子上画画呢？&lt;/p&gt;
&lt;p&gt;多年来我一直与这个观点作斗争。每当一个新的设计项目到来，我会拿出我的手绘本，寻找最棒的笔，然后开始手绘。结果通常是一部分时间在画草图，接着是耗费无数小时在电脑上完善一个想法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当我匆忙地拿起电脑开始完善，我失去了草图提供的机会。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随着时间的推移，我慢慢开始越来越多的理解了这个观点。没有人强迫我。我意识到为了使我的想法更加具体，在电脑上浪费了多少时间。如今，手绘是我工作中很的重要组成部分，不仅是对客户，也是对我自己。更好的设计现在来得也更快。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>从心理学角度来看待用户体验设计</title>
    <link href="http://yoursite.com/2017/05/02/Daily-Translation-03/"/>
    <id>http://yoursite.com/2017/05/02/Daily-Translation-03/</id>
    <published>2017-05-02T04:41:30.000Z</published>
    <updated>2017-05-30T03:06:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>五一小长假的最后一天把portfolio赶粗！来！了！下午睡了一会儿晚上去了超市，缺席了一天的翻译，今天死也不能落下了。（对不起距离信誓旦旦说下这话落下两天了）。本来想翻译 <a href="https://medium.com/swlh/the-current-state-of-adaptive-design-6b2b89b258c4" target="_blank" rel="external">The Current State of Adaptive Design</a> 翻译了一半发现不对劲，是对比工具的使用，然而这些工具我都没有用过，我又没有mac:( 睡了一觉起来放弃这篇文章（压箱底），换成<a href="http://uxmag.com/articles/the-psychologists-view-of-ux-design" target="_blank" rel="external">The Psychologist’s View of UX Design</a>。</p><p>你可能听说过这关于大象的故事：</p><blockquote><p>一个国王把六个人带进一个黑暗的建筑。他们看不见任何东西。国王对他们说，”我从野生地带买下这只动物并带回东方。它被称为大象。”人们问道：”大象是什么？”国王说：”你们摸一下大象，描述给我听。”摸到腿的人说大象像一根柱子，摸到尾巴的说大象像一根绳子，摸到躯干的说像一根树枝，摸到耳朵的说像一把扇子，摸到肚子的说像一堵墙，摸到象牙的说像一根硬管道。”你们说的都是对的”，国王说道，”你们每个人都摸到了大象的一部分。”</p></blockquote><p>这个大象的故事使我想起了不同背景、教育、经历的人对于设计的不同看法。视觉设计师从一个角度来看待交互设计，交互设计师换了另一个角度，而程序员又是另外一个角度。了解甚至是体验别人正在体验的大象的部分是很有帮助的。</p><p>我是一个受过教育和训练的心理学家。因此我体验的那部分大象就是我们对人的了解以及我们如何将其应用于用户体验设计。我对大脑、视觉系统、记忆、动机做了研究和学习，并从中推出用户设计体验的原则。</p><p>这篇文章是从心理学家的视角对大象进行简单描述。</p><a id="more"></a><h3 id="1-除非必要，人们不想工作或思考"><a href="#1-除非必要，人们不想工作或思考" class="headerlink" title="1.除非必要，人们不想工作或思考"></a>1.除非必要，人们不想工作或思考</h3><ul><li>人们会做<strong>尽可能少的工作</strong>来完成任务</li><li>最好只展示一点点信息并让他们选择是否需要查看更多。<strong>渐近展开</strong>是不错的选择，最近我也写了一篇与其相关的博文。</li><li>不要光描述事物，给人们看例子。</li><li>当你在设计时，注意对象在屏幕、页面、设备上的承担特质。如果某个对象是可点击的，请确保它看起来像是可点击的。</li><li>只提供<strong>人们确实需要的功能</strong>。不要用你的想法来衡量他们需要什么；做用户研究去找到他们的真实需求。给人们超过他们的需要的只会搞砸用户体验。</li><li>提供默认值。<strong>默认值让人们做更少的事</strong>就能完成任务。</li></ul><h3 id="2-人有局限性"><a href="#2-人有局限性" class="headerlink" title="2.人有局限性"></a>2.人有局限性</h3><ul><li>人们只能在屏幕上看有限的信息或读有限的文本而<strong>不失去兴趣</strong>。在当下只提供需要的信息（看上面的渐近展开）。</li><li>让信息<strong>容易浏览</strong>。</li><li>信息或文本使用<strong>标题</strong>或者<strong>短块</strong>。</li><li>人们<strong>不能同时进行多项任务</strong>。研究非常明确的说明了这点，所以别指望人们能够做到。</li><li>人们更喜欢<strong>短小的文案</strong>，但他们<strong>长文本能帮助他们更好的理解</strong>。这是个难题，所以在你的项目中决定偏好还是性能更重要，但记住<strong>人们会要求那些实际上对他们不是最好的东西</strong>。</li></ul><h3 id="3-人们会犯错误"><a href="#3-人们会犯错误" class="headerlink" title="3.人们会犯错误"></a>3.人们会犯错误</h3><ul><li>假设人们犯错误。<strong>预测</strong>他们可能犯的错误并避免它们发生。</li><li>如果一个错误的结果是非常严重的，那么在用户操作前请<strong>提供确认</strong>。</li><li>让<strong>撤销</strong>非常容易</li><li>避免错误的产生永远比帮助人们纠正它来得好。<strong>最好的错误消息就是完全没有消息</strong>。</li><li>如果一个任务容易出错，<strong>将它拆分成小块</strong>。</li><li>如果用户犯了错并且你能<strong>纠正它</strong>，那么就这么做，并且展示你做了什么。</li><li>设计用户体验的人也会犯错，所以确保有时间和精力来进行迭代、用户反馈和测试。</li></ul><h3 id="4-人类记忆是复杂的"><a href="#4-人类记忆是复杂的" class="headerlink" title="4.人类记忆是复杂的"></a>4.人类记忆是复杂的</h3><ul><li>人们会重构记忆，这意味着记忆是一直在变化的。你只能相信用户所说的极少的一部分，<strong>观察他们的行为</strong>比听从他们所说比来得更好。</li><li>记忆也是很脆弱的。它退化得很快并容易导致很多错误。<strong>不要让人们记忆从一个任务到另外一个任务</strong>或者一个页面到另外一个页面的东西。</li><li>人们只能同时记住大概3-4个项目。<strong>7±2法则是一个都市传奇</strong>。研究表明真实的数字就是3-4个。</li></ul><h3 id="5-人们都是社会的"><a href="#5-人们都是社会的" class="headerlink" title="5.人们都是社会的"></a>5.人们都是社会的</h3><ul><li>人们总是尝试<strong>使用科技来社交</strong>。这是一个千年来的真理。</li><li>人们对于<strong>他们应该做什么会咨询他人的意见</strong>，特别在他们不确定的时候。这被叫做<strong>社会确认</strong>。举个例子，这就是为什么，打分和评论在各网站中这么强大。</li><li>如果人们在同一时间一起做事（同步行为），这会将他们联系在一起。同时大脑中也会产生化学反应。笑声同样使人们紧密联系。</li><li>如果你帮助了我那么我也会感激并反过来帮助你（互助）。研究表明，如果你希望人们填一个表格，<strong>先给他们想要的东西，再请求他们填写表格</strong>，而不是先填后给。</li><li>当你看到某些人做某些事，在你大脑里的某些部分会亮起，并认为你也正在做这件事（镜像神经元）。我们生性就是会模仿的动物。如果你想要人们做某些事那么让其他人做这件事。</li><li>你最多只能与150个人有强联系。强联系指与其有身体邻近的亲切的关系。但是<strong>弱联系可以成千上万</strong>并且是非常有影响力的。（就像Facebook）</li></ul><h3 id="6-注意力"><a href="#6-注意力" class="headerlink" title="6.注意力"></a>6.注意力</h3><ul><li>我开始认为<strong>注意力这个概念是设计有趣的用户界面的关键</strong>。接下来我会写更多的关于这方面文章。抓住并且保持专注，不转移正专注于某事的人的注意力，是非常关键的。</li><li>人们天生就会关注<strong>不同或者新奇</strong>的事物。如果你做出与众不同的事情，它将会脱颖而出。</li><li>话说回来，人们事实上会忽视他们视野之内的变化。这被称为<strong>变化失明</strong>。有一些很有趣的视频，当人们走在街上与向他们问路的人谈话，然后完全没有注意到，问路人已经变了。</li><li>你可以利用感官来抓住注意力。<strong>亮丽的颜色，较大的字体，喇叭声以及语调</strong>会吸引注意力。</li><li><strong>人们很容易分心</strong>。如果你不想他们被转移，不要在页面上闪烁或播放视频。反之如果你想吸引他们的注意力，可以这么做。</li></ul><h3 id="7-人们渴望信息"><a href="#7-人们渴望信息" class="headerlink" title="7.人们渴望信息"></a>7.人们渴望信息</h3><ul><li>多巴胺是一种让人们寻找食物、性爱、信息的化学物质。学习是具有多巴胺能的，我们无法控制地想获取更多信息。</li><li>人们经常<strong>想获取超出他们实际处理范围的信息</strong>。具备更多信息让人们感觉他们有更多的选择；有了更多的选择人们感觉一切尽在掌控之中，感觉掌握了一切使人们感觉生存的更好。</li><li>人们需要反馈。计算机不需要告诉人们它正在加载文件。<strong>人们需要知道接下来会发生什么</strong>。</li></ul><h3 id="8-无意识过程"><a href="#8-无意识过程" class="headerlink" title="8.无意识过程"></a>8.无意识过程</h3><ul><li>大多数思想过程是无意识发生的。</li><li>如果你能让人们做出很小的动作（注册成为免费会员），<strong>之后他们会做出更大的动作</strong>（升级成付费用户）。</li><li>​</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;五一小长假的最后一天把portfolio赶粗！来！了！下午睡了一会儿晚上去了超市，缺席了一天的翻译，今天死也不能落下了。（对不起距离信誓旦旦说下这话落下两天了）。本来想翻译 &lt;a href=&quot;https://medium.com/swlh/the-current-state-of-adaptive-design-6b2b89b258c4&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;The Current State of Adaptive Design&lt;/a&gt; 翻译了一半发现不对劲，是对比工具的使用，然而这些工具我都没有用过，我又没有mac:( 睡了一觉起来放弃这篇文章（压箱底），换成&lt;a href=&quot;http://uxmag.com/articles/the-psychologists-view-of-ux-design&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;The Psychologist’s View of UX Design&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;你可能听说过这关于大象的故事：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一个国王把六个人带进一个黑暗的建筑。他们看不见任何东西。国王对他们说，”我从野生地带买下这只动物并带回东方。它被称为大象。”人们问道：”大象是什么？”国王说：”你们摸一下大象，描述给我听。”摸到腿的人说大象像一根柱子，摸到尾巴的说大象像一根绳子，摸到躯干的说像一根树枝，摸到耳朵的说像一把扇子，摸到肚子的说像一堵墙，摸到象牙的说像一根硬管道。”你们说的都是对的”，国王说道，”你们每个人都摸到了大象的一部分。”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个大象的故事使我想起了不同背景、教育、经历的人对于设计的不同看法。视觉设计师从一个角度来看待交互设计，交互设计师换了另一个角度，而程序员又是另外一个角度。了解甚至是体验别人正在体验的大象的部分是很有帮助的。&lt;/p&gt;
&lt;p&gt;我是一个受过教育和训练的心理学家。因此我体验的那部分大象就是我们对人的了解以及我们如何将其应用于用户体验设计。我对大脑、视觉系统、记忆、动机做了研究和学习，并从中推出用户设计体验的原则。&lt;/p&gt;
&lt;p&gt;这篇文章是从心理学家的视角对大象进行简单描述。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>虚拟现实中的手势设计</title>
    <link href="http://yoursite.com/2017/04/30/Daily-Translation-02/"/>
    <id>http://yoursite.com/2017/04/30/Daily-Translation-02/</id>
    <published>2017-04-30T00:36:55.000Z</published>
    <updated>2017-05-30T03:06:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天的翻译来源于 <a href="http://facebook.design" target="_blank" rel="external">http://facebook.design</a> ，一直觉得未来的VR会超屌的，就进一步了解一下。先是来到 <a href="http://facebook.design/virtual-hands#scroll" target="_blank" rel="external">http://facebook.design/virtual-hands#scroll</a> 下载了Virtual Hands，各种手势的png麻溜的存起来，保不准以后有用呢。页面上有一个链接跳转到Christophe Tauziet制作VR手势交互的文章 <a href="https://medium.com/facebook-design/designing-for-hands-in-vr-61e6815add99" target="_blank" rel="external">Designing for Hands in VR</a> 这篇文章。那今天就翻译这篇吧~</p><a id="more"></a><blockquote><p>建立自然人机交互下一步</p></blockquote><p>年初我加入了Facebook的Social VR团队。我们团队的目标是探索虚拟现实如何帮助人们创造新的有趣的、有意义的方式来与你关心的人沟通。</p><p>在十月，Mark Zuckerberg在Oculus Connetct 主题演讲中对我们最近的一些探索进行了现场演示（然后有一段超屌的视频，看完简直惊叹）。</p><p>VR由于不同的头戴式识图器和输入控制器仍然是一个非常分散和多样的媒介。我们团队决定工作重点放在近期发布的<a href="https://www3.oculus.com/en-us/rift/" target="_blank" rel="external">Oculus Touch</a> 控制器。我们在早期认识到通过这些控制器将整个手的能力带入VR，为有意义的社交创造了独特的机会。</p><p>在接下来的几年，VR的发展很可能会超出我们的认知，但我们已经知道的设计仍然是一个很有价值的经验教训。以下是我们在VR中为手创造自然和舒适的互动的一些经验教训。按照我们每周发现的速度，其中的一些发现在未来几年可能毫不相干或者看起来很蠢，但我们希望它能够帮助任何人取得进一步成果。</p><h2 id="创造自然的互动"><a href="#创造自然的互动" class="headerlink" title="创造自然的互动"></a>创造自然的互动</h2><p>我有时会将VR的设计描述成掉进海洋里，不知道如何游泳。你真的不知道如何游，也不知道往哪个方向游。唯一避免溺死的事就是重用一些我们在产品设计中用来解决复杂问题时使用的解决技巧，寻找任何的机会发展每一个对这种媒介有用的新技能，并帮助彼此。在传统的产品设计背景下，大多数互动具有许多已建立和经过验证的模式。迄今为止我发现最有效的能取得进步的方式，就是在VR中学到的通过不断测试和犯错。</p><p>当我们在为Oculus Touch设计时，我们最早意识到任何人刚接触VR时，不得不在他们的思维模式中做出两个转变。一个是沉浸式方面：环顾四周，有一种存在感，并将虚拟空间和物理空间区分开来。另一个是这种新型的输入控制器如何立即给你虚拟的手并且让你与周围的环境互动。</p><img src="/2017/04/30/Daily-Translation-02/2017/04/30/Daily-Translation-02/1-A9-pLXWmEOBdF8I29NUK5w.gif" alt="1-A9-pLXWmEOBdF8I29NUK5w.gif" title=""><p>在设计控制时，通过做一个特定的动作来查看最自然和直观的方式是非常重要的。这么做使你产生容易理解、令人愉悦的体验，不管是伸手、拾物、与2D面板交互、远距离物体等等。</p><p>通过无数的试验和可用性研究，我们发现依赖控制器按钮有一些缺点。没有玩过过电子游戏的人看不见控制器时查找按钮有点麻烦。使用无形的按钮会提醒你虚拟的手不是真实的，潜意识里打破这种体验。这个问题在社交环境下更加明显，因为人们在VR中与他人对话时试图执行动作。</p><img src="/2017/04/30/Daily-Translation-02/2017/04/30/Daily-Translation-02/1-Obf0CZsSdkg-W38MdS756w.jpeg" alt="Mark Zuckerberg using Oculus Touch in our Social VR demo" title="Mark Zuckerberg using Oculus Touch in our Social VR demo"><p>我们决定始终使用虚拟手本身做出默认的互动，不需要任何控制器的前置按钮。拿起并握住一个物体通过按下并握住手柄；与2D用户界面互动通过简单的触碰它；选择远距离的物体通过食指指向；滚动列表可以通过用你的手抓住列表上下移动，或通过抓取并移动其滚动条来完成。</p><p>我们还了解了视觉、听觉、触觉反馈在创造自然交互中的重要意义。当你用手接触一个物体时，通过打开或其他手势获得诸如物体变大、变亮，或是看见你的手适应的反馈，有助于理解该物体是可与之交互的。当触摸物体时，获得触觉和听觉上的反馈会增加身临其境的感觉，并且创建虚拟手与物理手之间的连接。想象你拍手发出的声音，或拿起和放下手机时的微妙感觉。</p><img src="/2017/04/30/Daily-Translation-02/2017/04/30/Daily-Translation-02/1-iYe7d8lyj4-Jgw1_Ejq3YA.jpeg" alt="1-iYe7d8lyj4-Jgw1_Ejq3YA.jpeg" title=""><h2 id="设计舒适的体验"><a href="#设计舒适的体验" class="headerlink" title="设计舒适的体验"></a>设计舒适的体验</h2><p>使用手在虚拟世界进行互动是难以置信的激动和沉浸的，也为设计人员的思考提供了物理上的挑战。这里有一些我学到的东西：</p><ul><li>抬高手臂进行互动的程度越高，交互应该越快，以避免疲劳。</li><li>手臂抬起足够高以至于肘部不再与身体连接，会造成疲劳。</li><li>与一个移动界面进行交互（比如连接到辅助手的用户界面，像手表）具有挑战性，应主要用于快速操作。</li><li>一些人在视觉解读深度方面有困难，可能需要一些时间和时间才能了解他们需要多远来接触并操作VR中的物体。</li><li>坐着的时候，伸手接触与虚拟身体很近的物体可能具有挑战。</li><li>视觉上看起来真实的手可能会令人感觉怪异。它们带来一种在别人身体里的感觉。它们也可能会打破沉浸感，当那些相同的手穿透一个似乎是物理上的对象，比如桌子。</li><li>一个熟悉的对象需要经常沟通如何拿起、握住和使用。比如人们期望使用一个枪支型道具来瞄准东西。</li><li>一个物理对象在被抓住时可能需要不同物理学。如果拿着一把比穿过一个桌子，你可能不希望笔撞到桌子并且弹跳，反而是用手穿透桌子。</li><li>在VR中人们会不再掌握真实物理环境的最新情况。手势可能导致人们达到墙壁或者扔掉他们的控制器。</li><li>手势本身就是社交的。当你的朋友正在VR中与虚拟对象进行互动，可以直观的看到他们正在进行的交互，从而消除了看到他们的手在空中挥动却不知道他们正在干嘛的尴尬。</li></ul><img src="/2017/04/30/Daily-Translation-02/2017/04/30/Daily-Translation-02/1-xQT0OFhsrbsifbFC6zCTcg.jpeg" alt="1-xQT0OFhsrbsifbFC6zCTcg.jpeg" title=""><h2 id="更多的资源"><a href="#更多的资源" class="headerlink" title="更多的资源"></a>更多的资源</h2><p>希望这些观点能帮助你思考VR中手的设计。我们仍然处在初期阶段，就像我们在这过程中取得的进展，我们的设计过程也在发展和改善。今天，我们很高兴发布一套我们创建的虚拟手，可以帮助设计师在传统的设计工具中模拟VR手的互动。如果你对构建一个社会虚拟现实体验其他方面感到好奇，可以看看<a href="https://www.youtube.com/watch?v=0EZn50XCueI" target="_blank" rel="external">Mike Booth’s talk</a>在Oculus Connect上的演讲。</p><img src="/2017/04/30/Daily-Translation-02/2017/04/30/Daily-Translation-02/1-Zu9yKwrbBvn5QEFAMqGcBg.jpeg" alt="1-Zu9yKwrbBvn5QEFAMqGcBg.jpeg" title="">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天的翻译来源于 &lt;a href=&quot;http://facebook.design&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://facebook.design&lt;/a&gt; ，一直觉得未来的VR会超屌的，就进一步了解一下。先是来到 &lt;a href=&quot;http://facebook.design/virtual-hands#scroll&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://facebook.design/virtual-hands#scroll&lt;/a&gt; 下载了Virtual Hands，各种手势的png麻溜的存起来，保不准以后有用呢。页面上有一个链接跳转到Christophe Tauziet制作VR手势交互的文章 &lt;a href=&quot;https://medium.com/facebook-design/designing-for-hands-in-vr-61e6815add99&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Designing for Hands in VR&lt;/a&gt; 这篇文章。那今天就翻译这篇吧~&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
