<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Colorjam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-19T06:51:10.142Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Colorjam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HEXO主题cactus修改</title>
    <link href="http://yoursite.com/2019/05/19/hexo-theme-cactus/"/>
    <id>http://yoursite.com/2019/05/19/hexo-theme-cactus/</id>
    <published>2019-05-19T03:05:14.000Z</published>
    <updated>2019-05-19T06:51:10.142Z</updated>
    
    <content type="html"><![CDATA[<p>cacuts的主题很简洁，用得蛮久，看到原库有更新，所以fork了新的版本并在上面做一些修改，顺便记录一下过程。</p><h3 id="主题镜像"><a href="#主题镜像" class="headerlink" title="主题镜像"></a>主题镜像</h3><p>首先根据<a href="https://help.github.com/en/articles/duplicating-a-repository" target="_blank" rel="external">Mirrow a repository</a>镜像一个库。在push的时候还遇到了403问题：</p><blockquote><p>remote: Permission to colorjam/hexo-theme-cactus-mirrored.git denied to xxx</p></blockquote><p>通过删除<strong>Keychain Access</strong>中存储的github.com的Internet password得到解决。然后把自己的库再Clone进<code>themes</code>中</p><h3 id="样式编辑"><a href="#样式编辑" class="headerlink" title="样式编辑"></a>样式编辑</h3><ul><li>主题颜色</li></ul><p>在<code>source/css/_colors</code>下新建了一个<code>pink.styl</code>，同时修改<code>_config.yml</code>中的<code>colorscheme:pink</code>。</p><ul><li>logo设置</li></ul><p>把<code>source/images/</code>下的<code>favicon.ico</code>和<code>logo.png</code>换成自己喜欢的图片。修改<code>source/css/_partial/header.styl</code>中的<code>#logo</code> 的<code>background-size: contain</code></p><ul><li>细节调整</li></ul><p>删除<code>header.styl</code>中html的<code>border-top</code></p><p>链接样式：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">a</span></div><div class="line">color: $color-text</div><div class="line"><span class="selector-tag">text-decoration</span>: <span class="selector-tag">none</span></div><div class="line"></div><div class="line"></div><div class="line">&amp;<span class="selector-pseudo">:hover</span></div><div class="line">background-image: linear-gradient(transparent, transparent 4px, $color-link 4px, $color-link)</div><div class="line"><span class="selector-tag">background-position</span>: <span class="selector-tag">bottom</span></div><div class="line"><span class="selector-tag">background-size</span>: 100% 6<span class="selector-tag">px</span></div><div class="line"><span class="selector-tag">background-repeat</span>: <span class="selector-tag">repeat-x</span></div></pre></td></tr></table></figure><p>行内代码样式：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">code</span></div><div class="line">  <span class="selector-tag">padding</span>: 0 5<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">background</span>: <span class="selector-id">#fafafa</span></div><div class="line">  <span class="selector-tag">border-radius</span>: 2<span class="selector-tag">px</span></div><div class="line">  <span class="selector-tag">-webkit-border-radius</span>: 2<span class="selector-tag">px</span></div></pre></td></tr></table></figure><h3 id="会动的粒子"><a href="#会动的粒子" class="headerlink" title="会动的粒子"></a>会动的粒子</h3><p>在背景加上<a href="https://github.com/VincentGarreau/particles.js/" target="_blank" rel="external">会动的粒子</a>，在<code>source/lib</code>里创建一个particles文件夹，把<code>particles.min.js</code>放进去。</p><p>在<code>layout.ejs</code>中加入</p><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"particles-js"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></div></pre></td></tr></table></figure><p>在<code>scripts.ejs</code>中添加脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;!-- particles --&gt;</div><div class="line">&lt;%- js(&apos;lib/particles/particles.min&apos;) %&gt;</div><div class="line">&lt;script type=&quot;text/javascript&quot;&gt;</div><div class="line">particlesJS(&apos;particles-js&apos;, &#123;</div><div class="line">        ...</div><div class="line">        &#125;</div><div class="line">      )</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure><p>在<code>style.css</code>中添加样式：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="selector-id">#particles-js</span> &#123;</div><div class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</div><div class="line">  <span class="attribute">position</span>: absolute;</div><div class="line">  <span class="attribute">margin-left</span>: -<span class="number">28%</span>;</div><div class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;cacuts的主题很简洁，用得蛮久，看到原库有更新，所以fork了新的版本并在上面做一些修改，顺便记录一下过程。&lt;/p&gt;
&lt;h3 id=&quot;主题镜像&quot;&gt;&lt;a href=&quot;#主题镜像&quot; class=&quot;headerlink&quot; title=&quot;主题镜像&quot;&gt;&lt;/a&gt;主题镜像&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="维修指南" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow内存泄漏</title>
    <link href="http://yoursite.com/2019/05/18/tf-memory-leak/"/>
    <id>http://yoursite.com/2019/05/18/tf-memory-leak/</id>
    <published>2019-05-18T05:13:31.000Z</published>
    <updated>2019-05-19T06:54:57.626Z</updated>
    
    <content type="html"><![CDATA[<p>用tf经常会出现OOM的现象，查了一下发现了一篇文章<a href="https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/" target="_blank" rel="external">How To Debug A Memory Leak In Tensorflow</a></p><p>由于tf存在内存泄漏问题，许多人会用 <a href="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" target="_blank" rel="external">tcmalloc</a> 来替代 malloc()。</p><p>但是运行程序的时候会报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ERROR: ld.so: object <span class="string">'/usr/lib/libtcmalloc.so.4'</span> from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.</div></pre></td></tr></table></figure><p>踩了一系列坑以后发现，将<code>/usr/lib/libtcmalloc.so.4</code>改为<code>/usr/local/lib/libtcmalloc.so.4</code>即可。</p><p><strong>参考链接：</strong></p><ul><li><a href="https://www.cnblogs.com/Lelouch/p/3365672.html" target="_blank" rel="external">https://www.cnblogs.com/Lelouch/p/3365672.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用tf经常会出现OOM的现象，查了一下发现了一篇文章&lt;a href=&quot;https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;How
      
    
    </summary>
    
    
      <category term="维修指南" scheme="http://yoursite.com/tags/%E7%BB%B4%E4%BF%AE%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>pair-wise-loss</title>
    <link href="http://yoursite.com/2019/05/17/pair-wise-loss/"/>
    <id>http://yoursite.com/2019/05/17/pair-wise-loss/</id>
    <published>2019-05-17T05:44:19.000Z</published>
    <updated>2019-05-17T08:08:40.815Z</updated>
    
    <content type="html"><![CDATA[<p>用Tensorflow复现论文中的pair wise loss<br>$$<br>\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathcal{R}} \sum_{j \in \mathcal{R}}\left(a_{ij}^{s}-a_{ij}^{t}\right){2}<br>$$<br>其中<br>$$<br>a_{i j}=\mathbf{f}_{i}^{\top} \mathbf{f}_{j} /\left(\left|\mathbf{f}_{i}\right|_{2}\left|\mathbf{f}_{j}\right|_{2}\right)<br>$$<br>$f_i$和$f_j$分别代表ith / jth像素点的c维特征。参考了余弦相似性的计算方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">similarity</span><span class="params">(x)</span>:</span></div><div class="line">    x = tf.reshape(x, [x.shape[<span class="number">0</span>], <span class="number">-1</span>, x.shape[<span class="number">-1</span>]])</div><div class="line">    norm = tf.nn.l2_normalize(x, <span class="number">2</span>)</div><div class="line">    a = tf.matmul(norm, norm, adjoint_b = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> a</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_loss</span><span class="params">(x, y)</span>:</span></div><div class="line">    _, h, w, _  = x.shape</div><div class="line">    pa = tf.reduce_sum(tf.pow((similarity(x) - similarity(y)), <span class="number">2</span>)) / tf.pow(tf.cast(h*w, tf.float32),<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> pa</div></pre></td></tr></table></figure><p>参考链接：</p><ul><li><a href="https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1" target="_blank" rel="external">https://stackoverflow.com/questions/48485373/pairwise-cosine-similarity-using-tensorflow?rq=1</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用Tensorflow复现论文中的pair wise loss&lt;br&gt;$$&lt;br&gt;\ell_{p a}(\mathrm{S})=\frac{1}{\left(W^{\prime} \times H{\prime}\right)^{2}} \sum_{i \in \mathc
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Survey Of Methods For Explaining Black Box Models</title>
    <link href="http://yoursite.com/2019/05/17/a-survey-of-methods-for-explaning-black-box-models/"/>
    <id>http://yoursite.com/2019/05/17/a-survey-of-methods-for-explaning-black-box-models/</id>
    <published>2019-05-17T02:35:35.000Z</published>
    <updated>2019-05-18T08:45:01.259Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了已有工作关于黑箱模型可解释性的方法。分为以下几个部分：1）通过一些案例表明为什么需要解释黑箱模型。2）什么是可解释性。3）阐明了本文的归类标准。4）现有的方法、论文及其解决方案。</p><h3 id="Needs-for-Interpretable-Models"><a href="#Needs-for-Interpretable-Models" class="headerlink" title="Needs for Interpretable Models"></a>Needs for Interpretable Models</h3><p>模型的预测结果可能会造成<strong>社会歧视问题</strong>：St. George’s Hospital Medical School在70-80年代编程利用申请者信息（姓氏、出生地点）降低少数民族和妇女的机会。<em>propublica.org</em>利用COMPAS分数预测再犯罪的的风险，其中黑人是白人的两倍。同时模型预测存在<strong>训练偏差问题：</strong>在训练和测试过程的精度可能很高，但是在实际应用效果不好（举了坦克、狼和狗的例子）。在深度学习的各个领域（计算机视觉、自然语言处理）也都存在人与计算机之间差距很大的问题。</p><h3 id="Interpretable-Explainable-and-Comprehensible-Models"><a href="#Interpretable-Explainable-and-Comprehensible-Models" class="headerlink" title="Interpretable, Explainable and Comprehensible Models"></a>Interpretable, Explainable and Comprehensible Models</h3><p>可解释性与决策判断密切相关，当需要利用模型的预测结果进行决策时，我们才需要解释模型为何会呈现这样的结果。进行解释的时候我们要考虑很多维度：全局/局部可解释性、时间限制、用户习惯等。模型需要具备的特征：多大程度能够被理解（interpretable/comprehensibility）、准确度（accuracy）、忠诚度（fidelity）。在特定领域还有一些特征比如公平性（fairness）、隐私性（privacy）、可靠性（raliability）、鲁棒性（robustness）、因果性（causality）、可扩展性（scalability）、通用型（generality）等也需要考虑。</p><p>目前已有的可解释模型包括决策树（decision tree）、规则（rules）、线性模型（linear models）。模型的复杂度（complexity）时常会被人们忘记。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文总结了已有工作关于黑箱模型可解释性的方法。分为以下几个部分：1）通过一些案例表明为什么需要解释黑箱模型。2）什么是可解释性。3）阐明了本文的归类标准。4）现有的方法、论文及其解决方案。&lt;/p&gt;
&lt;h3 id=&quot;Needs-for-Interpretable-Models
      
    
    </summary>
    
    
      <category term="Paper" scheme="http://yoursite.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>LQ-Nets</title>
    <link href="http://yoursite.com/2019/05/14/LQ-Nets/"/>
    <id>http://yoursite.com/2019/05/14/LQ-Nets/</id>
    <published>2019-05-14T09:02:31.000Z</published>
    <updated>2019-05-18T08:21:12.014Z</updated>
    
    <content type="html"><![CDATA[<p>传统的量化方法主要使用固定的或者手工设计的量化方案（均匀量化/对数量化）：<br>$$<br>Q(x)=q_{l}, \text { if } x \in\left(t_{l}, t_{l+1}\right]<br>$$<br>本文提出了可学习的量化方式：<br>$$<br>Q_{\text { ours }}(x, \mathbf{v})=\mathbf{v}^{\mathrm{T}} \mathbf{e}_{l}, \quad \text { if } x \in\left(t_{l}, t_{l+1}\right]<br>$$<br>其中 $\mathbf{v} \in \mathbb{R}^{K}$是$\mathbf{e}_{l} \in\{-1,1\}^{K}$, $K$代表bit数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;传统的量化方法主要使用固定的或者手工设计的量化方案（均匀量化/对数量化）：&lt;br&gt;$$&lt;br&gt;Q(x)=q_{l}, \text { if } x \in\left(t_{l}, t_{l+1}\right]&lt;br&gt;$$&lt;br&gt;本文提出了可学习的量化方式：&lt;br&gt;$$&lt;br
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.02</title>
    <link href="http://yoursite.com/2019/05/13/weekly-paper-02/"/>
    <id>http://yoursite.com/2019/05/13/weekly-paper-02/</id>
    <published>2019-05-13T02:50:43.000Z</published>
    <updated>2019-05-19T04:29:39.092Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION"><a href="#1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION" class="headerlink" title="1️⃣ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION"></a>1️⃣ ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION</h3><p>在《Revisiting the Importance of Individual Units in CNNs via Ablation》的基础上看了这篇论文。</p><p>本文探究的是激活值的单方向依赖对网络泛化性能的影响，通过对units进行抑制/加噪声，表示网络对单反向的依赖能较好的预测其泛化性能。文章讲了一个故事，一个网络只通过记忆每张输入和其对应的输出，泛化性差(memorizing network)，另一个网络能够找到数据中的结构性，泛化性佳(structure-finding network)。memorizing network找到的最小描述长度应该大于structure-finding network。因此，memorizing network会使用更多的单反向，那么，如果随机扰乱单一方向，对memorizing network的影响应该大于structure-finding network。</p><p>通过对dropout和BN实验（两个方法都增强了网络的泛化性），表明尽管dropout能在一定程度上避免记忆随机标签，但不能避免训练过程中的过度单方向依赖。加了BN的网络进行神经元抑制时，训练精度会降得比较慢，说明BN也不鼓励单方向依赖。</p><p>接着文章验证了class selectivity与神经元重要性的关系。提出了两个问题：</p><ol><li><p>BN不鼓励单方向依赖，那么是否会影响单反向的类别信息分布？</p><p>文章使用class selectivity来衡量类别信息分布，high class selectivity说明关注的是单一类别，low class selectivity说明关注的是多个类别。没有BN的网络反而显示出更高的class selectivity。表明BN层鼓励feature map去学习多种类别的信息，而不是关注单一类别。</p></li><li><p>是否能够利用unit的class selectivity，判断unit的重要性？</p><p>文章发现class selectivity和网络浅层的feature map负相关，与网络深层则无关。作者利用互信息也做了相同的实验。得到一致的结果。以此说明class selectiviy并不能代表unit的重要性。</p></li></ol><blockquote><p>🧐 本文的结论是紧凑网络对单方向的依赖性较少，那么如何找到一个衡量unit方向性的函数，来进行网络压缩呢？</p></blockquote><h3 id="2️⃣-MaskConnect-Connectivity-Learning-by-Gradient-Descent"><a href="#2️⃣-MaskConnect-Connectivity-Learning-by-Gradient-Descent" class="headerlink" title="2️⃣  MaskConnect: Connectivity Learning by Gradient Descent"></a>2️⃣  MaskConnect: Connectivity Learning by Gradient Descent</h3><p>用梯度下降自动学习连接。和网络权重一起学习<em>connectivity masks</em>，来决定网络block之间的连接。</p><p>第$j$个block的输入可以由前面所有输出相加而成，用二值的$m$表示是否连接：<br>$$<br>\mathbf{x}_{j}=\sum_{k=1}^{j-1} m_{j, k} \cdot \mathbf{y}_{k}<br>$$<br>本文表示每个block只和$K$个连接效果最好：<br>$$<br>m_{j, k} \in\{0,1\} \forall j, k, \quad and \quad \sum_{k=1}^{j-1} m_{j, k}=K \forall j<br>$$<br>🔺 训练过程：</p><p><strong>Forward Propagation</strong>. 限制实值的mask的和为1，即$\sum_{k=1}^{j-1} \tilde{m}_{j, k}=1$，代表一个多项式分布，从中采样K个样本$a_{1}, a_{2}, \ldots, a_{K} \in\{1, \ldots,(j-1)\}$，激活对应的mask $m_{j, a_{k}} \leftarrow 1$。</p><p><strong>Backward Propagation.</strong> 第$k$个block输出的梯度通过二值$m_{j,k}$和$x_j$的梯度获得。</p><p><strong>Mask Update.</strong> 通过clip实值mask，限制它们在[0,1]的范围。</p><p>🔺 训练结束：</p><p>（1）为每个$m_j$激活$\tilde{m}_{j}$中top-K的连接，</p><p>（2）固定二值mask，ft网络权重$\theta$</p><blockquote><p>🧐 本文算是NAS的分支吧，搜索的只是网络块之间的连接。每个block有一个多项式分布，代表它与之前所有block连接的概率，从这个分布中采样激活的连接。结合我想做的东西，<strong>根据不同的输入图片选择不同的block</strong>，每个block的输出为一个num_classes的分布，每个元素代表某个类激活这个block概率，利用这个概率进行二项式分布的采样。</p></blockquote><h3 id="3️⃣-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION"><a href="#3️⃣-MODEL-COMPRESSION-VIA-DISTILLATION-AND-QUANTIZATION" class="headerlink" title="3️⃣ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION"></a>3️⃣ MODEL COMPRESSION VIA DISTILLATION AND QUANTIZATION</h3><p>本文提出了两个压缩方法：1.<em> quantized distillation</em>：利用蒸馏训练权重是量化的小网络。 2. <em>differentiable quantization</em>：通过梯度下降优化量化点的位置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATION&quot;&gt;&lt;a href=&quot;#1️⃣-ON-THE-IMPORTANCE-OF-SINGLE-DIRECTIONS-FOR-GENERALIZATI
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Revisiting the Importance of Individual Units in CNNs via Ablation</title>
    <link href="http://yoursite.com/2019/05/12/Revisiting%20the%20Importance%20of%20Individual%20Units%20in%20CNNs%20via%20Ablation/"/>
    <id>http://yoursite.com/2019/05/12/Revisiting the Importance of Individual Units in CNNs via Ablation/</id>
    <published>2019-05-12T05:51:59.000Z</published>
    <updated>2019-05-14T08:10:47.189Z</updated>
    
    <content type="html"><![CDATA[<p>之前的一些工作通过可视化每个神经元的方式来理解神经网络，它们选择的是<em>high selectivity</em>的神经元，发现网络浅层识别的是具体的图案（e.g 纹理、图像），网络深层识别的是语义信息（e.g 狗头、车轮），论文[11]似乎打脸了这种方式，表明对于代表整体分类精度，<em>class selectivity</em>属性不能用来预测神经元的重要性。</p><p>本文表明这两种方式都是合理的。用<em>class selectivity</em>或其他属性来预测神经元的重要性从整体分类精度（网络的泛化性</p><p>）上来看确实不好，但是能作为具体类别的判断依据。</p><p><strong>抑制神经元的方式</strong>：将其weight和bias设置成0。</p><p><strong>两种精度下降类型：</strong>overall accuracy drop &amp; max class accuracy drop</p><p><strong>判断神经元重要性的属性：</strong></p><ul><li><p>L1 Norm：<br>$$<br>\operatorname{norm}_{1}(i)=\left|w_{i}\right|_{1}=\sum_{j}\left|\left(w_{i}\right)_{j}\right|<br>$$</p></li><li><p>Class Correlation：<br>$$<br>\operatorname{corr}(i, k)=\frac{E\left[\left(x_{i}-\overline{x}_{i}\right)\left(p_{k}-\overline{p}_{k}\right)\right]}{\sigma_{x_{i}} \sigma_{p_{k}}}<br>$$</p></li><li><p>Class Selectivity：</p></li></ul><p>$$<br>\operatorname{select}(i, k)=\frac{\overline{x}_{i}^{k}-\overline{x}_{i}^{-k}}{\overline{x}_{i}^{k}+\overline{x}_{i}^{-k}}<br>$$</p><p>​        其中$\overline{x}_{i}^{k}$表示神经元$i$属于kth类别的平均激活值，$\overline{x}_{i}^{-k}$表示神经元$i$属于non-kth类别的平均激活值的均值。这个值的范围是[0, 1]，0表示一个神经元的平均激活值与其他类别都相同，1表示一个神经元只对某个类别的输入有反应。</p><ul><li>Concept Alighment：IoU between unit activation and gt concepts</li><li>Unit Visualization</li></ul><p>🔺 <strong>实验一：</strong>验证抑制单个神经元/一组神经元对两种精度下降类型的影响。</p><ul><li>实验方式：<ul><li>抑制某个神经元，横轴表示CLass，纵轴表示Class Accuracy Drop。</li><li>针对特定的网络层，根据Mac Class Accuracy Drop进行排序，绘制三条曲线（Overal Accuracy Drop / Max Class Accuracy Drop / Min Class Accuracy Drop）。</li><li>利用greedy的方式迭代地移除降低特定类准确率最多的神经元。绘制了特定类别精度下降的曲线，和所有类别平均精度下降的曲线。同时用random作为baseline。</li></ul></li><li>结论：<ul><li>抑制单个神经元对某些类别的分类精度影响很大，但对总体的精度影响不大，并且能通过可视化的形式看出这些抑制的神经元确实展现出了相应类别的特点。</li><li>greedy地抑制一组神经元，能使这个类别的精度大大降低，但是random的方式影响不大。</li></ul></li></ul><p>🔺 <strong>实验二：</strong>验证不同属性与精度下降之间的关系。</p><ul><li>实验方式：<ul><li>用斯皮尔曼相关系数和P值统计了不同属性值与精度下降之间的相关性。</li><li>用不同属性判断出的最重要的那个神经元来预测分类</li></ul></li><li>结论：<ul><li>对于整体精度下降：class selectivity，class correlation和concept alighment表示出正相关，L1是负相关。也就是说，当抑制class selectivity值很大的神经元，对整体网络的精度下降影响较小，与论文[11]中结论一致。</li><li>对于最大类别精度下降：每个属性基本都表现出负相关。说明抑制这些属性值大的神经元，对特定类别精度影响很大。</li><li>Concept Alignment似乎最能代表神经元的重要性</li></ul></li></ul><p>🔺 <strong>实验三：</strong>验证选择的神经元与其方向相关，而不是随机方向。</p><p>🔺 <strong>实验四：</strong>验证BN和Dropout的影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的一些工作通过可视化每个神经元的方式来理解神经网络，它们选择的是&lt;em&gt;high selectivity&lt;/em&gt;的神经元，发现网络浅层识别的是具体的图案（e.g 纹理、图像），网络深层识别的是语义信息（e.g 狗头、车轮），论文[11]似乎打脸了这种方式，表明对于代表
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>每周论文 Vol.01</title>
    <link href="http://yoursite.com/2019/05/11/weekly-paper-01/"/>
    <id>http://yoursite.com/2019/05/11/weekly-paper-01/</id>
    <published>2019-05-10T23:57:11.000Z</published>
    <updated>2019-05-12T09:01:21.046Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019"><a href="#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019" class="headerlink" title="1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)"></a>1️⃣ SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY (ICLR2019)</h3><p>基于度量的一种剪枝方法。度量的是权重之间的连接敏感度。优点在于不需要layer-by-layer的训练过程。</p><p>砍掉某个连接$j$对loss的影响：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D})=L(\mathbf{1} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{1}-\mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)<br>$$<br>但是上式不可导，作者用$g_j$来代表评估指标，将离散的$e_j$松弛为连续值$\delta e_j$：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D}) \approx g_{j}(\mathbf{w} ; \mathcal{D})=\left.\frac{\partial L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})}{\partial c_{j}}\right|_{\mathbf{c}=1}=\lim _{\delta \rightarrow 0}\left.\frac{L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{c}-\delta \mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)}{\delta}\right|_{\mathbf{c}=1}<br>$$<br>定义了连接敏感度：<br>$$<br>s_{j}=\frac{\left|g_{j}(\mathbf{w} ; \mathcal{D})\right|}{\sum_{k=1}^{m}\left|g_{k}(\mathbf{w} ; \mathcal{D})\right|}<br>$$<br>算法过程：</p><p><img src="https://i.loli.net/2019/05/09/5cd3a33306172.png" alt=""></p><blockquote><p>连接敏感度的计算方式简单，就像是在权重上加了一些噪音，看一下它对loss的影响。但是它的算法过程是对随机初始化的网络权重进行剪枝，再训练q。这样的剪枝是否有意义？</p></blockquote><h3 id="2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks"><a href="#2️⃣-Numerical-Coordinate-Regression-with-Convolutional-Neural-Networks" class="headerlink" title="2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks"></a>2️⃣ Numerical Coordinate Regression with Convolutional Neural Networks</h3><p>📍<a href="https://github.com/anibali/dsntnn" target="_blank" rel="external">https://github.com/anibali/dsntnn</a></p><p>本文提出了一个空间可微的数值转换操作(DSNT)来找到输入图像的显著点坐标。</p><p>传统方法有<strong>Heatmap matching</strong>和<strong>Fully connected</strong>，代表性的运用是<em>Human pose estimation</em>和<em>STN</em>。前者不完全可微，后者缺乏空间泛化能力。</p><p><img src="https://i.loli.net/2019/05/07/5cd10b71062f2.png" alt=""></p><p>DSNT的输入是一个单通道归一化的大小为$m \times n$ 的 heatmap $\hat{Z}$，代表了概率分布。输出是显著点的坐标，即概率分布最大点的那个坐标值。用两个相同大小的矩阵$X$和$Y$分别代表$x-$和$y-$坐标，让坐标分布变为左上角是(-1, -1)，右下角是(1,1)。</p><p>我们可以通过坐标$c$计算出这个概率函数：<br>$$<br>\operatorname{Pr}\left(\mathbf{c}=\left[ \begin{array}{ll}{X_{i, j}} &amp; {Y_{i, j}}\end{array}\right]\right)=\hat{Z}_{i, j}<br>$$<br>传统方法利用的是$c$的模，DSNT利用的是$c$的期望$\boldsymbol{\mu}=\mathbb{E}[\mathbf{c}]$，可以用如下公式表示：<br>$$<br>\operatorname{DSNT}(\hat{Z})=\boldsymbol{\mu}=\left[\langle\hat{Z}, \boldsymbol{X}\rangle_{F}<br>\quad\langle\hat{\boldsymbol{Z}}, \boldsymbol{Y}\rangle_{F}\right]<br>$$</p><blockquote><p>The “mode” is the value that occurs most often. The “mean” is the “average” you’re used to, where you add up all the numbers and then divide by the number of numbers.</p></blockquote><p>一个直观的例子：</p><p><img src="https://i.loli.net/2019/05/07/5cd1422432340.png" alt=""></p><p>DSNT的损失函数由两个部分组成：<br>$$<br>\mathcal{L}(\hat{Z}, \boldsymbol{p})=\mathcal{L}_{e u c}(\operatorname{DSNT}(\hat{Z}), \boldsymbol{p})+\lambda \mathcal{L}_{r e g}(\hat{Z})<br>$$<br>第一部分直接计算预测坐标和gt的欧式距离：<br>$$<br>\mathcal{L}_{e u c}(\boldsymbol{\mu}, \boldsymbol{p})=|\boldsymbol{p}-\boldsymbol{\mu}|_{2}<br>$$<br>第二部分是正则项，文章对比了两种正则方法：</p><ol><li><p>方差正则：<br>$$<br>\begin{aligned} \operatorname{Var}\left[\mathrm{c}_{x}\right] &amp;=\mathbb{E}\left[\left(\mathrm{c}_{x}-\mathbb{E}\left[\mathrm{c}_{x}\right]\right)^{2}\right] \\ &amp;=\left\langle\hat{Z},\left(\boldsymbol{X}-\mu_{x}\right) \odot\left(\boldsymbol{X}-\mu_{x}\right)\right\rangle_{F} \end{aligned}<br>$$</p><p>$$<br>\mathcal{L}_{v a r}(\hat{Z})=\left(\operatorname{Var}\left[c_{x}\right]-\sigma_{t}^{2}\right)^{2}+\left(\operatorname{Var}\left[\mathrm{c}_{y}\right]-\sigma_{t}^{2}\right)^{2}<br>$$</p></li><li><p>分布正则（KL散度/JS散度）：<br>$$<br>\mathcal{L}_{D}(\hat{Z}, \boldsymbol{p})=D\left(p(\mathbf{c}) | \mathcal{N}\left(\boldsymbol{p}, \sigma_{t}^{2} \boldsymbol{I}_{2}\right)\right)<br>$$</p></li></ol><h3 id="3️⃣-Searching-for-MobileNetV3"><a href="#3️⃣-Searching-for-MobileNetV3" class="headerlink" title="3️⃣ Searching for MobileNetV3"></a>3️⃣ Searching for MobileNetV3</h3><ul><li><p>用搜索的方式搜索网络结构</p><ul><li>Block-wise：用MnasNet-A1作为初始大网络，修改了反馈$A C C(m) \times[L A T(m) / T A R]^{w}$</li><li>Layer-wise：NetAdapt的剪枝方式，修改了评估指标$\frac{\Delta \mathrm{Acc}}{|\Delta \mathrm{latency}|}$</li></ul></li><li><p>加入了SEblock，重新设计了网络的输出层</p><p><img src="https://i.loli.net/2019/05/09/5cd39f284e1fb.png" alt=""></p><p><img src="https://i.loli.net/2019/05/09/5cd39f84c5807.png" alt=""></p></li><li><p>基于swish提出了h-swish替代ReLU<br>$$<br>\mathrm{h}-\operatorname{swish}[x]=x \frac{\operatorname{Re} \mathrm{LU} 6(x+3)}{6}<br>$$</p></li><li><p>在语义分割任务上基于R-ASPP，提出了LR-ASSP。</p><p><img src="https://i.loli.net/2019/05/09/5cd3a1b27ccf1.png" alt=""></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY-ICLR2019&quot;&gt;&lt;a href=&quot;#1️⃣-SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-
      
    
    </summary>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="每周论文" scheme="http://yoursite.com/tags/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>namesilo域名购买 &amp; github pages域名设置</title>
    <link href="http://yoursite.com/2019/05/10/set-blog-domain/"/>
    <id>http://yoursite.com/2019/05/10/set-blog-domain/</id>
    <published>2019-05-10T15:42:05.000Z</published>
    <updated>2019-05-12T10:56:08.729Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-域名购买和设置"><a href="#1-域名购买和设置" class="headerlink" title="1. 域名购买和设置"></a>1. 域名购买和设置</h3><p>首先在<a href="http://xn--eqrt2g.xn--vuq861b/#" target="_blank" rel="external">工信部公示</a>查看可备案的网站后缀，在<a href="https://www.namesilo.com/" target="_blank" rel="external">namesilo</a>上购买喜欢的域名，可以使用支付宝付款。</p><ul><li>进入「域名管理」，点击刚申请到的域名</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd613b55c18e.png" alt=""></p><ul><li>更新「DNS记录」</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd612451ffa6.png" alt=""></p><p>在其中添加以下三条记录：</p><p><img src="https://i.loli.net/2019/05/11/5cd612ba05f60.png" alt=""></p><blockquote><p>A记录类型在<a href="https://help.github.com/en/articles/troubleshooting-custom-domains#dns-configuration-errors" target="_blank" rel="external">这里</a>查看Github的地址，最后一行CNAME是自己的<code>usernmae.github.io</code>，实际上A记录和CNAME指向的是相同的IP地址，设置一项即可。</p></blockquote><h3 id="2-修改域名DNS服务器"><a href="#2-修改域名DNS服务器" class="headerlink" title="2. 修改域名DNS服务器"></a>2. 修改域名DNS服务器</h3><ul><li>在namesilo上将域名服务商修改为<a href="https://www.dnspod.cn/console/dns" target="_blank" rel="external">DNSpod</a></li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6152a1859a.png" alt=""></p><blockquote><p>DNSpod提供了两个免费的DNS地址：f1g1ns1.dnspod.net / f1g1ns2.dnspod.net</p></blockquote><ul><li>登陆DNSpod，添加域名和记录</li></ul><p><img src="https://i.loli.net/2019/05/11/5cd6160b109d2.png" alt=""></p><p><img src="/Users/colorjam/Library/Application Support/typora-user-images/image-20190511082546709.png" alt="image-20190511082546709"></p><h3 id="3-更新Github上的自定义域名"><a href="#3-更新Github上的自定义域名" class="headerlink" title="3. 更新Github上的自定义域名"></a>3. 更新Github上的自定义域名</h3><p>进入本地存储博客的文件夹</p><p><img src="https://i.loli.net/2019/05/11/5cd6186e83770.png" alt=""></p><p>创建一个没有后缀的<code>CNAME</code>文件，输入购买的域名。</p><p><img src="https://i.loli.net/2019/05/11/5cd6190c11dab.png" alt=""></p><p>部署到github上，就可以利用自定义的域名访问博客啦～</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</div></pre></td></tr></table></figure><p><strong>参考链接：</strong></p><ul><li><a href="http://cps.ninja/2016/10/09/customize-your-blog-domain/" target="_blank" rel="external">http://cps.ninja/2016/10/09/customize-your-blog-domain/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-域名购买和设置&quot;&gt;&lt;a href=&quot;#1-域名购买和设置&quot; class=&quot;headerlink&quot; title=&quot;1. 域名购买和设置&quot;&gt;&lt;/a&gt;1. 域名购买和设置&lt;/h3&gt;&lt;p&gt;首先在&lt;a href=&quot;http://xn--eqrt2g.xn--vuq861
      
    
    </summary>
    
    
      <category term="指南" scheme="http://yoursite.com/tags/%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>沿着某个维度进行操作</title>
    <link href="http://yoursite.com/2019/05/10/along-axis/"/>
    <id>http://yoursite.com/2019/05/10/along-axis/</id>
    <published>2019-05-10T04:40:24.000Z</published>
    <updated>2019-05-10T04:57:27.083Z</updated>
    
    <content type="html"><![CDATA[<p>一直对沿着axis进行操作不太理解，今天决定还是要搞明白。</p><p>比如<code>np.sum(a, aixs=1)</code>，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动<code>a[0][0],a[0][1], a[0][2]​</code>。<br><img src="https://pic3.zhimg.com/80/v2-7a0716230a6f3d4840a6098001b1d2a2_hd.jpg" alt="img"></p><p>于是将红色框的元素相加得到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],</div><div class="line">[<span class="number">9</span>],</div><div class="line">[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>如果是<code>np.sum(a, axis=0)</code>，即<code>a[0][0],a[1][0], a[2][0]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">6</span>],[<span class="number">9</span>],[<span class="number">16</span>]</div></pre></td></tr></table></figure><p>参考链接：</p><p><a href="https://zhuanlan.zhihu.com/p/31275071" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/31275071</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直对沿着axis进行操作不太理解，今天决定还是要搞明白。&lt;/p&gt;
&lt;p&gt;比如&lt;code&gt;np.sum(a, aixs=1)&lt;/code&gt;，代表沿着1维操作。这个沿着指的是下标变化的方向，其他维度不动&lt;code&gt;a[0][0],a[0][1], a[0][2]​&lt;/code
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令 - 显示文件指定行</title>
    <link href="http://yoursite.com/2019/05/05/linux-cat/"/>
    <id>http://yoursite.com/2019/05/05/linux-cat/</id>
    <published>2019-05-05T01:42:07.000Z</published>
    <updated>2019-05-12T05:29:04.159Z</updated>
    
    <content type="html"><![CDATA[<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">head -n [行数a] # 显示开始a行</div><div class="line">tail -n [行数a] # 显示最后a行</div><div class="line">tail -n +[行数a] # 从a行以后开始显示</div></pre></td></tr></table></figure><p>单独使用 <code>tail</code> 和 <code>head</code> 比较简单，组合使用时它们的顺序是有讲究的。</p><p>从200行开始显示10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | tail -n +200 | head -n 10</div></pre></td></tr></table></figure><p>等同于显示200行到210行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat [filename] | head -n 210 | tail -n +200</div></pre></td></tr></table></figure><h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><p>还有一种使用<code>sed</code>命令的方式，从200行开始现实10行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sed -n <span class="string">'200, 210p'</span> [filename]</div></pre></td></tr></table></figure><p>参考链接：</p><ul><li><a href="https://blog.csdn.net/vitaminc4/article/details/78136696" target="_blank" rel="external">https://blog.csdn.net/vitaminc4/article/details/78136696</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;cat&quot;&gt;&lt;a href=&quot;#cat&quot; class=&quot;headerlink&quot; title=&quot;cat&quot;&gt;&lt;/a&gt;cat&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;d
      
    
    </summary>
    
    
      <category term="知识口袋" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%8F%A3%E8%A2%8B/"/>
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/08/06/adversiarial-compression/"/>
    <id>http://yoursite.com/2018/08/06/adversiarial-compression/</id>
    <published>2018-08-06T02:13:32.000Z</published>
    <updated>2018-08-17T07:36:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对抗网络压缩"><a href="#对抗网络压缩" class="headerlink" title="对抗网络压缩"></a>对抗网络压缩</h2><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrfwrb0vj314e0fwq4u.jpg" alt="network_architecture"></p><p>对抗网络压缩过程是在老师和学生之间进行博弈，判别器的任务是判别输入样本是来自老师or学生。老师网络事先用标签训练完成，在对抗压缩的过程中不更新。</p><h3 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h3><ol><li>D接受两个网络最后一层的特征图$f^k_{t}(x)$和$f^l_s(x)$作为输入，利用交叉熵进行real/fake判别（蓝色线）</li><li>利用学生和老师的logits计算L2损失，引导学生模仿老师，产生输出（绿色线）</li><li>D接受学生网络dropout后的特征作为输入（红色线），欺骗D将其判别为real。</li></ol><h3 id="目标函数："><a href="#目标函数：" class="headerlink" title="目标函数："></a>目标函数：</h3><p>借鉴cGAN的思想，定义对抗压缩的损失函数为：</p><p><img src="https://ws3.sinaimg.cn/large/0069RVTdly1ftzrynfkouj316e04cdgr.jpg" alt=""></p><p>由于利用的是特征图作为D的输入，学生网络中还有许多参数需要更新，因此需要加入一个损失函数，减少老师网络和学生网络输出数据的差异：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftzs7ncxe2j315o03c0t5.jpg" alt=""></p><p>最后再加上正则项，得出最终的优化目标：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftzs779uxaj315i03m0t9.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;对抗网络压缩&quot;&gt;&lt;a href=&quot;#对抗网络压缩&quot; class=&quot;headerlink&quot; title=&quot;对抗网络压缩&quot;&gt;&lt;/a&gt;对抗网络压缩&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/0069RVTdly1ftzr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>压缩网络模型之结构探索合辑</title>
    <link href="http://yoursite.com/2018/07/29/compression_models/"/>
    <id>http://yoursite.com/2018/07/29/compression_models/</id>
    <published>2018-07-29T06:39:32.000Z</published>
    <updated>2018-11-12T08:00:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟</p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><p>论文：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="external">Aggregated Residual Transformations for Deep Neural Networks</a></p><p>ResNeXt提出了聚集变换(aggregated transofrmations)：<br>$$<br>\mathcal { F } ( \mathbf { x } ) = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } )<br>$$<br>$C$为基数(cardinality)，表明进行聚集变换的集合大小。$\mathcal { T } _ { i } ( \mathbf { x } )$代表一系列变换。</p><p><strong>ResNext与ResNet</strong></p><p>ResNet block：$\mathbf { y } = \mathcal { F } \left( \mathbf { x } , \left\{ W _ { i } \right\} \right) + \mathbf { x }$</p><p>ResNeXt block：$\mathbf { y } = \sum _ { i = 1 } ^ { C } \mathcal { T } _ { i } ( \mathbf { x } ) + \mathbf { x }$</p><p>在计算代价相同的情况下，画出来就是下图的东东（左边的代表ResNet block，右边代表ResNeXt block）：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftrwnc8rz2j30t60cuq4s.jpg" alt="resnet_resnext"></p><p>ResNeXt将ResNet的一个大变换，分解为一系列小变换，然后将结果聚集起来（相加）。两个结构都做了跳跃连接。</p><p><strong>ResNeXt、Inception、Group Convolution</strong></p><p>文章还比较了与Inception组件和组卷积的关系，如下图所示：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftrwicz2c8j31f80fs0wu.jpg" alt=""></p><p>文章在后续的实验中表明，在计算代价近似的情况下，基数$C$越多，准确率越高。</p><h2 id="clcNet"><a href="#clcNet" class="headerlink" title="clcNet"></a>clcNet</h2><p>论文：<a href="https://arxiv.org/abs/1712.06145" target="_blank" rel="external">clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions</a></p><p>本文将深度卷积和组卷积归为一种通用卷积操作——通道局部卷积(channel local convolution, CLC)，并提出了通道依赖图(channel dependency graph, CDG)的表示方法。下图表示了常规卷积(a)、组卷积(b)、深度卷积(c)的CDG。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftry0ix2cwj30vm0b8q4u.jpg" alt=""></p><blockquote><p>CDG这个概念之前看的一篇文章中也提到了：<a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="external">Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast</a>，很直观地对比了ResNet、ResNeXt、MobileNet以及ShuffleNet的组件。</p></blockquote><p>通过CDG可以很明显的看出通道之间的依赖关系（箭头由输出指向输入）。这个依赖关系可以用通道感知域(channel receptive field, CRF)来表示。CRF代表每个输出通道所依赖的输入通道，CRF大小为每个输出通道依赖的输入通道数量。</p><p>分析上图三种卷积操作的CRF大小（size）：(a) $size = 6$，(b) $size =6/3 = 2$，(c) $size=1$</p><p>在这些概念的基础上，作者提出了<strong>全通道感知域(full channel receptive field, FCRF)</strong>，FCRF意味着_CRF大小 = 输入通道数_，即每个输出通道都依赖于所有的输入通道。作者表明FCRF能够更有效地表示特征，并且在实验中能获得更高的准确率。</p><p>作者还提出了交叉组卷积(interlaced group convolution, IGC)，如下图(b)所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt3uwgee8j30vq0ikac6.jpg" alt="igc"></p><p>与组卷积(a)进行比较，可以看出filed数量即为每组的通道数。IGC实际上就是将每组的输出通道，分到不同的field里去。因此交叉组卷积的CRF大小和组卷积是相同的。</p><p><strong>CLC block</strong>是本文提出的组件，由3x3的IGC和1x1的GC构成，组件结构如下图所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftt48c72dkj30x80hqq54.jpg" alt="clcblock"></p><p>在CLC block的基础上，作者探索了FCRF的规则，假设IGC的分组数为$g_1$，$M、L、N$分别为输入、中间阶段、输出的通道数，GC的分组数为$g_2$，要达到FCRF，应满足以下条件：<br>$$<br>L/g_2 \geq  g_1  \quad or  \quad  g _ { 1 } g _ { 2 } \leq L<br>$$<br>基于FCRF规则，使得计算代价最小：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftt4m3cte8j30x808y3zy.jpg" alt="computational_cost"></p><h2 id="ShuffleNet-v2"><a href="#ShuffleNet-v2" class="headerlink" title="ShuffleNet v2"></a>ShuffleNet v2</h2><p>本文指出模型的运算速度不单单取决于每秒浮点运算次数（FLOPS），还有其他的考虑因素，比如内存访问代价（MAC）、运算平台等。通过一系列的控制实验，对于模型的设计提出了4个指导方针：</p><ol><li>相同的输入输出通道宽度，能减少内存访问代价。</li><li>分组过大，会增加内存访问代价。</li><li>网络分割会降低并行度。</li><li>不能忽略逐元素操作。</li></ol><p>基于指导方针，设计了新的ShuffleNet单元：</p><p>针对输入输出相同的单元，在一开始加入了通道分割（Channel Split），取消了组卷积和相加操作，以符合提出的指导方针2和4；针对降采样的单元，利用两路相似的操作进行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总结了近期一些轻量网络模型在网络结构上进行的探索。这些论文的实现方法主要是利用一系列卷积层构成组件，利用堆叠的组件构成模型☟&lt;/p&gt;
&lt;h2 id=&quot;SqueezeNet&quot;&gt;&lt;a href=&quot;#SqueezeNet&quot; class=&quot;headerlink&quot; title=&quot;Sq
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Network compression, speedup</title>
    <link href="http://yoursite.com/2018/07/27/Network-compression-speedup/"/>
    <id>http://yoursite.com/2018/07/27/Network-compression-speedup/</id>
    <published>2018-07-27T03:19:32.000Z</published>
    <updated>2018-07-27T03:30:13.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h2><h3 id="Singular-value-Decomposition-SVD"><a href="#Singular-value-Decomposition-SVD" class="headerlink" title="Singular value Decomposition(SVD)"></a>Singular value Decomposition(SVD)</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto95l6e4aj30w60cajt9.jpg" alt=""><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto96k0qcsj30uy04e755.jpg" alt=""></p><h3 id="Flattened-Convolutions"><a href="#Flattened-Convolutions" class="headerlink" title="Flattened Convolutions"></a>Flattened Convolutions</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto96skp45j30vo0aoac6.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto98lg9m4j30w00asac3.jpg" alt=""></p><h2 id="Weight-Pruning"><a href="#Weight-Pruning" class="headerlink" title="Weight Pruning"></a>Weight Pruning</h2><h3 id="Magnitude-based-method"><a href="#Magnitude-based-method" class="headerlink" title="Magnitude-based method"></a>Magnitude-based method</h3><ul><li>Iterative Pruning + Retraining<br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99b7u7hj30x8084dh3.jpg" alt=""></li><li>Dynamic Network Surgery<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto99ja0soj30vm06ogml.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto99q0hilj30wi0cyq5s.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto99wbecrj30wm082dh2.jpg" alt=""></li></ul><h3 id="Hessian-based-method"><a href="#Hessian-based-method" class="headerlink" title="Hessian-based method"></a>Hessian-based method</h3><ul><li>Diagonal Hessian-based method: Optimal Brain Damage<br>💫  . Motivation<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9a3vy26j30yy04s3zc.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9a9srdtj310s0dqac9.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ae9ircj30zo0d040y.jpg" alt=""></li><li>Full Hessian-based method: Optimal Brain Surgeon<br>💫  . Motivation<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9ajoj63j310s0d4q5t.jpg" alt=""><br>⛓ . Formulation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9aof4apj310s0dutbh.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9atm2jzj311c0aiwgd.jpg" alt=""></li></ul><h2 id="Quantization-method"><a href="#Quantization-method" class="headerlink" title="Quantization method"></a>Quantization method</h2><h3 id="Full-Quantization"><a href="#Full-Quantization" class="headerlink" title="Full Quantization"></a>Full Quantization</h3><ul><li>Fixed-point format<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9ay9ggsj310o0b4tbh.jpg" alt=""><br>⛓ .  Rounding Modes<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9b5dkn2j310u0c6taq.jpg" alt=""><br>🍄 . Multiply and accumulate (MACC) operation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9b9p5z4j30zs0ceq5c.jpg" alt=""></li><li>Code book<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9begz5mj30zc0feq7k.jpg" alt=""></li></ul><h3 id="Quantization-with-full-precision-copy"><a href="#Quantization-with-full-precision-copy" class="headerlink" title="Quantization with full-precision copy"></a>Quantization with full-precision copy</h3><ul><li>Binnaryconnect<br>💫  . Motivation<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9c757zjj31000ag0ue.jpg" alt=""><br>⛓ .  Binarization<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fto9ccdumnj31080eetbw.jpg" alt=""><br>🍄 . Algorithm<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9cijm0jj30zo07o3zt.jpg" alt=""><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fto9cmyf5sj311i082gnq.jpg" alt=""></li><li>Binarized Neural Network (BNN)<br>💫  . Motivation<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fto9css9thj3114066gn9.jpg" alt=""><br>⛓ .  Method<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fto9cxe89pj311k0ecgoo.jpg" alt=""></li></ul><p>​    </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Matrix-Factorization&quot;&gt;&lt;a href=&quot;#Matrix-Factorization&quot; class=&quot;headerlink&quot; title=&quot;Matrix Factorization&quot;&gt;&lt;/a&gt;Matrix Factorization&lt;/h2&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>与Inception有关的那些事儿</title>
    <link href="http://yoursite.com/2018/07/24/inception/"/>
    <id>http://yoursite.com/2018/07/24/inception/</id>
    <published>2018-07-24T05:42:32.000Z</published>
    <updated>2018-07-26T07:26:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="external">《Going Deeper With Convolutions》</a>是最早提出Inception结构的文章。文章指出，增加神经网络的大小通常能获得更好的表现，但随之而来的缺点是模型很容易过拟合，并且计算代价高昂。关于这两个问题，最直观的解决办法是用稀疏的层来代替全连接层。</p><p>然而目前的计算机对于非均匀的稀疏数据运算不太友好，所以大多数系统是基于卷积实现的。卷积是我们稀疏空间域的好帮手，它像一个收集器，将前一层中的图像块稠密连接起来。但是，目前的卷积网络大多是均匀的结构。因此作者就提出了这么一个问题：如何利用好卷积的稀疏性，同时使用对计算机友好的稠密矩阵乘法进行计算。</p><h3 id="Inceptoin-module"><a href="#Inceptoin-module" class="headerlink" title="Inceptoin module"></a>Inceptoin module</h3><p>以往的许多文献表明，稀疏矩阵乘法运算可以通过将稀疏矩阵聚类成相对稠密的子矩阵来实现。Inception结构就是本文提出的稠密组件，用来近似卷积视觉网络的最佳局部稀疏结构。作者首先给出了naive版本的Inception组件(a)，如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftlxlf8uiwj30ss0f0q46.jpg" alt="inception_module_naive"></p><p>作者假设来自前一层的每个单元与输入的某些区域对应。在网络的浅层（靠近输入）部分，单元集中在局部区域，因此可以通过1x1卷积将其传递到下一层网络。同时，在较大的图像块上进行卷积，可以获得更少数量但扩展空间更大的特征图。因此作者将1x1卷积，3x3卷积，5x5卷积，再加上由经验表明效果很好的池化层，对它们的输出进行拼接，一起构成了Inception组件。</p><p>由于要将Inception组件层层堆叠，会导致网络高层部分的特征图通道数增加，这也意味着较大卷积的计算负担加重，特别是还要拼接上池化层的输出。因此作者提出了第二种减少维度的Inception组件(b)，即在较大卷积之前，池化层之后，加入1x1卷积来减少特征图的通道数：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftlxlr4cz3j30sg0fidhf.jpg" alt="inception_module_dimensionality_reduction"></p><h3 id="GoogleNet-architecture"><a href="#GoogleNet-architecture" class="headerlink" title="GoogleNet architecture"></a>GoogleNet architecture</h3><p>由以上两个组件构成的GoogleNet网络结构如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm3sth6elj31ga0w2aia.jpg" alt="GoogleNet"></p><p>在计算参数数量的时候发现了两个问题🤔</p><ol><li>7x7的卷积数量应该为$7^2 \times 3 \times 64 = 9408$，但表格中是2.7K，google了一下说有可能7x7卷积由7x1和1x7卷积构成，因此参数数量为$(7 \times 1+ 1\times7 ) \times 3 \times 64=2688$。</li><li>计算参数时都没有将depth计算在内，并且最后的结果需要除以1024。比如第二个3x3卷积，$(64 \times 64 + 9 \times 64 \times 192) =114,688$，除以1024后得到112K。</li></ol><h3 id="Auxiliary-classifier"><a href="#Auxiliary-classifier" class="headerlink" title="Auxiliary classifier"></a>Auxiliary classifier</h3><p>作者还在Inception (4a) 和 (4d) 之后添加了辅助分类器，将辅助分类器获得的损失以0.3的比例，添加到最后的损失函数中。然而辅助分类器的影响在之后的实验中证明发现是很小的。</p><p>完整的GoogleNet网络图太长啦，仅贴出含辅助分类器的Inception (4a)部分：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm6copd73j31020hudif.jpg" alt="googlenet_auxiliary_classifier"></p><h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception-v2"></a>Inception-v2</h2><p>在<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="external">《Rethinking the Inception Architecture for Computer Vision》</a>中，作者基于GoogleNet的Inception组件，提出了一些新的思考。</p><h3 id="Design-Principles"><a href="#Design-Principles" class="headerlink" title="Design Principles"></a>Design Principles</h3><p>作者首先提出了四个设计原则：</p><p>➊ 避免极端压缩的瓶颈。在信息传递的过程中，应缓慢减小特征图的大小。</p><p>➋ 更高维度的特征图更容易在网络中进行本地处理。在卷积网络中增加每个区块的激活能够解开更多特征。</p><p>❸ 空间聚合可以通过嵌入较低维度的特征图来完成，并且其表示能力没有太多损失。</p><p>❹ 平衡网络的宽度和深度。</p><h3 id="Factorizing-Convolutions"><a href="#Factorizing-Convolutions" class="headerlink" title="Factorizing Convolutions"></a>Factorizing Convolutions</h3><p>➊ 更小的卷积核</p><p>较大的卷积核可以用堆叠的较小的卷积核替代。比如一个5x5卷积就可以用两个连续的3x3替代：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftm6xrwibpj30vo0g40y3.jpg" alt="smaller_convolutions"></p><p>这样就减小了计算代价：$(3+3) \times 2 / (5+5)=18/25$</p><p>➋ 非对称卷积</p><p>一个3x3卷积又可以在空间上分解为3x1卷积和1x3卷积：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftm716z34xj30vs0jkq69.jpg" alt="asymmetric_convolution"></p><p>再一次减少了计算代价：$(1 \times 3) + (3\times1) / (3 \times 3) = 2/3$</p><h3 id="Inception-module"><a href="#Inception-module" class="headerlink" title="Inception module"></a>Inception module</h3><p>因此基于设计原则➂和卷积核的分解方法➀，本文更新了传统的Inception组件结构：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftm7neonf9j314u0u20vh.jpg" alt="inceptionv2_module1"></p><p>在实践中，作者发现将$n\times n$卷积替换成$1 \times n$和$n\times1$的组合卷积，对于早期的网络层效果不是很好，但是在中期的网络层有很好的表现。基于卷积核的分解方法➁的Inception组件结构：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftm7u6ebzvj31dk128tcb.jpg" alt="inceptionv2_module2"></p><p>基于设计原则➁，作者还提出了第三种Inception组件结构：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftm7zo7emqj31540yk0xc.jpg" alt="Inception_module3"></p><p>作者解释道，仅在最粗糙的网格上使用第三种结构，因为与空间聚集相比，通过1x1卷积进行本地处理的比率增强时，产生高维稀疏表示的位置是最关键的，</p><h3 id="Grid-Size-Reduction"><a href="#Grid-Size-Reduction" class="headerlink" title="Grid Size Reduction"></a>Grid Size Reduction</h3><p>假设输入特征图大小为$d \times d \times k$，我们希望获得$\frac{d}{2} \times \frac{d}{2} \times 2k$大小的特征图，通常会使用池化层来减小其网格大小，常见的方式有以下两种：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftn0p7kdgkj30y40eagmw.jpg" alt="two_ways_reduce_grid_size"></p><p>❶ 先进行池化，再进行卷积（左）</p><p>这种方法的计算代价是$2(\frac{d}{2})^2 k^2$，但是特征图一下子就减小到$(\frac{d}{2})^2 k$，产生了瓶颈，违反了设计原则➀</p><p>❷ 先进行卷积，再进行池化（右）</p><p>这种方法会造成较高的计算代价：$2d^2k^2$</p><p>于是本文就提出了一种既不产生瓶颈，又能减少计算代价的方法，如下图右边所示。该方法使用两个stride为2的模块：池化层$P$和卷积层$C$，并将它们的输出拼接起来。计算代价减小为$2(\frac{d}{2})^2k^2$，然而不是很懂这个方法为什么不产生瓶颈。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftn1hqzbx1j30vo0eutad.jpg" alt="reduce_gride_size"></p><h3 id="Inception-v2-architecture"><a href="#Inception-v2-architecture" class="headerlink" title="Inception-v2 architecture"></a>Inception-v2 architecture</h3><p>将上述方法结合在一起，构成了Incpetion-v2网络：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftkwd3szd8j30um0l6tdx.jpg" alt="Inception-v2"></p><h3 id="Label-Smoothing-Regularization"><a href="#Label-Smoothing-Regularization" class="headerlink" title="Label Smoothing Regularization"></a>Label Smoothing Regularization</h3><p>假设一个训练样本产生$k$个分类的可能性分别为为$p(k)$，实值的分布为$q(k)$，那么交叉熵定义的损失函数为：<br>$$<br>L = - \sum_{k=1}^{K}\log ({p(k)})q(k)<br>$$<br>对输出值$z_k$求偏导：$\frac { \partial \ell } { \partial z _ { k } } = p ( k ) - q ( k )$</p><p>顺便复习了一下Softmax函数及其求导过程👉🏻<a href="https://blog.csdn.net/behamcheung/article/details/71911133#softmax%E5%87%BD%E6%95%B0" target="_blank" rel="external">Softmax函数与交叉熵</a></p><p>假设第$y$个为正确的标签，当$k=y$时，$q(k)=1$，当$k \ne q$时，$q(k)=0$。在这种情况下，最小化交叉熵的过程相当于最大化正确标签的对数似然函数。假设分类正确，那么$z_y \gg  z_k$。文章指出，这样会带来两个问题：</p><p>❶ 可能造成过拟合，如果模型学会为每个训练样本分配到正确标签的完整概率，则不能保证泛化。</p><p>❷ 使得最大的输出值和其他输出值之间的差异变大，降低模型的适应性。</p><p>也就是说，模型对它的预测太过自信。为了让模型不要太骄傲，作者提出了将实值分布$q ( k | x ) = \delta _ { k , y }$替换为标签平滑正则化(label-smoothing regularizatoin, LSR)：<br>$$<br>q ^ { \prime } ( k | x ) = ( 1 - \epsilon ) \delta _ { k , y } + \epsilon u ( k )<br>$$<br>其中$\epsilon$表示权重，$ u(k)$表示一个基于标签的先验分布，本文将其设置为均匀分布，则上式可以写为：<br>$$<br>q ^ { \prime } ( k ) = ( 1 - \epsilon ) \delta _ { k , y } + \frac { \epsilon } { K }<br>$$<br>$q(k)$意味着取值不是1就是0，而$q ^ { \prime } ( k )$意味着所有不正确的分类都有一个正下界。</p><p>我们还可以用交叉熵来理解LSR：<br>$$<br>H \left( q ^ { \prime } , p \right) = - \sum _ { k = 1 } ^ { K } \log p ( k ) q ^ { \prime } ( k ) = ( 1 - \epsilon ) H ( q , p ) + \epsilon H ( u , p )<br>$$<br>其中，$H(u,p)$可以用KL散度来表示：<br>$$<br>H ( u , p ) = D _ { K L } ( u | p ) + H ( u )<br>$$<br>由于KL散度用于衡量两个分布之间的距离，当$u$代表均匀分布时，$H(u)$是固定的，则$H(u,p)$代表预测分布$p$与均匀分布的差异程度。</p><h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h2><p>Inception-v3其实也是👆🏻那篇论文提出的。文章表明移除GoogleNet中的辅助分类器，并没有对实验结果产生不利的影响。实验结果表示，将辅助分类器作为正则项，当支路采用BN的时候，能够使主分类器有更好的结果。因此，本文将Inception-v2加上使用BN的支路称为Inception-v3。</p><h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception-v4"></a>Inception-v4</h2><p><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》</a>探索了ResNet给Inception组件带来的变化，提出了Inception-ResNet-v1和Inception-ResNet-v2，同时加深了网络结构。文章中有很多Inception组件，感觉不是很有意思，就大概滴看一下🤐</p><h2 id="Questions💫"><a href="#Questions💫" class="headerlink" title="Questions💫"></a>Questions💫</h2><ol><li>在Inception组件中，concat操作是直接将特征拼在一起的，若维度采用交叉拼接是否会使特征分布更均匀？</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GoogleNet&quot;&gt;&lt;a href=&quot;#GoogleNet&quot; class=&quot;headerlink&quot; title=&quot;GoogleNet&quot;&gt;&lt;/a&gt;GoogleNet&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.cv-foundation.org/ope
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Interleaved Group Convolutions</title>
    <link href="http://yoursite.com/2018/07/17/Interleaved-Group-Convolutions/"/>
    <id>http://yoursite.com/2018/07/17/Interleaved-Group-Convolutions/</id>
    <published>2018-07-17T02:29:32.000Z</published>
    <updated>2018-07-17T06:44:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文作者提出了一个名为交叉组卷积(Interleaved Group Convolution, IGC)的神经网络模块。它由两个连续的交叉组卷积构成：主组卷积和次组卷积。主组卷积中的操作时空间卷积，而次组卷积中的操作是逐点卷积。作者的关注点在减少卷积核的冗余，这个冗余来自空间范围和通道范围。在空间范围，发展出了小的卷积核，比如$3 \times 3，3 \times 1, 1 \times 3$。在通道范围，发展出了组卷积、逐通道卷积。本文研究的是通道范围的卷积核设计。</p><p>IGC的结构如下图所示：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftcny366afj310g0b6jw8.jpg" alt="interleaved_group_convolution"></p><p>令$L$表示主组卷积的分组数，$M$表示次组卷积的分组数(亦即主组卷积中每组的通道数)。上图展示的是$L=2$，$M=3$。</p><p>主组卷积可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftco2e2f78j30ho0520t0.jpg" alt="primary_group_convolution"></p><p>其中，$z_l$表示大小为$(MS)$的向量，$S$为卷积核大小。e.g. conv $ 3 \times 3$ -&gt; $S = 9$。$W_{ll}^p$表示第$l$组的卷积核，大小为$M \times (MS)$。则输出$y_l$为大小为$M$的向量。</p><p>将${y_1, y_2, \dots, y_l }$重新排列(permutation)作为次组卷积的输入：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1ftcollshyyj30q402kq31.jpg" alt="permutation1"></p><p>$\overline { y } _ { m }$表示次组卷积中的第$m$个分组，由主组卷积中不同组的第$m$个通道构成，大小为$L$。</p><p>次组卷积可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp07vq3vj30pq02a0ss.jpg" alt="secondary_group_convolution"></p><p>其中，$W_{mm}^s$表示第$m$组的$1\times1$卷积，大小为$L \times L$。则每组的输出$\overline { z } _ { m }$大小仍然为$L$。</p><p>最后对次组卷积的输出进行重新排列：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp2dp0rwj30ps028aa5.jpg" alt="permutation2"></p><p>整个IGC块可以表示为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcp37eoqyj30pm02474b.jpg" alt="igc_block"></p><p>令$W = PW^sP^TW^p$为组合卷积核，则我们有：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcu3e9arnj30pe028dft.jpg" alt="composite_convolution_kernel"></p><p>表示由两个稀疏卷积相乘得到的正常卷积。</p><p>IGC的参数量为：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1ftcu6ltmeij30pi03ywes.jpg" alt="igc_num_parameters"></p><p>假设输入输出的通道数为$C$，正常卷积的参数量为：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcu8q7aixj30pe02sq2y.jpg" alt="rc_num_parameters"></p><p>假设$T_{igc} = T_{rc}$，可以得到：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1ftcuahfrqvj30p0030q33.jpg" alt="compare_igc_rc"></p><p>对于典型的$S=3\times3$，当$L&gt;1$就有$G&gt;C$。也就是说，除了极端的$L=1$情况下，IGC内所含通道数都比正常卷积来得多，也就能包含更多的信息。</p><p>接下来作者分析了何种情况下，$G$能够最大。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcug9o7hxj30pu08wq3w.jpg" alt="when_widest"></p><p>由基本不等式得出公式(12)。因此当$L=MS$时，通道数最多。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1ftcuqfychdj30s208adig.jpg" alt="classification_accuracy_comparison"></p><p>实验结果表明，在计算代价近似，甚至IGC-L24M2更少的情况下，IGC的总体表现还是比较好的。正是由于IGC增加了宽度，更有效地利用了参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文作者提出了一个名为交叉组卷积(Interleaved Group Convolution, IGC)的神经网络模块。它由两个连续的交叉组卷积构成：主组卷积和次组卷积。主组卷积中的操作时空间卷积，而次组卷积中的操作是逐点卷积。作者的关注点在减少卷积核的冗余，这个冗余来自空
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ShuffleNet</title>
    <link href="http://yoursite.com/2018/07/16/ShuffleNet/"/>
    <id>http://yoursite.com/2018/07/16/ShuffleNet/</id>
    <published>2018-07-16T07:14:32.000Z</published>
    <updated>2018-07-20T03:09:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>ShuffleNet是一个高效计算的CNN结构。该结构使用了两种新的操作：逐点组卷积(pointwise group convolution)和通道混洗(channel shuffle)。</p><h2 id="Gconv和channel-shuffle"><a href="#Gconv和channel-shuffle" class="headerlink" title="Gconv和channel shuffle"></a>Gconv和channel shuffle</h2><p>1）逐点组卷积</p><p>在较小的网络中，对所有通道进行逐点卷积计算代价高昂。一个直接的解决办法就是利用通道稀疏连接。在本文中，作者使用的方法是组卷积，即对通道进行分组，在组内进行逐点卷积操作。</p><p>2）通道混洗</p><p>如果仅仅使用组卷积，那么最终每组输出仅于该组的输入相关联，阻碍了通道组之间的信息流，减弱了特征信息的表达。因此作者提出了通道混洗操作。通过将通道组细分为子通道组，将子通道组重新排列，作为下一层网络的输入。</p><p>具体地，使用了通道混洗操作的逐点组卷积如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ftbq9e31puj31ca0iumzu.jpg" alt="channel_shuffle"></p><h2 id="ShuffleNet单元"><a href="#ShuffleNet单元" class="headerlink" title="ShuffleNet单元"></a>ShuffleNet单元</h2><p>基于上面提到的两个操作，作者提出了ShuffleNet单元：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbqcw622hj318k0m2gp1.jpg" alt="shufflenet_units"></p><p>在传统的ResNet单元基础上，将头尾的1x1 Conv改为1x1 Gconv，中间的3x3 Conv改为3x3 DWConv，并且在DWConv之前加入了单元混洗操作。假设输入大小为$c \times h \times w$，bottelneck中的通道数为m。ResNet计算代价为$hw(2cm + 9m^2) $，而ShuffleNet的计算代价为$hw(2cm/g + 9m) ​$，其中$g$代表组卷积的分组数。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>ShuffleNet的网络结构如下图所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbquzmce7j31eo0k4gpf.jpg" alt="ShuffleNet_architecture"></p><p>由于输入Stage2的通道数较小，因此Stage2的第一个逐点卷积层不进行分组。</p><p>❗️看了代码才注意到，论文中有说明，bottleneck中的通道数是output channels/4</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者首先实验了不同分组数在不同大小网络上的分类效果：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbqz2ulxmj313s07sjsy.jpg" alt="scale_group"></p><p>从结果可以看出，使用分组卷积($g&gt;1$)与没有使用分组卷积($g=1$)相比，逐渐地表现越来越好。网络越小，从分组卷积中受益得越多。作者表示，在限制复杂度的情况下，分组卷积能够包含更多地特征图通道，从而可以编码更多的信息。因为较小的网络包含更窄的特征图，意味着能从扩大特征图中收益更多。</p><p>作者接下来比较了有无通道混洗在不同分组以及不同大小网络上的分类效果：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftbr72m6a9j316o0bcdid.jpg" alt="is_shuffle"></p><p>通道混洗操作的目标是使多组卷积层之间能够产生跨组信息流。实验结果表明，使用了通道混洗操作的网络分类效果都比没有使用通道混洗操作来的好。并且分组越多($g=8$)，通道混洗操作的效果就越明显。</p><p>作者还在同等计算代价的情况下，将ShuffleNet与其他网络做比较：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ftbrebkp2xj31c006odha.jpg" alt="various_structures"></p><p>以及与MobileNet做比较：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbrf6b15rj319e0g642f.jpg" alt="shufflenet_mobilenet"></p><p>可以看出ShuffleNet的分类结果是优于其他网络结构的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ShuffleNet是一个高效计算的CNN结构。该结构使用了两种新的操作：逐点组卷积(pointwise group convolution)和通道混洗(channel shuffle)。&lt;/p&gt;
&lt;h2 id=&quot;Gconv和channel-shuffle&quot;&gt;&lt;a href
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>压缩网络模型性能比较</title>
    <link href="http://yoursite.com/2018/07/14/comparison_models/"/>
    <id>http://yoursite.com/2018/07/14/comparison_models/</id>
    <published>2018-07-14T02:07:32.000Z</published>
    <updated>2019-05-05T01:39:07.658Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th style="text-align:center">Million  Mult-Adds</th><th style="text-align:center">Million Parameters</th><th style="text-align:center">ImageNet  Accuracy</th></tr></thead><tbody><tr><td>Conv MobileNet</td><td style="text-align:center"><strong>4866</strong></td><td style="text-align:center"><strong>28.3</strong>(29.3)</td><td style="text-align:center">71.7%</td></tr><tr><td>MobileNet</td><td style="text-align:center"><strong>568</strong>(575)</td><td style="text-align:center"><strong>3.2</strong>(4.2)</td><td style="text-align:center">70.6%</td></tr><tr><td>MobileNetv2</td><td style="text-align:center"><strong>312</strong>(300)</td><td style="text-align:center"><strong>2.2</strong>(3.4)</td><td style="text-align:center">72.0%</td></tr><tr><td>ShuffleNet(1.5)</td><td style="text-align:center">292?</td><td style="text-align:center">3.4?</td><td style="text-align:center">71.5%</td></tr><tr><td>ShuffleNet(x2)</td><td style="text-align:center">524?</td><td style="text-align:center">5.4?</td><td style="text-align:center">73.7%</td></tr></tbody></table><p>黑体部分为我计算的结果，括号内为MobileNet2论文中列出的数值。均未加上最后的fc。论文中没有指出ShuffleNet使用的模型结构，因此在下面「计算过程」中参照原始论文做了单独的计算。</p><h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>Standard Convolution: $h _ { i } \cdot w _ { i } \cdot d _ { i } \cdot d _ { j } \cdot k \cdot k$</p><p>Depthwise separable convolution: $h _ { i } \cdot w _ { i } \cdot d _ { i } \left( k ^ { 2 } + d _ { j } \right)$</p><p>Bottleneck convolution: $h_{i} \cdot w_{i} \cdot d_{i} \cdot t \left( d_{i} + k ^ { 2 } + d_{j} \right)$</p><p>输入是 $h_{in} _ w_{in} _c_{in}$，输出时$h_{out} _ w_{out} _ c_{out}$，卷积核为$d _ d _ c_{in} * c_{out}$</p><p><strong>Multi-adds:</strong></p><p>$c_{in} _ c_{out} _ d _ d _ w_{out} * h_{out}$</p><p><strong>1/ Conv MobileNet</strong></p><p>❶ Mult-Adds: </p><p>9x3x32x112^2</p><p>+9x32x64x112^2</p><p>+9x64x128x56^2</p><p>+9x128x128x56^2</p><p>+9x128x256x28^2</p><p>+9x256x256x28^2</p><p>+9x256x512x14^2</p><p>+5x(9x512x512x14^2)</p><p>+9x512x1024x7^2</p><p>+9x1024x1024x7^2</p><p>=<strong>4,866,269,184</strong></p><p>➋ Parameters:</p><p>9x3x32+9x32x64+9x64x128+9x128x128+9x128x256+9x256x256</p><p>+9x256x512+5x(9x512x512)+9x512x1024+9x1024x1024 = <strong>28,257,120</strong></p><p><strong>2/ MobileNet</strong></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1ft80stio5xj30tg0vkgrg.jpg" alt="MobileNet_architecture"></p><p>❗️14个block的stride应该是1，已通过阅读模型更正。</p><p>Seperable Depth convolution的参数数量：$d_i\times(9+d_j)$</p><p>➊ Mult-Adds: </p><p>9x3x32x112^2</p><p>+112^2 x (9x32 + 32x64) </p><p>+56^2 x (9x64 + 64x128)</p><p>+56^2 x (9x128 + 128x128)</p><p>+28^2 x (9x128 + 128x256)</p><p>+28^2 x (9x256 + 256x256)</p><p>+14^2 x (9x256 + 256x512)</p><p>+5x14^2 x (9x512 + 512x512) </p><p>+7^2 x (9x512 + 512x1024) </p><p>+7^2 x (9x1024 + 1024x1024) </p><p>=<strong>567,716,352</strong></p><p>➋ Parameters:</p><p>9x3x32+(9x32+32x64)+(9x64+64x128)+(9x128+128x128)+(9x128+128x256)+(9x256+256x256)+(9x256+256x512)+5x(9x512+512x512)+9x512+512x1024+9x1024+1024x1024 = <strong>3,185,088</strong></p><p><strong>3/ MobileNetV2</strong></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1ft87vpxvhrj30qk0icgok.jpg" alt="MobileNetV2_architecture"><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ft88vgekt5j30v40be0up.jpg" alt="Bottlenet_residual_block"></p><p>Bottleneck residual block的参数数量： $k \cdot tk + 3^2 \cdot tk  + tk \cdot k’ = tk(k+9 + k’)$</p><p>Bottleneck residual block的Mult-adds：$h \cdot w \cdot tk(k+9/s^2 + k’/s^2)$</p><p>➊ Mult-Adds: </p><p>112^2x3x9x32</p><p>+112^2x32x(32+9+16)</p><p>+112^2x6x16x(16+9/4+24/4) + 56^2x6x24x(24+9+24)</p><p>+56^2x6x24x(24+9/4+32/4) + 28^2x6x32x(32+9+32)x2</p><p>+28^2x6x32x(32+9/4+64/4) + 14^2x6x64x(64+9+64)x3</p><p>+14^2x6x64x(64+9+96) + 14^2x6x96x(96+9+96)x2</p><p>+14^2x6x96x(96+9/4+160/4) + 7^2x6x160x(160+9+160)x2</p><p>+7^2x6x160x(160+9+320)</p><p>+7^2x320x1280</p><p>=<strong>312,339,328</strong></p><p>➋ Parameters:</p><p>9x3x32</p><p>+32x(32+9+16)</p><p>+6x16x(16+9+24)+6x24x(24+9+24)</p><p>+6x24x(24+9+32)+6x32x(32+9+32)x2</p><p>+6x32x(32+9+64)+6x64x(64+9+64)x3</p><p>+6x64x(64+9+96)+6x96x(96+9+96)x2</p><p>+6x96x(96+9+160)+6x160x(160+9+160)x2</p><p>+6x160x(160+9+320)</p><p>+320x1280</p><p>=<strong>2,190,784</strong></p><p><strong>3/ ShuffleNet</strong></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1ftbie6hfd7j31960icjuo.jpg" alt="ShuffleNet"></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1ftbid72bosj30oc0kawgc.jpg" alt="ShuffleNet_unit"></p><p>假设bottleneck的输入通道数为c，输出通道数为$m$，ShuffleNet中设置中间通道数为$m/4$</p><p>$s=1$的ShuffleNet Units使用相加操作，Mult-adds：$hwm(2c/4g + 9/4) $</p><p>$s=2$的ShuffleNet Units使用拼接操作，Mult-adss：$hw(m-c)(c/4g + (9/4 + (m-c)/4g)/s^2$</p><p>令$c=m-c$，当且仅当$m=2c$。所以ShuffleNet中每个阶段的输出通道皆为输入的两倍。</p><p>所以$s=2$时的计算公式可以转化为：$hwc(c/4g + (9/4 + c/4g)/s^2$</p><p>➊ Mult-Adds(g=2): </p><p>112^2x3x9x24</p><p>+56^2x24x44 + 28^2x(9x44 + 44x176/2)</p><p>+28^2x(200x50/2 + 9x50 + 50x200/2) x 3</p><p>+28^2x200x50/2 + 14^2x(9x50+50x200/2)</p><p>+14^2x(400x100/2 + 9x100 + 100x400/2) x 7</p><p>+14^2x400x100/2 + 7^2x(9x100+100x400/2) </p><p>+7^2x(800x200/2 + 9x200 + 200x800/2) x 3</p><p>=<strong>129,196,340</strong></p><p><strong>Shufflenetx2 (g=3)</strong></p><p>112^2x3x9x48</p><p>+56^2x48x108 + 28^2x(9x108 + 108x432/3)</p><p>+28^2x(480x120/3 + 9x120 + 120x480/3) x 3</p><p>+28^2x480x120/3 + 14^2x(9x120+120x480/3)</p><p>+14^2x(960x240/3 + 9x240 + 240x960/3) x 7</p><p>+14^2x960x240/3 + 7^2x(9x240+240x960/2) </p><p>+7^2x(1920x480/3 + 9x480 + 480x1920/3) x 3</p><p>= 482,811,504</p><p>❗️和文章中的140M有些差距，在下面列出具体的计算过程：</p><table><thead><tr><th>Layer</th><th>params</th><th>macs</th></tr></thead><tbody><tr><td>Conv2D</td><td>3x9x24 = 648✔︎</td><td>112^2 x 648 = 8,128,512</td></tr><tr><td>Stage2_1 / 1x1conv</td><td>24x(200-24)/4 = 1,056✔︎</td><td>56^2 x1056 = 3,311,616</td></tr><tr><td>Stage2_1 / 3x3DWconv(s2)</td><td>9x44 = 396✔︎</td><td>28^2 x 396 = 310,464</td></tr><tr><td>Stage2_1 / 1x1Gconv</td><td>44/2 x 176/2 x 2 =  3,872✔︎</td><td>28^2 x 3872 = 3,035,648</td></tr><tr><td>Stage2_2 / 1x1Gconv</td><td>200/2 x 200/4/2 x 2 = 5,000✔︎</td><td>28^2 x 5000 = 3,920,000</td></tr><tr><td>Stage2_2 / 3x3DWconv</td><td>9x50 = 450✔︎</td><td>28^2 x 450 = 352,800</td></tr><tr><td>Stage2_2 / 1x1Gconv</td><td>50/2 x 200/2 x 2 = 5,000✔︎</td><td>28^2 x 5,000 = 3,920,000</td></tr><tr><td>Stage2_2 x 3</td><td>-</td><td>-</td></tr><tr><td>Stage3_1 / 1x1Gconv</td><td>200/2 x (400-200)/4/2 x2 = 5,000✔︎</td><td>28^2 x 5000 = 3,920,000</td></tr><tr><td>Stage3_1 / 3x3DWconv(s2)</td><td>9x50= 450✔︎</td><td>14^2 x 450 = 88,200</td></tr><tr><td>Stage3_1 / 1x1Gconv</td><td>50/2 x 200/2 x 2 = 5,000✔︎</td><td>14^2 x 5000 = 980,000</td></tr><tr><td>Stage3_2 / 1x1Gconv</td><td>400/2 x 400/4/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage3_2 / 3x3DWconv</td><td>9x100 = 900✔︎</td><td>14^2 x 900 = 176,400</td></tr><tr><td>Stage3_2 / 1x1Gconv</td><td>100/2 x 400/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage3_2 x 7</td><td>-</td><td>-</td></tr><tr><td>Stage4_1 / 1x1Gconv</td><td>400/2 x (800-400)/4/2 x 2 = 20,000✔︎</td><td>14^2 x 20000 = 3,920,000</td></tr><tr><td>Stage4_1 / 3x3DWconv(s2)</td><td>9x100= 900✔︎</td><td>7^2 x 900 = 44,100</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>200/2 x 800/4/2  x 2 = 20,000✔︎</td><td>7^2 x 20000 = 980,000</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>800/2 x 800/4/2 x 2 = 80,000✔︎</td><td>7^2 x 80,000 = 3,920,000</td></tr><tr><td>Stage4_2 / 3x3DWconv</td><td>9x200 = 1,800✔︎</td><td>7^2 x 1800 = 88,200</td></tr><tr><td>Stage4_2 / 1x1Gconv</td><td>200/2 x 800/2 x 2 = 80,000✔︎</td><td>7^2 x 80,000 = 3,920,000</td></tr><tr><td>Stage4_2 x 3</td><td>-</td><td>-</td></tr></tbody></table><p>看到一篇超级好的比较文章：</p><p><a href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d" target="_blank" rel="external">Why MobileNet and Its Variants (e.g. ShuffleNet) Are Fast</a></p><p><strong>Questions：</strong></p><ol><li>论文中关于Mut-adds的计算公式，是否仅考虑s=1的情况？比如在Depthwise separable convolution中，若s=2，那么计算代价应为$h _ { i }/ s \cdot w _ { i }/s \cdot d _ { i } \left( k ^ { 2 } + d _ { j } \right)$</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Million  Mult-Adds&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Million Parameters&lt;/th&gt;
&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>MobileNets</title>
    <link href="http://yoursite.com/2018/07/06/MobileNets/"/>
    <id>http://yoursite.com/2018/07/06/MobileNets/</id>
    <published>2018-07-06T02:07:32.000Z</published>
    <updated>2018-11-13T13:09:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></p><p>本文提出了一个MobileNets网络结构和两个超参数，可以用于移动和嵌入式视觉应用。与传统的卷积网络相比，大大减少了参数个数。构建较小网络的方法大致可以分为两类：1）压缩预训练模型；2）直接训练小的网络。</p><p>MobileNets基于流线型架构，使用<strong>深度分离卷积</strong>来构建轻量级深度神经网络。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>假设输入特征图$F$的大小为$D_F \times D_F\times M$，输出特征图$G$的大小为$D_G \times D_G \times N$，卷积核$K$的大小为$D_K \times D_K \times M \times N$</p><p>那么<strong>标准卷积</strong>可以表示为：<br>$$<br>\mathbf { G } _ { k , l , n } = \sum _ { i , j , m } \mathbf { K } _ { i , j , m , n } \cdot \mathbf { F } _ { k + i - 1 , l + j - 1 , m }<br>$$<br>计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot N \cdot D _ { F } \cdot D _ { F }<br>$$<br>深度分离卷积将标准卷积分解为<strong>深度卷积(depthwise convolutions)</strong>和称为<strong>逐点卷积(point wise convolutions)</strong>的1×1卷积。</p><p>每个输入通道使用一个卷积核进行<strong>深度卷积</strong>可以表示为：<br>$$<br>\hat { \mathbf { G } } _ { k , l , m } = \sum _ { i , j } \hat { \mathbf { K } } _ { i , j , m } \cdot \mathbf { F } _ { k + i - 1 , l + j - 1 , m }<br>$$<br>计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot D _ { F } \cdot D _ { F }<br>$$<br>深度卷积只对输入的通道进行了处理，相当于生成了M个大小为$D_G \times D_G$的特征图。接着我们利用<strong>逐点卷积</strong>将<strong>深度卷积</strong>的输出线性连接起来，产生新的特征图。</p><p>总的计算成本为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot M \cdot D _ { F } \cdot D _ { F } + M \cdot N \cdot D _ { F } \cdot D _ { F }<br>$$<br>标准卷积过滤器与深度分离卷积过滤器的比较如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fszxe85zmsj30ni0vmq6k.jpg" alt="convolutional_filters"></p><p>MobileNet的网络结构如下表所示：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fszypbbny0j30os0qejym.jpg" alt=""></p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>为了更进一步减少参数个数，本文接着提出了两个超参数：宽度乘数(width multiplier)和分辨率乘数(resolution multiplier)</p><p><strong>宽度乘数$\alpha$</strong>的作用是在每层均匀地稀疏网络，含$\alpha$的深度分离卷积的计算成本可以表示为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot \alpha M \cdot D _ { F } \cdot D _ { F } + \alpha M \cdot \alpha N \cdot D _ { F } \cdot D _ { F }<br>$$<br><strong>分辨率乘数$\rho$</strong>的作用是在每层减少输入图像的大小，含$\alpha$和$\rho$的深度分离卷积的计算成本可以表示为：<br>$$<br>D _ { K } \cdot D _ { K } \cdot \alpha M \cdot \rho D _ { F } \cdot \rho D _ { F } + \alpha M \cdot \alpha N \cdot \rho D _ { F } \cdot \rho D _ { F }<br>$$</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>MobileNet模型可以应用于许多设备智能的识别任务中，包括物体检测、细粒度分类、面部属性、地标识别等。本文通过将MobileNet与当前流行的其他网络模型作比较，可以看出在各项任务中，MobileNet虽然准确率不如大的网络，但是它大大减少了参数的个数，提升了模型计算效率，正如它的名字一样，是有志于应用在移动设备上的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1704.04861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文提出了一个MobileNets网络
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>模型误差从哪儿来？</title>
    <link href="http://yoursite.com/2018/06/22/where-does-the-error-come-from/"/>
    <id>http://yoursite.com/2018/06/22/where-does-the-error-come-from/</id>
    <published>2018-06-22T02:41:29.000Z</published>
    <updated>2018-07-16T08:15:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>开始学李宏毅的机器学习，用pokerman来举线性回归的例子实在是太有意思啦～有受到一些启发，以此巩固一下基础知识。</p><h2 id="误差来源"><a href="#误差来源" class="headerlink" title="误差来源"></a>误差来源</h2><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fsjrf9bj1gj31g4122h0d.jpg" alt="estimator"></p><p>在前面几节课里提到，机器学习的过程其实就是在一堆模型里找到那个最符合所给数据的最优模型$f^\ast$。这个$f^\ast$实际上是真实数据模型$\hat{f}$的一个估计。</p><p>只要是估计就会存在误差，那么这个误差是从哪里来的呢？它有两个来源，一是偏差（Bias），二是方差（Variance）。</p><h2 id="均值和方差"><a href="#均值和方差" class="headerlink" title="均值和方差"></a>均值和方差</h2><p>基于大数据的机器学习很大程度上可以看作是在利用数理统计的知识来解决问题。在数理统计中，我们利用样本的信息（统计量）来推断总体的未知信息（估计量）。</p><p>我们可以利用样本的均值来估计数学期望：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fsjs1yq7isj31g211udli.jpg" alt="bias-estimator"></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fsjsaxm0uuj31gg134jxg.jpg" alt="unbiased-estimator"></p><p>也可以利用样本的方差来估计方差：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fsjs9tt7k6j31ge11s7a2.jpg" alt="variance-estimator"></p><p>假设我们在100个平行宇宙里做实验，获取到100个$f^_$，我们可以计算出这100个$f^_$的均值，来判断这些模型与$\hat{f}$之间的偏差：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fsjsjesachj31gs124x0f.jpg" alt="bias"></p><p>我们还可以计算出这100个$f^*$的方差，来判断这些模型的分散程度：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fsjskoyd96j31gg12q4kf.jpg" alt=""></p><h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p>将偏差和方差产生的两条误差曲线和在一起，就变成了观察到的误差曲线。因此也引出了过拟合（Overfitting）和欠拟合（Underfitting）的概念。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fsjsob3ibhj31g412edr3.jpg" alt="overfitting-underfitting"></p><p><strong>欠拟合：</strong>当我们发现模型不能很好地拟合训练样本，很可能是偏差较大。可以通过1）增加数据特征 2）建立更复杂的模型来解决欠拟合。</p><p><strong>过拟合：</strong>当我们的模型很好的拟合训练样本，但在测试样本上产生了巨大的误差，很可能是方差较大。可以通过1）增加数据量 2）正则化来解决过拟合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;开始学李宏毅的机器学习，用pokerman来举线性回归的例子实在是太有意思啦～有受到一些启发，以此巩固一下基础知识。&lt;/p&gt;
&lt;h2 id=&quot;误差来源&quot;&gt;&lt;a href=&quot;#误差来源&quot; class=&quot;headerlink&quot; title=&quot;误差来源&quot;&gt;&lt;/a&gt;误差来源&lt;/h2
      
    
    </summary>
    
    
  </entry>
  
</feed>
